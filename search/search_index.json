{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MLE Knowledge Base!","text":"<p>Hello and welcome, MLE staff!</p> <p>We're excited to launch this new, centralized knowledge base. Our goal is to create a single source of truth where every department can share their processes, guides, and important information with the rest of the organization. By documenting our collective knowledge, we can improve collaboration, streamline onboarding, and make it easier for everyone to find the information they need to excel.</p>"},{"location":"#important-this-site-is-public","title":"Important: This Site is Public","text":"<p>Please be aware that this website is publicly accessible on the internet. Do not add any sensitive or private information to these pages. Any information that requires access control (like personal data, financial details, or private credentials) should remain in its current, secure location, such as our shared Google Drive folders.</p>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>Contributing is the best way to help us keep this knowledge base up-to-date. All you need is a GitHub account. The following steps will guide you through editing a file, which includes creating your own personal copy (a \"fork\") of the repository and submitting your changes for review (a \"pull request\").</p>"},{"location":"#editing-an-existing-page-the-easiest-way","title":"Editing an Existing Page (The Easiest Way)","text":"<ol> <li>Find the file on GitHub: In the main repository, navigate to the <code>docs/</code> directory and find the file for the page you want to edit.</li> <li>Click the Edit icon: In the top-right corner of the file view, click the pencil icon (\"Edit this file\").</li> <li>GitHub Creates Your Fork: If you don't have direct edit access, GitHub will automatically create a personal copy (a \"fork\") of this repository for you. You'll see a notice at the top of the page explaining this. This is your personal workspace where you can make changes safely.</li> <li>Make Your Changes: Use the web editor to make your desired changes to the content.</li> <li>Describe Your Change: Scroll to the bottom. In the first text box, write a short, clear summary of your changes (e.g., \"Updated contact info for the Marketing department\").</li> <li>Propose the Change: Click the green \"Propose changes\" button. This saves the changes to a new branch in your personal fork.</li> <li>Create the Pull Request: You'll be taken to a new page to open a \"pull request.\" This is how you ask the project maintainers to review your changes and add them to the main knowledge base. Simply click the green \"Create pull request\" button.</li> </ol> <p>That's it! Your proposed change will be sent for review.</p>"},{"location":"#adding-a-new-page","title":"Adding a New Page","text":"<p>This process is slightly more manual.</p> <ol> <li>Fork the Repository First: Go to the main page of the knowledgeBase repository and click the Fork button in the top-right corner. This creates your own copy.</li> <li>Navigate to the <code>docs/</code> Directory in Your Fork: Make sure you are in your forked repository (the URL will be <code>github.com/YourUsername/knowledgeBase</code>). Go to the <code>docs/</code> directory (or a subdirectory) where you want to add the page.</li> <li>Create the File: In the top-right corner, click Add file &gt; Create new file.</li> <li>Name Your File: Give your file a descriptive name that ends with <code>.md</code> (e.g., <code>new-style-guide.md</code>).</li> <li>Add Your Content: Write your content using Markdown.</li> <li>Commit the New File: At the bottom, provide a short description and click \"Commit new file.\" This saves the file to your fork.</li> <li>Open a Pull Request: Navigate to the \"Pull requests\" tab in your forked repository and click the \"New pull request\" button. Follow the prompts to create a pull request, which sends your new page to the main repository for review.</li> </ol>"},{"location":"#lets-build-this-together","title":"Let's Build This Together!","text":"<p>A knowledge base is only as good as the information within it. We encourage every member of the MLE staff to take an active role in populating this site. If you see something missing, add it! If a process is unclear, clarify it! Your contributions are essential to making this a powerful resource for all of us.</p>"},{"location":"departments/community/","title":"Community","text":""},{"location":"departments/community/#important-info","title":"Important Info","text":"<ul> <li>Add important information here.</li> </ul>"},{"location":"departments/community/#contacts","title":"Contacts","text":"<ul> <li>Add contact information here.</li> </ul>"},{"location":"departments/marketing/","title":"Marketing","text":""},{"location":"departments/marketing/#overview","title":"Overview","text":"<ul> <li>Marketing is an essential cornerstone of the Minor League Esports ecosystem. from getting new members and new eyes onto our community to driving funding and sponsorships, our work helps \"keep the lights on\", keeps our community engaged and is the engine that drives new members into our ranks! this page and the informtion herein is to help our beloved staff get familiar with the processes involved in keeping Marketing running smoothly. </li> </ul>"},{"location":"departments/marketing/#contacts","title":"Contacts","text":"<ul> <li>Head of Marketing: Spike / Han1ckz</li> <li>Marketing Coordinator: SkyGuy17</li> <li>Social Media Team Lead: Flap</li> <li>Merchandise Team Lead: Tunloink</li> <li>Sponsorship Team Lead: Pander</li> </ul>"},{"location":"departments/marketing/#social-media","title":"Social Media","text":"<ul> <li> <p>Social Media staff is responsible for posting informtion and graphics to X (formerly twitter), youtube and other socials. The goal is to make our Social Medias a \"one stop shop\" for new info, easily digestable and interactable sections for match dates, holiday graphics, and Sponsorship updates, among others. </p> </li> <li> <p>For access to MLE's Socials, contact Spike to grant you access. (for example, on X we add your account as a \"delegate\", this permission is something only Director+ can give you.)</p> </li> <li> <p>MLEs standards for quality are high, but not impossible for the inexperinced to achieve. We ask that graphics and videos follow the same \"vibe\" and \"atmosphere\", so that styles dont clash week to week, or even post to post. you can expirament and be creative while also keeping a consistent style, think outside the box! </p> </li> <li> <p>This should go without saying, but dont post anything that will get MLE on the news (in a bad way), no offensive or vulgar content, all posts must fall within the MLE guidelines as well as the guidelines of the platform you are using. exersise common sense please! if you are unsure, err on the side of caution or get approval from Coordinator+. </p> </li> <li> <p>For new Social media staff, have Spike add you to the Google Drive for MLE assets for use in image and video editing / creation. please note that these assets are ONLY for use within official an official MLE capacity, and the assets themselves are not to be edited directly. for examples or if you are unsure if your idea falls outside these boundries, have Team Lead+ review it on a case by case basis. </p> </li> <li> <p>Once your graphic, post, video, etc is ready to ship, share it in your teams respective Discord channel and get the \"green light\" before clicking post. </p> </li> </ul>"},{"location":"departments/marketing/#sponsorships","title":"Sponsorships","text":"<ul> <li>This department is responsible for maintaining current sponsorships, seeking out new ones, fulfilling orders and keeping winners updated on the current status of their winnings. This department also works alongside the Merch department for any merch related winnings s determined by our Fourthwall store or our Jersey store Gurdian Proline. </li> </ul>"},{"location":"departments/marketing/#merchandise","title":"Merchandise","text":"<ul> <li> <p>This department is responsible for the creation of new Merch, the design of products in our Fourthwall store, our relationship with Guardian Proline, as well as finding ways to implement community ideas for merchandise. </p> </li> <li> <p>For new Merch staff, please have Coordinator+ add you as an \"editor\" on our fourthwall store. From here, you can create new merch, edit individual teams merch, create discount codes, create custom \"collections\", and browse hundreds of customizable items to sell. </p> </li> <li> <p>Historically, for new merch rollouts we go by Division as to not overload out team. 4 teams at a time is easier than 32 all at once. </p> </li> </ul>"},{"location":"departments/production/","title":"Production","text":""},{"location":"departments/production/#leadership","title":"Leadership","text":"<ul> <li>Head of Production: pandy</li> </ul>"},{"location":"departments/production/#coordinators","title":"Coordinators","text":"<ul> <li>JustStayinAlive</li> <li>Hunted</li> </ul>"},{"location":"departments/production/#production-management","title":"Production Management","text":"<ul> <li>Half Past White</li> <li>Just Jelly</li> </ul>"},{"location":"departments/production/#affiliate-team","title":"Affiliate Team","text":"<p>The affiliate team are the ones who distribute matches that want to be streamed but are not on the main stream to community members who volunteer to stream that game. - Manager: MonsterJamers</p>"},{"location":"departments/production/#production-team-leads","title":"Production Team Leads","text":"<ul> <li>Talent Manager: fallen_dan</li> <li>Lead Broadcast Manager: JustStayinAlive</li> <li>Head Match Official: Kend0</li> <li>Archive Team Lead: Fahhr</li> <li>Media Team Lead: TheGamingBear</li> </ul>"},{"location":"departments/production/#open-net","title":"Open Net","text":"<p>Open net is a weekly podcast discussing current events in MLE. - Project Manager: SkyGuy17 - Content Manager: IceColeBrew - Talent Manager: Quilli - Graphic Designer: Crazy - Social Media: Snipz21</p>"},{"location":"departments/rocket-league-operations/","title":"Rocket League Operations","text":""},{"location":"departments/rocket-league-operations/#important-info","title":"Important Info","text":"<ul> <li>Add important information here.</li> </ul>"},{"location":"departments/rocket-league-operations/#key-system-interactions","title":"Key System Interactions","text":""},{"location":"departments/rocket-league-operations/#player-management-plm","title":"Player Management (PLM)","text":"<p>Rocket League Operations staff have the ability to manage players directly through the system via CIC actions. This includes:</p> <ul> <li>Initial Placement: Assigning new players to a league upon joining MLE.</li> <li>Salary Adjustments: Modifying a player's salary.</li> <li>League Placement Changes: Moving a player between leagues.</li> </ul>"},{"location":"departments/rocket-league-operations/#eligibility-validation","title":"Eligibility Validation","text":"<p>The department is also responsible for ensuring player eligibility for matches. This involves:</p> <ul> <li>Scrim Point Verification: Using system tools to validate that a player has accumulated the required number of scrimmage points for the week.</li> </ul>"},{"location":"departments/rocket-league-operations/#contacts","title":"Contacts","text":"<ul> <li>Add contact information here.</li> </ul>"},{"location":"departments/trackmania-operations/","title":"Trackmania Operations","text":""},{"location":"departments/trackmania-operations/#important-info","title":"Important Info","text":"<ul> <li>*Add important information here.</li> </ul>"},{"location":"departments/trackmania-operations/#contacts","title":"Contacts","text":"<ul> <li>Director of Trackmania Operations     Ant - Discord: @Ant Email: ant@mlesports.gg</li> <li>Head of Trackmania LO     OkBoomer - Discord: @okboomer</li> <li>Trackmania LO Team Leads     Zyta - Discord: @zytabyte</li> <li>CIC Coordinator     Arunos - @wyntahfox</li> </ul>"},{"location":"departments/development/development/","title":"Development","text":""},{"location":"departments/development/development/#code-ownership-disclaimer","title":"Code Ownership: Disclaimer","text":"<p>All code and tools developed for Minor League Esports (\"MLE\") are owned and controlled by MLE. Credit will be apportioned for whatever you work on, and you may use code created in other places. You are also free to use projects created for MLE on your own personal resumes/CVs. However, all code written for MLE is property of the organization and should be treated as such.</p>"},{"location":"departments/development/development/#important-info","title":"Important Info","text":"<ul> <li>Systems Overview</li> <li>Sprocket Identity System</li> </ul>"},{"location":"departments/development/development/#new-team-members-start-here","title":"New Team Members Start Here!","text":"<ul> <li>New Team Members</li> </ul>"},{"location":"departments/development/development/#contacts","title":"Contacts","text":"<ul> <li>Head of Development: Nigel Thornbrake (@nigelthornbrake)</li> </ul>"},{"location":"departments/development/new-team-members/","title":"New Team Members - Start Here","text":""},{"location":"departments/development/new-team-members/#welcome-to-the-team","title":"Welcome to the team!","text":"<p>Welcome to the MLE Development team. We understand it can be overwhelming getting used to a new development environment, ESPECIALLY in this case. No matter how much experience you have, you can help here. Below you'll see what skills we mostly use and where they are used. This way,  you can either get to work right away OR see what you need to learn to be of assistance.</p>"},{"location":"departments/development/new-team-members/#code-ownership-disclaimer","title":"Code Ownership: Disclaimer","text":"<p>All code and tools developed for Minor League Esports (\"MLE\") are owned and controlled by MLE. Credit will be apportioned for whatever you work on, and you may use code created in other places. You are also free to use projects created for MLE on your own personal resumes/CVs. However, all code written for MLE is property of the organization and should be treated as such.</p>"},{"location":"departments/development/new-team-members/#getting-started","title":"Getting Started","text":"<ul> <li>DM Keegabyte for Click Up access: We use Click Up for our project management and a lot of information sharing. DM Keegabyte for access to the clickup workspace. He will need an email that you would be willing to make a Click Up account under.</li> <li>Introduce yourself: Introduce yourself to the team. I promise, we're friendly.</li> <li>Consult the Systems Overview: Here you can see what systems we manage and who to contact about getting involved in those systems.</li> <li>Take a look at the general skills: These are just some skills that get used everywhere. You can learn these on the job. See them below.</li> <li>Figure out how you want to help: Check below for what skills we currently need help with.</li> </ul>"},{"location":"departments/development/new-team-members/#general-skills","title":"General Skills","text":"<p>These are some skills that are used practically everywhere.</p> <ul> <li>Google Suite: It's always a good idea to be knowledgeable with the Google Suite. We use a LOT of Google Sheets.</li> <li>Github: Learning how to colaborate on a team in Github is important to what we do here.</li> </ul>"},{"location":"departments/development/new-team-members/#in-demand-skills","title":"In-Demand Skills","text":"<p>These are our current needs...</p> <ul> <li>Database Design: Several projects are in need of database designers. SQL, Sheets, and data analysis skills will be required.</li> <li>Python: Most of our Discord bots run on Python, more specifically the Discord.py module. We also have several applications that are programmed in Python.</li> <li>Web development: Many of our current and future applications rely on web interfaces. Javascript, HTML, and CSS are required.</li> <li>Data Analysis &amp; Visualization: Our Evidence, Sprocket, and user management products each heavily use data analysis. Python and Google Sheets will be required.</li> <li>LDAP/AD: We are currently exploring LDAP solutions that would work for our purposes.</li> <li>Project Management: As part of our continuing efforts to become more organized, we are in need of project managers. Professional experience not required.</li> </ul>"},{"location":"departments/development/new-team-members/#what-next","title":"What next?","text":"<p>Next, send a message in the dev-talk channel in the staff Discord saying you want to help and how you can be of assistance. We will be glad to have your help wherever you can provid it!</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/","title":"Sprocket V2: Unified Matchmaking System Design Proposal","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#executive-summary","title":"Executive Summary","text":"<p>This proposal outlines a fundamental transformation of MLE's matchmaking system from isolated skill-group-based queues to a unified, cross-league matchmaking platform. By implementing a single Elo rating pool across all skill groups and introducing intelligent matchmaking algorithms, we can significantly improve player experience, reduce queue times, and create more balanced matches while maintaining competitive integrity across all skill levels.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#current-system-analysis","title":"Current System Analysis","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#the-fragmented-landscape","title":"The Fragmented Landscape","text":"<p>MLE's current matchmaking system operates like a collection of isolated islands, each skill group (Foundation League, Academy League, Champion League, etc.) maintains its own separate Elo rating pool. A player rated 1300 in Foundation League cannot meaningfully compare their skill to a 900-rated player in Champion League, despite potentially similar actual skill levels. This fragmentation creates several critical issues:</p> <p>Skill Group Silos: Players are locked into their respective skill groups with no mechanism for cross-group competition. The system enforces rigid boundaries where a 10.0 salary player in Foundation League (approaching promotion) might have nearly identical skill to a 10.5 salary player in Academy League (at risk of demotion), yet they can never compete against each other. This creates artificial barriers that don't reflect the continuous nature of skill development.</p> <p>Queue Fragmentation: Each skill group operates independent queues for different game modes (2v2, 3v3, LFS). A player seeking any competitive match must manually check each skill group's queue separately, leading to reduced match frequency and longer wait times. The current system processes matchmaking by finding players within expanding skill ranges (starting at \u00b1100 rating, expanding to \u00b1500), but this expansion happens within isolated skill groups rather than across the entire player base.</p> <p>Bubble Player Instability: Players near skill group boundaries experience constant promotion/demotion cycles without adequate hysteresis mechanisms. The current system lacks sufficient buffer zones, causing players to oscillate between skill groups based on small rating fluctuations rather than sustained performance changes.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#technical-architecture-overview","title":"Technical Architecture Overview","text":"<p>The existing system centers around several key entities: <code>PlayerEntity</code> contains a <code>salary</code> field (string-based rating representation) and links to <code>SkillGroupEntity</code>. The <code>ScrimQueueEntity</code> manages individual queue entries with <code>skillRating</code> values, while the <code>QueueService</code> implements the current matchmaking logic with expanding skill ranges.</p> <p>Match results flow through a submission system where replay files are validated, ratified by players, and then processed for rating updates. However, the current implementation shows several TODO comments indicating incomplete Elo update logic: <code>// TODO: Elo Side Effects</code> appears in player mutation handlers, suggesting the rating update system requires significant development.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#industry-best-practices-analysis","title":"Industry Best Practices Analysis","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#league-of-legends-tier-based-mmr-with-promotion-series","title":"League of Legends: Tier-Based MMR with Promotion Series","text":"<p>Riot Games implements a sophisticated hybrid system combining visible tiers (Iron through Challenger) with hidden Match Making Rating (MMR). Players compete within divisions (I-IV) while their MMR operates on a continuous scale. The system includes promotion series that require sustained performance to advance, preventing random fluctuation-based promotions. Most importantly, players can be matched across tier boundaries when their MMR aligns, creating larger matchmaking pools while maintaining competitive integrity.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#dota-2-visible-mmr-with-seasonal-recalibration","title":"Dota 2: Visible MMR with Seasonal Recalibration","text":"<p>Valve's approach uses fully visible MMR numbers ranging from 0 to over 10,000, with separate ratings for solo and party play. The system implements seasonal recalibration, allowing players to reset their ratings periodically while maintaining some historical data. Dota 2's matchmaking algorithm considers not just MMR but also uncertainty values, matching new accounts with wider skill ranges to quickly establish accurate ratings.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#counter-strike-global-offensive-glicko-2-implementation","title":"Counter-Strike: Global Offensive: Glicko-2 Implementation","text":"<p>CS:GO utilizes the Glicko-2 rating system, which maintains both a rating value and a Rating Deviation (RD) that represents uncertainty. Higher RD values allow for larger rating changes and broader matchmaking ranges. The system naturally implements hysteresis through the RD mechanism\u2014players with stable performance have lower RD, making their ratings more resistant to change.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#proposed-unified-system-architecture","title":"Proposed Unified System Architecture","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#core-philosophy-continuous-skill-spectrum","title":"Core Philosophy: Continuous Skill Spectrum","text":"<p>The new system replaces discrete skill group silos with a continuous Elo rating scale spanning all skill groups. Rather than maintaining separate rating pools, all players exist on a single spectrum where skill group boundaries represent ranges rather than absolute barriers. This approach acknowledges that skill development is continuous, not discrete, and allows for more nuanced player matching.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#unified-elo-pool-implementation","title":"Unified Elo Pool Implementation","text":"<p>Single Rating System: All players share one Elo pool ranging from 0 to 3000+, with skill groups mapped to specific rating ranges:</p> <ul> <li>Foundation League: 0-800</li> <li>Academy League: 700-1200  </li> <li>Champion League: 1100-1600</li> <li>Master League: 1500-2000</li> <li>Premier League: 1900+</li> </ul> <p>The overlapping ranges (100-point buffers) create natural transition zones where cross-group matches can occur, while the unified pool ensures meaningful skill comparisons across all levels.</p> <p>Hysteresis Mechanisms: Implement promotion/demotion thresholds with built-in buffers. Players promote when exceeding the upper threshold (e.g., 820 for Foundation \u2192 Academy) but only demote when falling below the lower threshold (e.g., 680 for Academy \u2192 Foundation). This 140-point buffer prevents constant oscillation and requires sustained performance changes for skill group transitions.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#single-master-queue-system","title":"Single Master Queue System","text":"<p>Multi-Queue Capability: Players can simultaneously queue for multiple scrim types (2v2 Round Robin, 3v3 Round Robin, Team 2v2/3v3, LFS) across all skill groups. The system maintains separate queue entries but processes them through a unified matchmaking algorithm that finds the best available match regardless of skill group boundaries.</p> <p>Intelligent Matchmaking Algorithm: The proposed algorithm expands search parameters based on queue time:</p> <ol> <li>Initial Search (0-2 minutes): Match players within \u00b1100 rating points of the same skill group</li> <li>Expanded Search (2-5 minutes): Extend to \u00b1150 rating points, allow cross-group matches within 50 rating points of boundaries</li> <li>Broad Search (5+ minutes): Expand to \u00b1250 rating points, prioritize match quality over skill group boundaries</li> <li>Emergency Match (10+ minutes): Maximum expansion to \u00b1400 rating points, create matches with available players</li> </ol>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#cross-skill-group-matchmaking-benefits","title":"Cross-Skill-Group Matchmaking Benefits","text":"<p>Bubble Player Solution: The unified system naturally addresses bubble player issues. A 795-rated Foundation League player can match against a 805-rated Academy League player\u2014these players are only 10 rating points apart despite being in different skill groups. The system recognizes their similar skill levels and creates balanced matches that wouldn't be possible in the current siloed approach.</p> <p>Expanded Matchmaking Pools: By removing skill group barriers, the effective player pool for matchmaking increases dramatically. During off-peak hours, when individual skill groups might have only 2-3 queued players, the unified system can draw from 15-20 players across all skill groups, significantly reducing queue times.</p> <p>Skill Development Acceleration: Cross-group matches provide valuable learning opportunities. Foundation League players competing against Academy League opponents experience higher-level play, accelerating skill development. Conversely, Academy League players mentoring Foundation League opponents reinforce fundamental concepts.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#technical-implementation-strategy","title":"Technical Implementation Strategy","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#database-schema-evolution","title":"Database Schema Evolution","text":"<p>The current <code>PlayerEntity</code> requires modification to support the unified system:</p> <pre><code>@Entity('player')\nexport class PlayerEntity extends BaseEntity {\n    @Column()\n    unifiedEloRating: number; // Single Elo value (0-3000+)\n\n    @Column()\n    skillGroup: SkillGroup; // Current skill group for display/organization\n\n    @Column()\n    eloUncertainty: number; // Glicko-2 RD equivalent\n\n    @Column()\n    lastRatingUpdate: Date;\n\n    @Column({ nullable: true })\n    promotionThreshold: number; // Personal promotion threshold\n\n    @Column({ nullable: true })\n    demotionThreshold: number; // Personal demotion threshold\n}\n</code></pre>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#matchmaking-service-architecture","title":"Matchmaking Service Architecture","text":"<p>The <code>QueueService</code> requires fundamental restructuring:</p> <pre><code>@Injectable()\nexport class UnifiedQueueService {\n    private readonly INITIAL_SKILL_RANGE = 100;\n    private readonly MAX_SKILL_RANGE = 400;\n    private readonly CROSS_GROUP_THRESHOLD = 50; // Allow cross-group within 50 points\n\n    async findMatches(gameId: string): Promise&lt;MatchmakingResult[]&gt; {\n        const allQueuedPlayers = await this.getAllQueuedPlayers(gameId);\n\n        // Group by skill level rather than skill group\n        const skillSortedPlayers = allQueuedPlayers.sort((a, b) =&gt; a.skillRating - b.skillRating);\n\n        // Implement time-based expansion\n        const currentTime = new Date();\n        const expansionFactor = this.calculateExpansionFactor(skillSortedPlayers, currentTime);\n\n        return this.createBalancedMatches(skillSortedPlayers, expansionFactor);\n    }\n}\n</code></pre>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#rating-update-algorithm","title":"Rating Update Algorithm","text":"<p>Implement a Glicko-2 inspired system that maintains both rating and uncertainty:</p> <pre><code>interface RatingUpdateResult {\n    newRating: number;\n    newUncertainty: number;\n    skillGroupChange?: SkillGroupChange;\n}\n\nasync function updateRating(\n    player: PlayerEntity,\n    matchResult: MatchResult,\n    opponentRatings: number[]\n): Promise&lt;RatingUpdateResult&gt; {\n    // Calculate rating change based on expected vs actual outcome\n    const expectedScore = calculateExpectedScore(player.unifiedEloRating, opponentRatings);\n    const actualScore = matchResult.playerScore;\n\n    // Apply Glicko-2 style update with uncertainty adjustment\n    const ratingChange = (actualScore - expectedScore) * player.eloUncertainty;\n    const uncertaintyChange = calculateUncertaintyChange(player, matchResult);\n\n    return {\n        newRating: player.unifiedEloRating + ratingChange,\n        newUncertainty: Math.max(player.eloUncertainty + uncertaintyChange, MIN_UNCERTAINTY),\n        skillGroupChange: determineSkillGroupChange(player, newRating)\n    };\n}\n</code></pre>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#user-experience-enhancements","title":"User Experience Enhancements","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#multi-queue-interface","title":"Multi-Queue Interface","text":"<p>Players can select multiple scrim types simultaneously through an intuitive interface:</p> <ul> <li>Primary Queue: Player's preferred scrim type (highest priority)</li> <li>Secondary Queues: Additional scrim types they're willing to play</li> <li>Cross-Group Preference: Option to enable/disable cross-skill-group matches</li> <li>Maximum Skill Gap: Player-defined maximum rating difference for matches</li> </ul>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#transparent-skill-group-boundaries","title":"Transparent Skill Group Boundaries","text":"<p>Display current skill group ranges and personal thresholds:</p> <pre><code>Your Rating: 1,247 (Academy League)\nPromotion to Champion League: 1,250 (3 points needed)\nDemotion to Foundation League: 1,150 (97 points buffer)\nCross-group matches: Enabled (\u00b150 rating points)\n</code></pre>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#matchmaking-feedback","title":"Matchmaking Feedback","text":"<p>Provide real-time feedback during queue:</p> <ul> <li>Estimated wait time based on current queue population</li> <li>Number of players in compatible skill ranges</li> <li>Cross-group match opportunities identified</li> <li>Skill rating changes from recent matches</li> </ul>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#risk-assessment-and-mitigation","title":"Risk Assessment and Mitigation","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#competitive-integrity-concerns","title":"Competitive Integrity Concerns","text":"<p>Risk: Cross-group matches might create unfair competitive environments.</p> <p>Mitigation: Implement strict rating-based matching regardless of skill group. A 1200-rated Foundation League player should never face a 1600-rated Champion League player, even if both are queued. The system prioritizes rating proximity over skill group boundaries.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#player-confusion-about-skill-groups","title":"Player Confusion About Skill Groups","text":"<p>Risk: Players might not understand why they're matching against different skill groups.</p> <p>Mitigation: Provide clear educational materials explaining the continuous rating system. Show pre-match information: \"This opponent is 45 rating points higher\u2014an even match!\" Emphasize that skill groups are organizational tools, not absolute skill barriers.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#rating-inflationdeflation","title":"Rating Inflation/Deflation","text":"<p>Risk: Unified system might experience rating drift over time.</p> <p>Mitigation: Implement rating decay for inactive players and periodic recalibration events. Monitor rating distributions and adjust algorithms if drift exceeds acceptable thresholds (\u00b15% from baseline distributions).</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#implementation-considerations","title":"Implementation Considerations","text":"<p>This system represents a fundamental architectural change that will require careful planning and execution. Key implementation phases should include:</p> <ul> <li>Database schema migration to support unified rating system</li> <li>Unified rating system backend implementation with proper Elo update logic</li> <li>Basic cross-group matchmaking algorithm development and testing</li> <li>Multi-queue interface development for enhanced user experience</li> <li>Rating update algorithm integration with existing submission system</li> <li>Skill group transition logic with proper hysteresis mechanisms</li> <li>Advanced matchmaking algorithms optimization</li> <li>Beta testing with select groups before full rollout</li> <li>Gradual deployment across all leagues with continuous monitoring</li> </ul> <p>The complexity of this transformation requires thorough testing and gradual implementation to ensure system stability and competitive integrity throughout the transition.</p>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#success-metrics","title":"Success Metrics","text":""},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#primary-metrics","title":"Primary Metrics","text":"<ul> <li>Average Queue Time: Target 50% reduction across all skill groups</li> <li>Match Balance: 70% of matches within \u00b1100 rating points</li> <li>Player Satisfaction: 80%+ positive feedback on new system</li> <li>Cross-Group Match Rate: 25% of matches involve different skill groups</li> </ul>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#secondary-metrics","title":"Secondary Metrics","text":"<ul> <li>Skill Group Transition Stability: &lt;10% of players experience promotion/demotion cycles within 30 days</li> <li>Rating Accuracy: Predicted match outcomes align with actual results 75% of the time</li> <li>System Utilization: 90%+ of queued players find matches within maximum time limits</li> </ul>"},{"location":"departments/development/features-and-designs/sprocket-v2-unified-matchmaking-proposal/#conclusion","title":"Conclusion","text":"<p>The proposed unified matchmaking system represents a fundamental evolution in MLE's competitive infrastructure. By embracing the continuous nature of skill development and leveraging industry-proven matchmaking techniques, we can create a more engaging, efficient, and fair competitive environment for all players.</p> <p>The system's success depends on careful implementation of rating algorithms, clear communication with players, and continuous monitoring of competitive integrity. However, the potential benefits\u2014reduced queue times, improved match quality, and enhanced player development\u2014justify the significant architectural changes required.</p> <p>This transformation aligns with MLE's mission to provide the best possible competitive experience while maintaining the organizational structure that makes our leagues unique. The unified system preserves skill group identities while removing artificial barriers that currently limit competitive opportunities.</p> <p>The path forward requires careful execution, but the destination\u2014a more vibrant, accessible, and competitive MLE ecosystem\u2014is well worth the journey.</p>"},{"location":"departments/development/systems/modmail/","title":"Modmail","text":"<p>Github Repo</p>"},{"location":"departments/development/systems/modmail/#maintainers","title":"Maintainers","text":"<ul> <li>Keegabyte</li> <li>Icy_Fatal99</li> </ul> <p>Before messaging either one with questions, please reference the FAQ section of this page.</p>"},{"location":"departments/development/systems/modmail/#appropriation","title":"Appropriation","text":"<p>MLE Modmail is a custom implementation of Dragory's Modmail Bot with added customization and dockerfiles for MLE specifically.</p>"},{"location":"departments/development/systems/modmail/#setup","title":"Setup","text":"<p>Please reference the Modmail Readme for setup instructions.</p>"},{"location":"departments/development/systems/modmail/#faq","title":"FAQ","text":""},{"location":"departments/development/systems/modmail/#how-do-i-use-this","title":"How do I use this?","text":"<p>You can see a list of commands for this bot here.</p>"},{"location":"departments/development/systems/modmail/#this-is-brand-new-but-looks-a-lot-like-the-old-one-are-the-logs-and-snippets-saved","title":"This is brand new but looks a lot like the old one, are the logs and snippets saved?","text":"<p>No, unfortunately. Due to the nature of how the old bot was hosted, we were unable to save the old logs and message snippets. You will have to make new snippets.</p>"},{"location":"departments/development/systems/modmail/#are-we-at-risk-of-data-loss-in-the-future","title":"Are we at risk of data loss in the future?","text":"<p>After update 1.0.1, we made that pretty much impossible. If there is data loss, we will have much bigger problems.</p>"},{"location":"departments/development/systems/modmail/#known-issues","title":"Known Issues","text":""},{"location":"departments/development/systems/modmail/#knex-crash-unable-to-contact-internal-db","title":"Knex Crash: Unable to contact internal db","text":"<ul> <li>Cause: Currently unknown.</li> <li>Quick Fix: Restart container.</li> </ul>"},{"location":"departments/development/systems/modmail/#catastrophic-loss-rebuild-procedures","title":"Catastrophic Loss Rebuild Procedures","text":"<p>The following instructions are in place in case the working bot, docker container, docker image, and Github repo are all somehow deleted simultaneously.</p> <ol> <li>Clone Dragory's Modmail Bot (see above).</li> <li>Follow the setup procedures listed in the bot's documentation. Edit the config file as stated, except add the following to the optional section: </li> </ol> <pre><code>allowMove = on\nbotMentionResponse = Thanks for pinging our mailbox bot. You can contact staff by DMing me. Our Community team will receive your message and route it to the proper department.\ncategoryAutomation.newThread = 632480538527137812\ncloseMessage = This thread has been closed.\nrolesInThreadHeader = on\n</code></pre> <ol> <li>Creat a standard Dockerfile and .dockerignore file in the bot's root directory.</li> <li>Upload the new build to a Github repo. Copy the URL.</li> <li><code>bash    git clone [URL]</code></li> <li>cd to the bot's new directory (will likely be \"modmail\")</li> <li><code>bash    # Build the docker image    docker build -t mle-modmail .</code></li> <li><code>bash    # Create the docker container with persistent storage    docker run -d \\    --name mle-modmail-bot \\    -v $(pwd)/logs:/usr/src/app/logs \\    -v $(pwd)/db:/usr/src/app/db \\    -v $(pwd)/attachments:/usr/src/app/attachments \\    -v $(pwd)/config.ini:/usr/src/app/config.ini:ro \\    --restart unless-stopped \\    mle-modmail</code></li> <li><code>bash    # Ensure the container is running    docker logs mle-modmail-bot</code></li> </ol>"},{"location":"departments/development/systems/sprocket-identity-system/","title":"Sprocket Identity System","text":""},{"location":"departments/development/systems/sprocket-identity-system/#overview","title":"Overview","text":"<p>Sprocket's identity system can be complex due to its multi-layered structure, which sometimes leads to errors like mismatched player assignments in scrims. This complexity arises from historical design decisions and migrations. Understanding the interplay between the different identity types is crucial, as each part (identity) depends on the broader context of the system's architecture and legacy integrations.</p> <p>The system currently involves four main types of identities for users, reflecting Sprocket's evolution from supporting multiple organizations and games to its current state. Common errors, such as using the wrong ID type during account reporting, can result in issues like incorrect player mappings. These errors highlight the need for precise handling of IDs.</p> <p>Below, we distill key learnings from an investigation into such errors, expand on each identity type with examples and implications, and note planned simplifications in Sprocket V2.</p>"},{"location":"departments/development/systems/sprocket-identity-system/#key-learnings-from-investigations","title":"Key Learnings from Investigations","text":"<ul> <li>Common Error: Mismatched Player Assignments</li> <li>In certain scrims, the system throws a \"mismatched player\" error because an account is incorrectly linked to the wrong player.</li> <li>Root Cause: Support personnel often mistakenly use a Sprocket Member ID instead of the Sprocket User ID when reporting accounts via mutations. This is a recurring issue across various incidents.</li> <li>Implication: This error disrupts scrim processing and requires manual investigation using SQL queries to trace IDs across tables (e.g., joining sprocket.user, user_authentication_account, mledb.player, sprocket.member, and sprocket.player).</li> <li> <p>Prevention: Always verify and use the correct ID type (User ID) for reporting. Document and train on ID distinctions to reduce errors.</p> </li> <li> <p>Historical Context</p> </li> <li>The system's complexity stems from Sprocket v1's design for multiple leagues/organizations sharing a database (e.g., different esports organizations).</li> <li>Incomplete migration of legacy bots (Emilio and Emilia) led to embedding the old MLEDB into the new system, creating redundant player tables.</li> <li>This results in users having multiple IDs, increasing confusion and error potential, especially for players who participate in multiple games like Rocket League (RL) and TrackMania (TM).</li> </ul>"},{"location":"departments/development/systems/sprocket-identity-system/#identity-types-explained","title":"Identity Types Explained","text":"<p>Here, we expand on the four identity types, providing practical examples and implications based on the system's hermeneutic structure\u2014where understanding one type requires context from the others, and the whole system emerges from their interconnections.</p> <ol> <li>Sprocket User</li> <li>Description: The foundational identity for any platform user. It's tied to login credentials and represents a single person across the system.</li> <li>Example: When logging in to Sprocket, your User ID (e.g., from the <code>sprocket.user</code> table) is used for authentication.</li> <li>Implications: This is the core, unique identifier. Errors often occur when it's confused with derived IDs like Member or Player. In mismatch errors, the User ID should be used for accurate mapping via Discord ID linkages.</li> <li> <p>Practical Note: Every user has exactly one Sprocket User ID, making it the \"whole\" from which other identities branch.</p> </li> <li> <p>Sprocket Member</p> </li> <li>Description: An outdated concept from v1, representing a User attached to a specific organization (e.g., a league or development org).</li> <li>Example: A user might have multiple Member IDs if involved in different organizations. In queries, this is represented as <code>sprocketMemberId</code>.</li> <li>Implications: This layer adds unnecessary complexity in a single-org setup. Misusing Member IDs (e.g., in reporting) causes mismatches, leading to incorrect linkages.</li> <li> <p>Practical Note: Users can have multiple Members, but in current usage, it's often one-to-one with Users for most people.</p> </li> <li> <p>Sprocket Player</p> </li> <li>Description: A Member attached to a specific game (e.g., Rocket League or TrackMania).</li> <li>Example: A player participating in multiple games would have separate Player IDs for each, such as one for RL (<code>sprocketPlayerId</code>) and one for TM, each linked to their Member ID.</li> <li>Implications: This allows game-specific tracking but compounds ID confusion. In multi-game scenarios, ensuring the correct Player ID is used is vital to avoid errors like mismatched scrims.</li> <li> <p>Practical Note: Multiple Players per Member reflect game diversity, but this can lead to fragmentation if not managed carefully.</p> </li> <li> <p>MLEDB Player</p> </li> <li>Description: A legacy artifact from migrating the old MLE database. It's a separate table for RL players only, maintained alongside Sprocket's systems.</li> <li>Example: In queries, this is represented as <code>mledbPlayerId</code> and associated names. Mismatches occur when legacy entries don't align with Sprocket's IDs.</li> <li>Implications: This duplication requires dual maintenance and is prone to desynchronization, where legacy data overrides correct information. It's RL-specific, adding asymmetry.</li> <li>Practical Note: Only relevant for RL; it's a historical \"bolt-on\" that complicates the overall identity circle.</li> </ol>"},{"location":"departments/development/systems/sprocket-identity-system/#future-improvements-in-sprocket-v2","title":"Future Improvements in Sprocket V2","text":"<p>To address this complexity, Sprocket V2 plans significant simplifications: - Remove Multi-Org Support: Eliminate the Member concept entirely, as the system will focus on single-org databases. - Integrate Legacy DB: Fully migrate Emilio and Emilia into core Sprocket, removing MLEDB and its separate player table. - Expected Outcome: Reduces IDs per person, decreasing dimensionality and confusion. Users will still have multiple IDs (e.g., for different games), but the system will be streamlined. - Implications: This will make error-prone processes like account reporting more straightforward and reduce incidents like mismatched players.</p> <p>For troubleshooting, refer to example SQL queries for tracing IDs. If issues persist, consult the Development team.</p> <p>```mermaid graph TD     A[Sprocket User] --&gt;|Attached to Org| B[Sprocket Member]     B --&gt;|Attached to Game| C[Sprocket Player]     A --&gt;|Legacy Mapping| D[MLEDB Player]     subgraph \"Current Complexity\"         B         C         D     end     subgraph \"V2 Simplification\"         A --&gt; C     end</p>"},{"location":"departments/development/systems/systems-overview/","title":"Systems Overview","text":"<p>This document provides an overview of the various software systems used in MLE's day-to-day operations.</p>"},{"location":"departments/development/systems/systems-overview/#sprocket","title":"Sprocket","text":"<ul> <li>Product Owner: Nigel Thornbrake</li> <li>Description: A multi-feature Discord bot for managing MLE servers.</li> <li>Repository: github.com/Sprocketbot/sprocket</li> </ul>"},{"location":"departments/development/systems/systems-overview/#sprocketdb","title":"SprocketDB","text":"<ul> <li>Product Owner: Nigel Thornbrake</li> <li>Description: The database backend for Sprocket.</li> </ul>"},{"location":"departments/development/systems/systems-overview/#emilio-and-emilia-bots","title":"Emilio and Emilia Bots","text":"<ul> <li>Product Owner: Nigel Thornbrake</li> <li>Description: Discord bots for various tasks within the MLE community.</li> <li>Repository: github.com/minor-league-esports/MLEDB</li> </ul>"},{"location":"departments/development/systems/systems-overview/#mle-website","title":"MLE Website","text":"<ul> <li>Product Owners: Fatality, Mallow</li> <li>Description: The official website for Minor League Esports.</li> <li>URL: mlesports.gg</li> </ul>"},{"location":"departments/development/systems/systems-overview/#trackmania-league-operations","title":"Trackmania League Operations","text":"<ul> <li>Project Manager: Smallant</li> <li>Contributors: Keegabyte (Keegan Scott), ScaryBadger245</li> <li>Description: The collection of applications and databases that make the Trackmania league run.</li> </ul>"},{"location":"departments/development/systems/systems-overview/#mlebb","title":"MLEBB","text":"<ul> <li>Product Owner: Nigel Thornbrake</li> <li>Description: A Discord bot for MLE's fantasy league.</li> <li>Repository: github.com/minor-league-esports/MLEBB</li> </ul>"},{"location":"departments/development/systems/systems-overview/#harmony","title":"Harmony","text":"<ul> <li>Project Owner: AptFox</li> <li>Description: A scheduling platform currently in development.</li> <li>Status: In Development</li> </ul>"},{"location":"departments/development/systems/systems-overview/#modmail","title":"Modmail","text":"<ul> <li>Project Owner: Keegabyte (Keegan Scott)</li> <li>Contributors: Icy_Fatal99</li> <li>Description: The primary contact for the general MLE community to contact the staff.</li> <li>Status: Completed!</li> <li>Knowledge Base Page: Modmail</li> </ul>"},{"location":"departments/development/systems/systems-overview/#evidence","title":"Evidence","text":"<ul> <li>Project Owners: Ol Dirty Dirty, OwnerOfTheWhiteSedan</li> <li>Description: The primary data endpoint for MLE.</li> <li>Repository: https://github.com/Minor-League-Esports/evidence</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/","title":"Sprocket Production Infrastructure Documentation","text":"<p>Created: November 8, 2025 Status: Complete and Ready for Use Purpose: Comprehensive documentation for the Sprocket infrastructure project</p>"},{"location":"departments/development/systems/production-infrastructure/#overview","title":"Overview","text":"<p>This section contains comprehensive documentation for the Sprocket production infrastructure deployment. It was created as the culmination of an 8-week infrastructure rebuild project (September 14 - November 8, 2025).</p>"},{"location":"departments/development/systems/production-infrastructure/#what-is-sprocket","title":"What is Sprocket?","text":"<p>Sprocket is a comprehensive platform for competitive Rocket League esports leagues. It provides the infrastructure and tools to run organized competitive gaming leagues similar to traditional sports leagues.</p> <p>Key Capabilities: - Player registration and profile management - League and team management - Automated matchmaking and scheduling - Match result processing from replay files - Discord integration for notifications and interactions - Statistics tracking and leaderboards</p>"},{"location":"departments/development/systems/production-infrastructure/#infrastructure-scale","title":"Infrastructure Scale","text":"<p>As of November 2025: - Users: Hundreds of active players - Concurrent Users: &lt;1,000 (current capacity) - Uptime: 99.9%+ (less than 9 hours downtime per year) - Response Time: &lt;500ms for most requests - Infrastructure Cost: $131/month</p>"},{"location":"departments/development/systems/production-infrastructure/#documentation-navigation","title":"Documentation Navigation","text":""},{"location":"departments/development/systems/production-infrastructure/#getting-started","title":"Getting Started","text":"<p>New to the project? Start here:</p> <ol> <li>README - Quick package overview</li> <li>Context Summary (15 min) - Current infrastructure state</li> <li>Architecture (60 min) - System design and architecture</li> <li>Postmortem (45 min) - Complete project history and lessons learned</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/#deployment-operations","title":"Deployment &amp; Operations","text":"<p>Deploying or maintaining the infrastructure:</p> <ul> <li>Deployment Guide - Step-by-step deployment instructions</li> <li>Operations Runbook - Day-to-day operations manual</li> <li>Externalities and Dependencies - External systems and prerequisites</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#reference-deep-dives","title":"Reference &amp; Deep Dives","text":"<p>Technical details and problem-solving:</p> <ul> <li>Technical Challenges - Major challenges and solutions</li> <li>Git History Analysis - Commit-by-commit journey</li> <li>Glossary - Technical terms and definitions</li> <li>Claude Conversations Summary - AI-assisted development insights</li> <li>Technical Challenges from Claude Conversations - Additional problem-solving details</li> <li>Documentation Review Recommendations - Documentation improvement suggestions</li> <li>Transfer Instructions - Guide for transferring this documentation</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#quick-reference","title":"Quick Reference","text":""},{"location":"departments/development/systems/production-infrastructure/#architecture-overview","title":"Architecture Overview","text":"<p>The infrastructure uses a three-layer architecture:</p> <ul> <li>Layer 1: Core Infrastructure (Traefik, Vault, Socket Proxy)</li> <li>Layer 2: Data Services (Redis, RabbitMQ, PostgreSQL, InfluxDB)</li> <li>Platform: Application Services (Web, API, Discord Bot, Microservices)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#technology-stack","title":"Technology Stack","text":"<ul> <li>Orchestration: Docker Swarm</li> <li>Reverse Proxy: Traefik</li> <li>Secrets Management: Vault + Doppler</li> <li>Database: PostgreSQL (Digital Ocean Managed)</li> <li>Cache: Redis</li> <li>Message Queue: RabbitMQ</li> <li>Storage: Digital Ocean Spaces (S3-compatible)</li> <li>Monitoring: Grafana + InfluxDB</li> <li>Infrastructure as Code: Pulumi</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#critical-external-dependencies","title":"Critical External Dependencies","text":"<ol> <li>Doppler - Secrets management</li> <li>GitHub Organization - Vault authentication</li> <li>Digital Ocean - Managed database and storage</li> <li>DNS Provider - Let's Encrypt certificates</li> <li>Pulumi Backend - Infrastructure state management</li> <li>Docker Hub - Private image repository</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/#key-project-outcomes","title":"Key Project Outcomes","text":""},{"location":"departments/development/systems/production-infrastructure/#what-we-built","title":"What We Built","text":"<p>Over 8 weeks (September 14 - November 8, 2025), we:</p> <ol> <li>Completely rebuilt the infrastructure from scratch</li> <li>Adopted Infrastructure as Code (Pulumi)</li> <li>Implemented proper secrets management (Vault)</li> <li>Set up comprehensive monitoring (Grafana, InfluxDB)</li> <li>Migrated to managed services (PostgreSQL, S3)</li> <li>Documented everything comprehensively</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/#results-achieved","title":"Results Achieved","text":"<ul> <li>Uptime: 99.9%+ (vs. frequent downtime before)</li> <li>Recovery Time: ~6 hours (vs. days/weeks before)</li> <li>Deployment: Repeatable and automated</li> <li>Documentation: Comprehensive and production-ready</li> <li>Team Confidence: Can operate and maintain the platform</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Use managed services for critical infrastructure</li> <li>Design for production first, add dev overrides later</li> <li>Automate secret provisioning to reduce human error</li> <li>Document decisions when making them, not after</li> <li>Test each layer independently before stacking</li> <li>Git history is invaluable for understanding evolution</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/#documentation-statistics","title":"Documentation Statistics","text":"<ul> <li>Total Documentation: ~250 KB, 1,800+ lines</li> <li>Total Pages: ~150 pages (if printed)</li> <li>Total Diagrams: 12 ASCII diagrams</li> <li>Code Examples: 100+ code blocks</li> <li>Time to Create: ~8 hours of focused work</li> <li>Based On: 20+ commits analyzed, 8 weeks of project history</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/#getting-help","title":"Getting Help","text":"<p>If you have questions about this documentation:</p> <ol> <li>Check the specific document - Each has detailed sections</li> <li>Review Technical Challenges - Common issues documented</li> <li>Ask the team - #infrastructure Slack channel</li> <li>Update the docs - Found something missing? Add it!</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/#license-usage","title":"License &amp; Usage","text":"<p>This documentation is for internal use by the Sprocket team and authorized personnel.</p> <p>Do Not Share: - Connection strings, credentials, API tokens - Internal IPs, proprietary architecture details</p> <p>OK to Share: - General architecture patterns - Lessons learned, best practices - Problem-solving approaches</p> <p>Documentation Package Version: 1.0 Created: November 8, 2025 Status: Complete and Production-Ready Next Review: February 2026</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/","title":"Sprocket Infrastructure Documentation Package","text":"<p>Created: November 8, 2025 Status: Complete and Ready for Use Purpose: Comprehensive documentation for the Sprocket infrastructure project</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#whats-in-this-package","title":"\ud83d\udce6 What's in This Package","text":"<p>This documentation package contains everything needed to understand, deploy, operate, and maintain the Sprocket infrastructure. It was created as the culmination of an 8-week infrastructure rebuild project.</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#what-is-sprocket","title":"\ud83c\udfae What is Sprocket?","text":"<p>For Those New to the Project</p> <p>Sprocket is a comprehensive platform for competitive Rocket League esports leagues. It provides the infrastructure and tools to run organized competitive gaming leagues similar to traditional sports leagues.</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#what-sprocket-does","title":"What Sprocket Does","text":"<p>For Players: - Register and Create Profiles: Link gaming accounts (Epic Games, Steam, Xbox, PlayStation) - Join Leagues and Teams: Find teams, join competitive leagues, track player stats - Compete in Matches: Scheduled matches with proper matchmaking and rankings - Track Performance: View statistics, ELO ratings, match history, and leaderboards - Integrate with Discord: Receive notifications, submit scores, check schedules without leaving Discord</p> <p>For League Administrators: - Manage Leagues: Create seasons, configure rules, set match schedules - Matchmaking: Automated team matching based on skill ratings - Results Processing: Parse Rocket League replay files automatically, verify match results - Analytics: Track league health, player engagement, match completion rates - Communication: Automated Discord notifications for matches, results, and announcements</p> <p>For Spectators: - Browse Matches: View upcoming and past matches - Player Stats: Research players and team performance - Leaderboards: See top-ranked players and teams - Match Replays: Download and watch competitive match replays</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#the-business-value","title":"The Business Value","text":"<p>Problem Sprocket Solves: Before Sprocket, running a Rocket League league required: - Manual player registration and team management (spreadsheets) - Manual match scheduling and coordination (Discord messages) - Manual result verification (watching replays, checking screenshots) - Manual stat tracking (spreadsheets and databases) - Fragmented tools (different systems for each task)</p> <p>Sprocket's Solution: - All-in-one Platform: Single integrated system for everything - Automation: Replay parsing, matchmaking, notifications handled automatically - Discord Integration: 70% of interactions happen via Discord bot (where players already are) - Scalability: Support hundreds of players across multiple leagues simultaneously - Data-Driven: Analytics and insights for league administrators</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#the-tech-stack-simplified","title":"The Tech Stack (Simplified)","text":"<p>Frontend (What users see): - Web application built with Next.js (React) - Discord bot for in-Discord interactions - GraphQL API for data access</p> <p>Backend (What makes it work): - Microservices architecture (small, specialized services) - PostgreSQL database (player data, matches, teams, stats) - Redis cache (fast data access, reduce database load) - RabbitMQ message queue (asynchronous processing) - S3 storage (replay files, generated images)</p> <p>Infrastructure (What this documentation is about): - Docker Swarm (container orchestration) - Traefik (reverse proxy, routing, HTTPS) - Vault (secrets management) - Grafana/InfluxDB (monitoring and metrics) - Digital Ocean (hosting provider)</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#current-scale","title":"Current Scale","text":"<p>As of November 2025: - Users: Hundreds of active players - Concurrent Users: &lt;1,000 (current capacity) - Matches: Thousands of matches recorded - Uptime: 99.9%+ (less than 9 hours downtime per year) - Response Time: &lt;500ms for most requests - Cost: $131/month infrastructure</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#who-uses-sprocket","title":"Who Uses Sprocket?","text":"<p>Primary Audience: - Competitive Rocket League players looking for organized leagues - League administrators running community tournaments - Team captains managing rosters and schedules</p> <p>Geographic Distribution: - Primarily North American players - Some European players - Global community via Discord</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#why-this-documentation-exists","title":"Why This Documentation Exists","text":"<p>The Context: In September 2025, the Sprocket infrastructure was in a fragile state: - Manual deployment processes - Frequent downtime - Difficult to maintain - Missing critical services - No disaster recovery plan</p> <p>The Solution: Over 8 weeks (September 14 - November 8, 2025), we: 1. Completely rebuilt the infrastructure from scratch 2. Adopted Infrastructure as Code (Pulumi) 3. Implemented proper secrets management (Vault) 4. Set up comprehensive monitoring (Grafana, InfluxDB) 5. Documented everything (this package)</p> <p>The Result: - 99.9%+ uptime - ~6 hour recovery time (vs. days/weeks before) - Repeatable deployments - Comprehensive documentation - Team can confidently operate and maintain the platform</p> <p>This documentation package is that knowledge captured - everything needed to understand, deploy, operate, and maintain Sprocket's infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#documentation-files","title":"\ud83d\udcda Documentation Files","text":""},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#1-postmortemmd-41-kb","title":"1. POSTMORTEM.md (41 KB)","text":"<p>The complete project story from start to finish</p> <p>What's Inside: - Executive summary of the 8-week rebuild - Detailed timeline (September 14 - November 8, 2025) - Six major phases of the project - Deep dives into the 7 biggest technical challenges - Architecture evolution (before/after) - Key decisions and their rationale - What went well and what could be better - Lessons learned for future projects - 20+ recommendations</p> <p>Best For: - Understanding the \"why\" behind architectural decisions - Learning from mistakes and successes - New team members getting up to speed - Planning similar infrastructure projects</p> <p>Time to Read: 45-60 minutes</p> <p>Key Takeaways: - Use managed services for critical infrastructure - Design for production first, add dev overrides later - Automate secret provisioning to reduce human error - Document decisions when making them, not after - Git history is invaluable for understanding evolution</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#2-architecturemd-52-kb","title":"2. ARCHITECTURE.md (52 KB)","text":"<p>Complete architecture reference and design documentation</p> <p>What's Inside: - High-level architecture overview with diagrams - Three-layer architecture detailed breakdown   - Layer 1: Infrastructure (Traefik, Vault, Socket Proxy)   - Layer 2: Data Services (Redis, RabbitMQ, InfluxDB, etc.)   - Platform: Application Services (Web, API, Bot, Microservices) - Network topology (5 overlay networks) - Complete service catalog (22 services) - Data flow diagrams - Security architecture - Scalability considerations - Future architectural improvements</p> <p>Best For: - Understanding how everything fits together - Troubleshooting connectivity issues - Planning infrastructure changes - Onboarding new developers - Architecture reviews</p> <p>Time to Read: 60-90 minutes</p> <p>Contains: - 7 ASCII diagrams - 22 service descriptions - Network topology maps - Security architecture details - Scaling strategies</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#3-deployment_guidemd-64-kb","title":"3. DEPLOYMENT_GUIDE.md (64 KB)","text":"<p>Step-by-step guide to deploy from scratch</p> <p>What's Inside: - Complete prerequisites checklist - Pre-deployment configuration (all 6 external dependencies) - Layer-by-layer deployment instructions   - Layer 1: Traefik, Vault, Socket Proxy   - Secret provisioning via Vault   - Layer 2: All data services   - Platform: All application services - Verification procedures (automated + manual) - Comprehensive troubleshooting section - Rollback procedures</p> <p>Best For: - First-time deployment - Disaster recovery - Setting up staging environment - Understanding deployment flow - Training new ops team members</p> <p>Time to Complete: 4-6 hours (first time), 2-3 hours (experienced)</p> <p>Includes: - 15+ code examples - Expected outputs for each step - 6 common issue resolutions - Emergency rollback procedures</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#4-operations_runbookmd-48-kb","title":"4. OPERATIONS_RUNBOOK.md (48 KB)","text":"<p>Day-to-day operations manual</p> <p>What's Inside: - Daily operations (morning health check) - Weekly review procedures - Service management (start, stop, restart, update, rollback) - Monitoring &amp; alerting setup - Backup &amp; recovery procedures   - Database backups (automated + manual)   - Vault backups   - Pulumi state backups   - Disaster recovery testing - Scaling operations (vertical + horizontal) - Security operations (secret rotation, access review) - Incident response (P0-P3 severity levels) - Maintenance procedures - Common tasks reference</p> <p>Best For: - Daily operations - On-call engineers - Incident response - Maintenance planning - Training new ops team members</p> <p>Time to Read: 90-120 minutes (reference document, read sections as needed)</p> <p>Contains: - Daily checklist (10 minutes) - Weekly review checklist (30 minutes) - 20+ operational procedures - Incident response playbook - Emergency contacts template</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#5-context_summarymd-10-kb","title":"5. CONTEXT_SUMMARY.md (10 KB)","text":"<p>Quick reference for current state</p> <p>What's Inside: - Current deployment status - Architecture overview - Key infrastructure decisions - Critical external dependencies - Configuration reference - Major service listing - Known issues and gotchas</p> <p>Best For: - Quick orientation - Reference during troubleshooting - Understanding current state</p> <p>Time to Read: 15-20 minutes</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#6-technical_challengesmd-17-kb","title":"6. TECHNICAL_CHALLENGES.md (17 KB)","text":"<p>Detailed problem/solution analysis</p> <p>What's Inside: - 7 major technical challenges with detailed analysis - Evolution of solutions through commits - Patterns and best practices discovered - \"What we'd do differently\" section - Critical success factors</p> <p>Best For: - Learning from past issues - Troubleshooting similar problems - Understanding why things are done a certain way</p> <p>Time to Read: 30-45 minutes</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#7-git_history_analysismd-12-kb","title":"7. GIT_HISTORY_ANALYSIS.md (12 KB)","text":"<p>Commit-by-commit journey</p> <p>What's Inside: - Complete timeline from Sept 14 to Nov 8, 2025 - Commit-by-commit breakdown by phase - Key patterns in commit history - Commit message quality analysis - Timeline visualization - Git best practices learned</p> <p>Best For: - Understanding project evolution - Learning from iteration - Seeing what was tried and failed</p> <p>Time to Read: 20-30 minutes</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#8-externalities_and_dependenciesmd-16-kb","title":"8. EXTERNALITIES_AND_DEPENDENCIES.md (16 KB)","text":"<p>External systems documentation</p> <p>What's Inside: - 6 critical external dependencies   - Doppler (secrets management)   - GitHub Organization (Vault auth)   - Digital Ocean (database, storage)   - DNS Provider (Let's Encrypt)   - Pulumi Backend (state management)   - Docker Hub (private images) - Required CLI tools - Network requirements - Complete access checklist - Dependency graph - Cost considerations - Security considerations - Disaster recovery for external dependencies</p> <p>Best For: - Setting up new deployment - Understanding prerequisites - Cost planning - Access provisioning</p> <p>Time to Read: 25-35 minutes</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#how-to-use-this-documentation","title":"\ud83d\uddfa\ufe0f How to Use This Documentation","text":""},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#for-new-team-members","title":"For New Team Members","text":"<p>Recommended Reading Order: 1. START HERE \u2190 You are here! 2. <code>CONTEXT_SUMMARY.md</code> (15 min) - Get oriented 3. <code>ARCHITECTURE.md</code> (60 min) - Understand the system 4. <code>POSTMORTEM.md</code> (45 min) - Learn the history 5. <code>DEPLOYMENT_GUIDE.md</code> (skim) - Know how to deploy 6. <code>OPERATIONS_RUNBOOK.md</code> (skim) - Know where to look for ops tasks</p> <p>Total Time: ~2-3 hours of focused reading</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#for-deployment","title":"For Deployment","text":"<p>Reading Order: 1. <code>EXTERNALITIES_AND_DEPENDENCIES.md</code> - Verify prerequisites 2. <code>DEPLOYMENT_GUIDE.md</code> - Follow step-by-step 3. <code>OPERATIONS_RUNBOOK.md</code> - Post-deployment verification</p> <p>Total Time: 4-6 hours (includes deployment)</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#for-operations","title":"For Operations","text":"<p>Daily Reference: 1. <code>OPERATIONS_RUNBOOK.md</code> - Daily checklist and procedures 2. <code>TECHNICAL_CHALLENGES.md</code> - When troubleshooting 3. <code>ARCHITECTURE.md</code> - When understanding connectivity</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#for-planningdecision-making","title":"For Planning/Decision Making","text":"<p>Reading Order: 1. <code>POSTMORTEM.md</code> - Learn from past decisions 2. <code>ARCHITECTURE.md</code> - Understand current state 3. <code>EXTERNALITIES_AND_DEPENDENCIES.md</code> - Understand constraints</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#for-troubleshooting","title":"For Troubleshooting","text":"<p>Quick Reference: 1. <code>OPERATIONS_RUNBOOK.md</code> \u2192 Incident Response section 2. <code>TECHNICAL_CHALLENGES.md</code> \u2192 Similar issues 3. <code>DEPLOYMENT_GUIDE.md</code> \u2192 Troubleshooting section 4. <code>ARCHITECTURE.md</code> \u2192 Service catalog</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#documentation-statistics","title":"\ud83d\udcca Documentation Statistics","text":"<p>Total Documentation: ~250 KB, 1,800+ lines Total Pages: ~150 pages (if printed) Total Diagrams: 12 ASCII diagrams Code Examples: 100+ code blocks Time to Create: ~8 hours of focused work Based On: - 20+ commits analyzed - 8 weeks of project history - 70+ files in codebase - 22 deployed services</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#documentation-goals-achieved","title":"\ud83c\udfaf Documentation Goals Achieved","text":"<ul> <li>\u2705 Comprehensive Postmortem: Complete project story with lessons learned</li> <li>\u2705 Architecture Documentation: Detailed system design and rationale</li> <li>\u2705 Deployment Guide: Step-by-step instructions for rebuild</li> <li>\u2705 Operations Runbook: Day-to-day operational procedures</li> <li>\u2705 All diagrams in text: ASCII diagrams work in all editors</li> <li>\u2705 Self-contained: No external dependencies</li> <li>\u2705 Ready for MkDocs: Can be imported into documentation site</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#whats-not-in-this-package","title":"\ud83d\udcdd What's NOT in This Package","text":"<p>Intentionally Excluded: - Actual secrets or credentials (see Doppler/Vault) - Proprietary business logic - Application code documentation (separate repo) - Automated runbooks/scripts (see main repo) - Screenshots (text-based documentation only)</p> <p>Future Additions (recommended): - MkDocs site with search - Interactive architecture diagrams - Video walkthroughs - Automated runbook scripts - Prometheus/Grafana dashboard configs</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#how-to-keep-this-updated","title":"\ud83d\udd04 How to Keep This Updated","text":""},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#when-to-update","title":"When to Update","text":"<p>Update Documentation When: - Major architecture changes - New services added - External dependencies change - Lessons learned from incidents - Deployment procedures change - New operational procedures</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#how-to-update","title":"How to Update","text":"<ol> <li>Edit Markdown Files: All docs are standard Markdown</li> <li>Keep Diagrams Simple: Use ASCII art for diagrams</li> <li>Update Version Numbers: Update \"Last Updated\" dates</li> <li>Test Procedures: Verify accuracy before publishing</li> <li>Commit Changes: Use git for version control</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#document-ownership","title":"Document Ownership","text":"Document Owner Review Cycle POSTMORTEM.md Team Lead After major milestones ARCHITECTURE.md Architect Quarterly DEPLOYMENT_GUIDE.md Ops Lead After any deployment changes OPERATIONS_RUNBOOK.md Ops Team Monthly Other docs Team Lead Quarterly"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#next-steps","title":"\ud83d\ude80 Next Steps","text":""},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#immediate-within-1-week","title":"Immediate (Within 1 Week)","text":"<ol> <li>\u2705 Review all documentation</li> <li>\u2705 Share with team</li> <li>\u2705 Add to team wiki/knowledge base</li> <li>\u2705 Create MkDocs site (optional)</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#short-term-within-1-month","title":"Short-term (Within 1 Month)","text":"<ol> <li>\ud83d\udcdd Add screenshots/diagrams to MkDocs</li> <li>\ud83d\udcdd Create video walkthrough of deployment</li> <li>\ud83d\udcdd Test deployment guide with new team member</li> <li>\ud83d\udcdd Create quick reference card</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#long-term-within-3-months","title":"Long-term (Within 3 Months)","text":"<ol> <li>\ud83d\udcdd Automate documentation updates</li> <li>\ud83d\udcdd Add interactive elements</li> <li>\ud83d\udcdd Create disaster recovery playbook</li> <li>\ud83d\udcdd Document advanced troubleshooting</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#how-to-import-into-mkdocs","title":"\ud83d\udca1 How to Import into MkDocs","text":"<pre><code># 1. Create MkDocs project\npip install mkdocs mkdocs-material\nmkdocs new sprocket-docs\ncd sprocket-docs\n\n# 2. Copy documentation\nmkdir -p docs/postmortem docs/architecture docs/deployment docs/operations docs/reference\ncp ../sprocket-infra/docs-output/POSTMORTEM.md docs/postmortem/index.md\ncp ../sprocket-infra/docs-output/ARCHITECTURE.md docs/architecture/index.md\ncp ../sprocket-infra/docs-output/DEPLOYMENT_GUIDE.md docs/deployment/index.md\ncp ../sprocket-infra/docs-output/OPERATIONS_RUNBOOK.md docs/operations/index.md\ncp ../sprocket-infra/docs-output/TECHNICAL_CHALLENGES.md docs/reference/challenges.md\ncp ../sprocket-infra/docs-output/GIT_HISTORY_ANALYSIS.md docs/reference/history.md\ncp ../sprocket-infra/docs-output/EXTERNALITIES_AND_DEPENDENCIES.md docs/reference/dependencies.md\ncp ../sprocket-infra/docs-output/CONTEXT_SUMMARY.md docs/index.md\n\n# 3. Configure mkdocs.yml (see TRANSFER_INSTRUCTIONS.md)\n\n# 4. Serve locally\nmkdocs serve\n\n# 5. Build for deployment\nmkdocs build\n\n# 6. Deploy to GitHub Pages or hosting\nmkdocs gh-deploy\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#questions-or-issues","title":"\ud83d\udcde Questions or Issues?","text":"<p>If you have questions about this documentation:</p> <ol> <li>Check the specific document - Each has detailed sections</li> <li>Review TECHNICAL_CHALLENGES.md - Common issues documented</li> <li>Ask the team - #infrastructure Slack channel</li> <li>Update the docs - Found something missing? Add it!</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#acknowledgments","title":"\ud83c\udf89 Acknowledgments","text":"<p>This documentation package represents: - 8 weeks of infrastructure work - 20+ commits analyzed - Months of learning and iteration - Countless hours of debugging and problem-solving</p> <p>Special thanks to the entire team for their persistence and dedication to getting this infrastructure to production.</p>"},{"location":"departments/development/systems/production-infrastructure/00-START-HERE/#license-usage","title":"\ud83d\udcc4 License &amp; Usage","text":"<p>This documentation is for internal use by the Sprocket team and authorized personnel.</p> <p>Do Not Share: - Connection strings - Credentials - API tokens - Internal IPs - Proprietary architecture details</p> <p>OK to Share: - General architecture patterns - Lessons learned - Best practices - Problem-solving approaches</p> <p>Documentation Package Version: 1.0 Created: November 8, 2025 Status: \u2705 Complete and Production-Ready Next Review: February 2026</p> <p>Happy deploying! \ud83d\ude80</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/","title":"Sprocket Infrastructure Architecture","text":"<p>Version: 1.1 Last Updated: December 4, 2025 Status: Production</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#executive-summary","title":"Executive Summary","text":"<p>For Decision Makers &amp; Non-Technical Stakeholders</p> <p>The Sprocket platform runs on a robust, cost-effective infrastructure designed to support competitive Rocket League esports leagues. This infrastructure serves hundreds of active users with 99.9%+ uptime while keeping costs low.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#at-a-glance","title":"At a Glance","text":"Metric Value Monthly Cost ~$131/month Current Capacity Supports &lt;1,000 concurrent users Uptime Target 99.9% (less than 9 hours downtime per year) Deployment Model Single-node Docker Swarm (can scale to multi-node) Recovery Time ~6 hours to rebuild from scratch Team Size Supported 1-3 engineers can manage effectively"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#cost-breakdown","title":"Cost Breakdown","text":"Component Monthly Cost Purpose Can We Optimize? Digital Ocean Droplet $96 Main server (4 vCPUs, 8GB RAM) \u2713 Yes - can right-size based on actual usage Managed PostgreSQL $15 Production database with auto-backups \u2717 No - critical for reliability Block Storage Volumes $15 Persistent data storage ~ Maybe - review volume usage Spaces (S3 Storage) $5 File storage (images, replays, backups) ~ Maybe - cleanup old files Total $131/month $1,572/year <p>Cost Optimization Opportunities: - Droplet could potentially be reduced to $72/month tier (3 vCPUs, 6GB RAM) if usage analysis shows headroom - Block storage volumes could be consolidated if some are underutilized - Estimated potential savings: $15-30/month without impacting performance</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#business-value","title":"Business Value","text":"<p>What This Infrastructure Provides: 1. Reliability: Automatic restarts if services crash, daily database backups 2. Security: All traffic encrypted (HTTPS), secrets stored securely in Vault 3. Scalability: Can handle traffic growth by adding more servers when needed 4. Developer Velocity: Engineers can deploy updates in minutes, not hours 5. Cost Efficiency: Managed services save 10-20 hours/month vs. self-managing everything</p> <p>Alternative We Avoided: Running everything ourselves (self-hosted database, self-hosted storage) would save ~$35/month but would require an estimated 10-15 hours/month of additional maintenance work. At even a modest engineering rate, managed services are significantly more cost-effective.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#risk-assessment","title":"Risk Assessment","text":"<p>Current Risks:</p> Risk Impact Probability Mitigation Single Server Failure Complete outage until recovery (~1-6 hours) Low (hosting provider has 99.99% uptime SLA) Daily automated backups, documented recovery process Database Failure Data loss if backup fails Very Low (managed service with auto-backups) Digital Ocean handles backups, 7-day retention Cost Overrun Budget exceeded if traffic spikes Low (current usage well below capacity) Monitoring alerts, can scale down if needed Key Person Dependency Knowledge concentrated in 1-2 people Medium This documentation addresses this risk Vendor Lock-in Difficult to move from Digital Ocean Low (using standard Docker/Pulumi, can migrate) Infrastructure as Code makes migration feasible <p>Single Points of Failure (High Priority for Future): - Single server node (if it fails, everything fails) - No automated disaster recovery testing - Certificate renewal depends on Let's Encrypt being available</p> <p>Recommended Next Steps (in priority order): 1. Set up automated monitoring alerts (PagerDuty/similar) - $0-29/month 2. Test disaster recovery procedure quarterly - 4-6 hours/quarter 3. Consider multi-node setup when traffic exceeds 500 concurrent users - 2-3x current infrastructure cost</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#scaling-timeline","title":"Scaling Timeline","text":"User Load Infrastructure Needed Est. Monthly Cost When to Upgrade &lt;500 concurrent users Current setup (1 node) $131 We're here now 500-1,500 concurrent Add 1-2 worker nodes $250-350 When CPU consistently &gt;70% 1,500-5,000 concurrent 3-5 node cluster + load balancer $500-800 Traffic growth requires it 5,000+ concurrent Multi-region, auto-scaling $1,000+ Significant growth phase <p>Current Status: Infrastructure has significant headroom. Single-node setup can handle 2-3x current traffic without issues.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#for-technical-readers","title":"For Technical Readers","text":"<p>The rest of this document provides detailed technical information about: - Three-layer architecture design and rationale - Network topology and security model - Service catalog (22 services across 3 layers) - Data flows and integration patterns - Scalability considerations and growth path</p> <p>Key Terms Explained: If you encounter unfamiliar technical terms, see GLOSSARY.md for plain-language definitions.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Architecture 101: Core Concepts</li> <li>Architecture Principles</li> <li>Three-Layer Architecture</li> <li>Network Topology</li> <li>Service Catalog</li> <li>Data Flow</li> <li>Security Architecture</li> <li>Scalability Considerations</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#overview","title":"Overview","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-youre-looking-at","title":"What You're Looking At","text":"<p>If you're new to this documentation, here's the essential context:</p> <p>Sprocket is NOT a game - it's a web platform (think: website + Discord bot) for managing competitive Rocket League esports leagues. Players play Rocket League (the video game by Psyonix), but they use Sprocket to: - Register and create teams - Schedule and track matches - Submit match results and replay files - View statistics and leaderboards - Communicate via Discord integration</p> <p>This document describes the infrastructure (servers, databases, networks) that runs the Sprocket platform - NOT the Rocket League game itself. Think of it like documentation for the servers running ESPN.com, not the sports being covered.</p> <p>Key Distinction: - Rocket League = The video game (made by Psyonix, runs on players' computers/consoles) - Sprocket = The league management platform (made by us, runs on our servers, documented here)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#infrastructure-overview","title":"Infrastructure Overview","text":"<p>The Sprocket platform infrastructure is built on a three-layer architecture deployed using Docker Swarm and managed via Pulumi (Infrastructure as Code). The architecture prioritizes:</p> <ul> <li>Separation of Concerns: Clear boundaries between infrastructure, data, and applications</li> <li>Managed Services: Critical components (database, storage) use managed cloud services</li> <li>Automation: Repeatable deployments with minimal manual intervention</li> <li>Security: Secrets management via Vault, TLS everywhere</li> <li>Observability: Comprehensive monitoring and logging</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#high-level-architecture","title":"High-Level Architecture","text":"<p>The diagram above shows the overall Sprocket system architecture with four main layers: - Users &amp; Clients: Web browsers (Next.js app), Discord bot interface, and mobile devices - Core Services: Sprocket Web (Next.js), Sprocket API (NestJS/GraphQL), and Discord Bot - Microservices: Specialized background services (image generation, matchmaking, analytics, ELO, replay parsing, notifications) - Data Layer: PostgreSQL database, Redis cache, and S3 storage for files</p> <p>All services communicate through the API layer, with microservices using RabbitMQ message queue for asynchronous processing.</p> <p>For a text-based representation, see below:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PUBLIC INTERNET                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u2502 HTTPS (443)\n                         \u2502 HTTP (80) - Redirects to HTTPS\n                         \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Traefik \u2502  Layer 1: Infrastructure\n                    \u2502 (Proxy) \u2502  Reverse proxy, TLS termination,\n                    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  Let's Encrypt automation\n                         \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                \u2502                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n   \u2502  Vault  \u2502      \u2502Platform \u2502     \u2502  Layer  \u2502\n   \u2502(Secrets)\u2502      \u2502Services \u2502     \u2502   2     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2502Services \u2502\n        \u2502                \u2502           \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n        \u2502                \u2502                \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Digital \u2502          \u2502  Digital  \u2502\n         \u2502 Ocean   \u2502          \u2502  Ocean    \u2502\n         \u2502Postgres \u2502          \u2502  Spaces   \u2502\n         \u2502         \u2502          \u2502  (S3)     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#architecture-101-core-concepts","title":"Architecture 101: Core Concepts","text":"<p>For Junior Engineers and Those New to Container Infrastructure</p> <p>Before diving into our specific architecture, let's establish some foundational concepts. If you're already familiar with Docker, Pulumi, and Infrastructure as Code, feel free to skip to Architecture Principles.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-docker","title":"What is Docker?","text":"<p>Simple Explanation: Docker is like a shipping container for software. Just like physical shipping containers make it easy to move goods from ships to trucks to trains, Docker containers make it easy to move applications from your laptop to test servers to production.</p> <p>Key Concepts: - Container: A running instance of your application with everything it needs (code, dependencies, configuration) - Image: The template for creating containers (like a blueprint) - Why containers?: \"It works on my machine\" becomes \"it works everywhere\" because the container is the same environment everywhere</p> <p>Real Example: Our Sprocket Web service uses the image <code>asaxplayinghorse/sprocket-web:main</code>. When we run this image, it becomes a container serving the website on port 3000.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-docker-swarm","title":"What is Docker Swarm?","text":"<p>Simple Explanation: Docker Swarm turns multiple servers into a cluster that Docker manages as one unit. Think of it like managing a team instead of individual people - you say \"I need 3 people working on this task\" and the manager (Swarm) assigns them.</p> <p>Key Features: - Service: A definition of what to run (\"run 3 copies of the web app\") - Replica: Each copy/instance - Orchestration: Swarm automatically restarts failed containers, distributes them across servers, and handles updates</p> <p>Our Setup: Currently 1 server (node), but Swarm allows us to add more servers later without changing our application code.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-infrastructure-as-code-iac","title":"What is Infrastructure as Code (IaC)?","text":"<p>Simple Explanation: Instead of clicking through web interfaces to create servers, databases, and networks, you write code that describes what you want. The IaC tool reads your code and creates/updates everything automatically.</p> <p>Why This Matters:</p> <pre><code>Without IaC:\n\"To recreate production, first log into Digital Ocean, click...\"\n(50 steps later...)\n\"...and hope you didn't miss anything\"\n\nWith IaC:\n\"To recreate production, run: pulumi up\"\nDone. Everything exactly as defined in code.\n</code></pre> <p>Benefits: - Version Control: Infrastructure changes are tracked in git like code changes - Repeatable: Deploy the same way every time - Self-Documenting: The code shows exactly what exists - Reversible: Roll back to previous versions easily</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-pulumi","title":"What is Pulumi?","text":"<p>Simple Explanation: Pulumi is our IaC tool. We write TypeScript code describing our infrastructure (services, networks, volumes), and Pulumi makes it happen.</p> <p>Example:</p> <pre><code>// This code creates a Redis service\nnew docker.Service(\"redis\", {\n  image: \"redis:6-alpine\",\n  replicas: 1,\n  networks: [ingressNetwork.id],\n  ports: [{ targetPort: 6379 }]\n});\n</code></pre> <p>Pulumi reads this, compares it to what currently exists, and creates/updates/deletes resources to match.</p> <p>Why Pulumi vs. Others?: We chose Pulumi over alternatives like Terraform because it lets us use a real programming language (TypeScript) instead of a domain-specific language. This means better IDE support, type checking, and ability to use familiar programming patterns.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-are-layers","title":"What Are Layers?","text":"<p>Simple Explanation: We organize our infrastructure into three layers, where each layer depends on the previous one, like floors in a building:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Platform (Floor 3)          \u2502 \u2190 Applications (web, API, bot)\n\u2502  Depends on: Layer 2, Layer 1\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Layer 2 (Floor 2)           \u2502 \u2190 Data services (Redis, databases)\n\u2502  Depends on: Layer 1         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Layer 1 (Foundation)        \u2502 \u2190 Core infrastructure (networking, secrets)\n\u2502  Depends on: nothing         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why Layers?: 1. Dependency Management: Layer 2 can't work without Layer 1, so we deploy Layer 1 first 2. Independent Updates: Can update Layer 2 without touching Layer 1 3. Logical Organization: Related services grouped together 4. Easier Troubleshooting: If something breaks, check the layer it's in and the layer below</p> <p>Real Example: When the web app needs to cache data: 1. Web app (Platform layer) talks to Redis (Layer 2) 2. Redis talks to Vault (Layer 1) to get its password 3. All traffic goes through Traefik (Layer 1) to reach the web app</p> <p>The layers create clear boundaries: Platform knows about Layer 2 services, but Layer 2 doesn't know about Platform services.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-are-networks","title":"What Are Networks?","text":"<p>Simple Explanation: In Docker Swarm, networks are like virtual LANs. Services on the same network can talk to each other; services on different networks are isolated.</p> <p>Key Concept: Services connect by name, not IP address.</p> <pre><code># From inside the API container, you can do:\ncurl http://redis:6379  # \"redis\" is the service name\n</code></pre> <p>Docker's DNS automatically resolves \"redis\" to the right IP.</p> <p>Our Networks: - traefik-ingress: Public-facing services (anything users access via browser) - platform-network: Internal app communication (API \u2194 microservices) - vault-network: Secret distribution (most services \u2194 Vault) - monitoring-network: Metrics collection (services \u2192 InfluxDB/Grafana) - socket-proxy: Highly restricted (only Traefik \u2194 Docker API)</p> <p>Why Multiple Networks?: Security and organization. The matchmaking service doesn't need access to Vault's network, so it doesn't get it. If the matchmaking service is compromised, attackers can't reach Vault.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-a-reverse-proxy","title":"What is a Reverse Proxy?","text":"<p>Simple Explanation: A reverse proxy sits in front of your application servers and forwards requests to them. Users talk to the proxy, the proxy talks to your apps.</p> <pre><code>User Request:\n  https://sprocket.mlesports.gg\n    \u2193\n  Traefik (reverse proxy)\n    \u2193\n  Decides where to send it based on URL\n    \u2193\n  Forwards to: Sprocket Web service (port 3000)\n    \u2193\n  Returns response to user\n</code></pre> <p>Why Use One?: 1. TLS Termination: Traefik handles HTTPS encryption/decryption. Your apps just deal with plain HTTP. 2. Routing: One IP address, many applications. Traefik routes based on domain/path. 3. Load Balancing: If you have 3 copies of the web app, Traefik distributes traffic across them. 4. Let's Encrypt: Traefik automatically gets and renews TLS certificates.</p> <p>Our Reverse Proxy: Traefik handles ALL incoming HTTP/HTTPS traffic and routes to the appropriate service.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-is-secrets-management","title":"What is Secrets Management?","text":"<p>The Problem: Applications need sensitive information (passwords, API keys, certificates). Where do you store them?</p> <p>Bad Solutions (that we DON'T use): - \u274c Hard-code in source code \u2192 Anyone with code access sees secrets - \u274c Environment variables in deployment files \u2192 Secrets in version control - \u274c Shared document \u2192 No access control, no audit trail</p> <p>Our Solution: Hierarchical secrets management:</p> <pre><code>Doppler (Source of Truth)\n  \u2193 Manual bootstrap script\nVault (Runtime Distribution)\n  \u2193 Pulumi reads at deploy time\nDocker Secrets (Secure Container Delivery)\n  \u2193 Mounted as read-only files\nApplication (Reads from /app/secret/)\n</code></pre> <p>Why This Flow?: - Doppler: Team members can update secrets without touching infrastructure - Vault: Centralized, encrypted, auditable secret distribution - Docker Secrets: Encrypted in transit and at rest, mounted securely - Application: Reads from filesystem, never from environment variables (more secure)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#what-are-managed-services","title":"What Are Managed Services?","text":"<p>Simple Explanation: Instead of running your own database server, you pay a cloud provider to run it for you. They handle updates, backups, scaling, monitoring - you just use it.</p> <p>Example - PostgreSQL:</p> <p>Self-Hosted (what we used to do):</p> <pre><code>We manage:\n- Installing PostgreSQL in a container\n- Configuring backups\n- Monitoring disk space\n- Handling crashes\n- Security patches\n- Performance tuning\n- Disaster recovery\n\nCost: ~$0-5/month in storage\nEffort: 10-15 hours/month\n</code></pre> <p>Managed (what we do now):</p> <pre><code>Digital Ocean manages:\n- Installation and updates\n- Daily automated backups\n- Monitoring and alerts\n- Automatic failover\n- Security patches\n- Performance optimization\n- Point-in-time recovery\n\nCost: $15/month\nEffort: ~0 hours/month\n</code></pre> <p>ROI: Even at minimum wage, 10 hours/month costs more than $15. For skilled engineers, it's a no-brainer.</p> <p>Our Managed Services: - PostgreSQL (Digital Ocean): $15/month - database - Spaces/S3 (Digital Ocean/AWS): $5/month - file storage</p> <p>Still Self-Hosted: - Application services (our custom code) - Redis, RabbitMQ (standard services we configure) - Monitoring stack (Grafana, InfluxDB, Loki)</p> <p>Why Not Everything Managed?: Cost vs. complexity trade-off. Managing Redis in a container is easy. Managing a production database is hard. We use managed services where the operational complexity is high.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#architecture-principles","title":"Architecture Principles","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#1-infrastructure-as-code","title":"1. Infrastructure as Code","text":"<p>All infrastructure is defined in code using Pulumi (TypeScript).</p> <p>Benefits: - Version controlled (git) - Repeatable deployments - Easy rollbacks - Self-documenting infrastructure</p> <p>Implementation:</p> <pre><code>sprocket-infra/\n\u251c\u2500\u2500 layer_1/     # Infrastructure layer (Traefik, Vault)\n\u251c\u2500\u2500 layer_2/     # Data services layer\n\u251c\u2500\u2500 platform/    # Application layer\n\u2514\u2500\u2500 global/      # Shared code and service definitions\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#2-layered-deployment","title":"2. Layered Deployment","text":"<p>Infrastructure is deployed in three distinct layers, each depending on the previous.</p> <p>Layer 1 \u2192 Layer 2 \u2192 Platform</p> <p>Benefits: - Clear dependency management - Independent layer updates - Easier troubleshooting - Logical organization</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#3-managed-services-for-critical-components","title":"3. Managed Services for Critical Components","text":"<p>Don't self-host what you can buy as a service.</p> <p>Managed: - PostgreSQL (Digital Ocean Managed Database) - S3 Storage (Digital Ocean Spaces + AWS S3)</p> <p>Self-Hosted: - Application services (our code) - Observability stack (Grafana, Loki, InfluxDB) - Message queues and caches (Redis, RabbitMQ)</p> <p>Rationale: Time is valuable. Let professionals manage infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#4-security-by-default","title":"4. Security by Default","text":"<p>Security is not optional.</p> <ul> <li>All secrets in Vault, never in code</li> <li>TLS for all public-facing services</li> <li>Minimal exposed ports (80, 443 only)</li> <li>Secret rotation capability</li> <li>Audit trails (Vault, Doppler)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#5-observability-first","title":"5. Observability First","text":"<p>If you can't monitor it, you can't trust it.</p> <ul> <li>Metrics (InfluxDB, Grafana)</li> <li>Logs (Loki, centralized)</li> <li>Service health (Gatus)</li> <li>Distributed tracing (Telegraf)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#three-layer-architecture","title":"Three-Layer Architecture","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#layer-1-infrastructure-foundation","title":"Layer 1: Infrastructure Foundation","text":"<p>Purpose: Core infrastructure services that everything else depends on.</p> <p>Services: 1. Traefik - Reverse proxy and ingress controller 2. Vault - Secrets management 3. Socket Proxy - Secure Docker API access for Traefik</p> <p>Deployment: <code>layer_1/</code></p> <p>Key Characteristics: - Must be deployed first - Provides networking foundation (ingress network) - Handles TLS termination - Manages all secrets</p> <p>Configuration (<code>layer_1/Pulumi.layer_1.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\npostgres-port: \"25060\"\nvault-s3-bucket: vault-secrets\nvault-s3-endpoint: https://nyc3.digitaloceanspaces.com\n</code></pre> <p>Architecture Diagram:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       Layer 1                            \u2502\n\u2502                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502              Traefik Service                     \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502\n\u2502  \u2502  \u2502  Entrypoints:                            \u2502   \u2502   \u2502\n\u2502  \u2502  \u2502  - web (80) \u2192 redirect to websecure      \u2502   \u2502   \u2502\n\u2502  \u2502  \u2502  - websecure (443) \u2192 TLS termination     \u2502   \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502\n\u2502  \u2502                                                   \u2502   \u2502\n\u2502  \u2502  Let's Encrypt: Automatic certificate mgmt       \u2502   \u2502\n\u2502  \u2502  Routing: Host-based rules                       \u2502   \u2502\n\u2502  \u2502  Service Discovery: Docker provider              \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                  \u2502                                       \u2502\n\u2502                  \u2502 Communicates via                      \u2502\n\u2502                  \u2502 Socket Proxy                          \u2502\n\u2502                  \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502  \u2502     Socket Proxy Service      \u2502                      \u2502\n\u2502  \u2502  (Secure Docker API Access)   \u2502                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502              Vault Service                       \u2502   \u2502\n\u2502  \u2502                                                   \u2502   \u2502\n\u2502  \u2502  Storage Backend: Digital Ocean Spaces (S3)      \u2502   \u2502\n\u2502  \u2502  Auto-Initialize: Yes (via script)               \u2502   \u2502\n\u2502  \u2502  Auto-Unseal: Via bind-mounted unseal keys       \u2502   \u2502\n\u2502  \u2502  Auth Methods:                                    \u2502   \u2502\n\u2502  \u2502  - GitHub OAuth (primary)                        \u2502   \u2502\n\u2502  \u2502  - Token (for services)                          \u2502   \u2502\n\u2502  \u2502                                                   \u2502   \u2502\n\u2502  \u2502  Mount Points:                                    \u2502   \u2502\n\u2502  \u2502  - infrastructure/ (infra secrets)               \u2502   \u2502\n\u2502  \u2502  - platform/ (app secrets)                       \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                           \u2502\n\u2502  Networks:                                               \u2502\n\u2502  - traefik-ingress (overlay)  \u2190 Exposed to Layer 2/Platform\n\u2502  - vault-network (overlay)                               \u2502\n\u2502  - socket-proxy (overlay)                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Network Exports: - <code>IngressNetwork</code>: Used by all public-facing services - <code>VaultAddress</code>: Used by Layer 2 and Platform for secret retrieval</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#layer-2-data-services","title":"Layer 2: Data Services","text":"<p>Purpose: Shared data infrastructure used by multiple applications.</p> <p>Services: 1. Redis - Caching and pub/sub 2. RabbitMQ - Message queue (AMQP) 3. InfluxDB - Time-series metrics database 4. Grafana - Metrics visualization 5. N8n - Workflow automation 6. Neo4j - Graph database 7. Gatus - Service health monitoring 8. Loki - Log aggregation 9. Telegraf - Metrics collection</p> <p>Deployment: <code>layer_2/</code></p> <p>Key Characteristics: - Depends on Layer 1 (Vault, Ingress network) - Provides shared infrastructure for Platform - Each service has dedicated persistent volumes - Connected to ingress network for web UIs</p> <p>Configuration (<code>layer_2/Pulumi.layer_2.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\ns3-endpoint: nyc3.digitaloceanspaces.com\n</code></pre> <p>Architecture Diagram:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            Layer 2                                   \u2502\n\u2502                                                                       \u2502\n\u2502  Data Services (Shared Infrastructure)                               \u2502\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502     Redis      \u2502  \u2502   RabbitMQ     \u2502  \u2502    InfluxDB    \u2502        \u2502\n\u2502  \u2502                \u2502  \u2502                \u2502  \u2502                \u2502        \u2502\n\u2502  \u2502  Port: 6379    \u2502  \u2502  Port: 5672    \u2502  \u2502  Port: 8086    \u2502        \u2502\n\u2502  \u2502  Auth: Vault   \u2502  \u2502  AMQP protocol \u2502  \u2502  Org: sprocket \u2502        \u2502\n\u2502  \u2502  Persistence:  \u2502  \u2502  Virtual hosts \u2502  \u2502  Bucket: per   \u2502        \u2502\n\u2502  \u2502    Volume      \u2502  \u2502  Vault creds   \u2502  \u2502    environment \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502    Grafana     \u2502  \u2502      N8n       \u2502  \u2502     Neo4j      \u2502        \u2502\n\u2502  \u2502                \u2502  \u2502                \u2502  \u2502                \u2502        \u2502\n\u2502  \u2502  Dashboards    \u2502  \u2502  Workflows     \u2502  \u2502  Graph DB      \u2502        \u2502\n\u2502  \u2502  Metrics viz   \u2502  \u2502  Automation    \u2502  \u2502  Bolt: 7687    \u2502        \u2502\n\u2502  \u2502  Data sources: \u2502  \u2502  External DB   \u2502  \u2502  HTTP: 7474    \u2502        \u2502\n\u2502  \u2502  - InfluxDB    \u2502  \u2502  S3 storage    \u2502  \u2502                \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502     Gatus      \u2502  \u2502      Loki      \u2502  \u2502   Telegraf     \u2502        \u2502\n\u2502  \u2502                \u2502  \u2502                \u2502  \u2502                \u2502        \u2502\n\u2502  \u2502  Health checks \u2502  \u2502  Log storage   \u2502  \u2502  Metrics       \u2502        \u2502\n\u2502  \u2502  Uptime mon.   \u2502  \u2502  Aggregation   \u2502  \u2502  collection    \u2502        \u2502\n\u2502  \u2502  Alerting      \u2502  \u2502  Querying      \u2502  \u2502  \u2192 InfluxDB    \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                                                       \u2502\n\u2502  Networks:                                                           \u2502\n\u2502  - traefik-ingress (from Layer 1)                                   \u2502\n\u2502  - monitoring-network (overlay)  \u2190 Exported to Platform             \u2502\n\u2502  - chatwoot-network (overlay)                                       \u2502\n\u2502                                                                       \u2502\n\u2502  Secrets:                                                            \u2502\n\u2502  All services retrieve credentials from Vault                        \u2502\n\u2502                                                                       \u2502\n\u2502  Exports:                                                            \u2502\n\u2502  - InfrastructureVaultToken (for Platform)                          \u2502\n\u2502  - PlatformVaultToken (for Platform)                                \u2502\n\u2502  - PostgresHostname, PostgresPort                                   \u2502\n\u2502  - MonitoringNetworkId                                               \u2502\n\u2502  - MinioUrl (S3 endpoint)                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Network Exports: - <code>MonitoringNetworkId</code>: For services that need to push metrics - <code>InfrastructureVaultToken</code>: For infrastructure-level secret access - <code>PlatformVaultToken</code>: For application-level secret access</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#platform-application-services","title":"Platform: Application Services","text":"<p>Purpose: The Sprocket application itself and its microservices.</p> <p>Core Services: 1. Sprocket Web - Next.js web UI 2. Sprocket API - GraphQL API (NestJS) 3. Discord Bot - Main Discord integration</p> <p>Client Services: 4. Image Generation Frontend - Image generation UI</p> <p>Microservices: 5. Image Generation Service - Image creation backend 6. Notification Service - User notifications 7. Analytics Service - Server analytics 8. Matchmaking Service - Game matchmaking 9. Replay Parse Service - Rocket League replay parsing 10. ELO Service - Player rating calculations 11. Submission Service - Match submissions</p> <p>Legacy: 12. Legacy Bot - Backward compatibility</p> <p>Deployment: <code>platform/</code></p> <p>Key Characteristics: - Depends on Layer 1 and Layer 2 - All services share configuration structure - Services communicate via RabbitMQ - All data in managed PostgreSQL - All files in cloud S3</p> <p>Configuration (<code>platform/Pulumi.prod.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\nsubdomain: \"main\"\nimage-tag: main\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\n</code></pre> <p>Architecture Diagram:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              Platform Layer                                  \u2502\n\u2502                                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                        Core Services                                   \u2502 \u2502\n\u2502  \u2502                                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502 \u2502\n\u2502  \u2502  \u2502  Sprocket Web    \u2502  \u2502  Sprocket API    \u2502  \u2502  Discord Bot      \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  (Next.js)       \u2502  \u2502  (NestJS/GraphQL)\u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502                  \u2502  \u2502                  \u2502  \u2502  Listens to       \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  Port: 3000      \u2502  \u2502  Port: 3001      \u2502  \u2502  Discord events   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  Route:          \u2502  \u2502  Route:          \u2502  \u2502  Sends commands   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  sprocket.mles*  \u2502  \u2502  api.sprocket*   \u2502  \u2502  Uses RabbitMQ    \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502                  \u2502  \u2502                  \u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  Auth: OAuth     \u2502  \u2502  Auth: JWT       \u2502  \u2502  Bot Token: Vault \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  - Google        \u2502  \u2502  Database: All   \u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  - Discord       \u2502  \u2502    queries       \u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  - Epic          \u2502  \u2502  Cache: Redis    \u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502  - Steam         \u2502  \u2502  Queue: RabbitMQ \u2502  \u2502                   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                      Microservices                                     \u2502 \u2502\n\u2502  \u2502                                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 \u2502\n\u2502  \u2502  \u2502 Image Gen Svc   \u2502  \u2502 Notification    \u2502  \u2502  Analytics      \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502                 \u2502  \u2502  Service        \u2502  \u2502  Service        \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Creates images  \u2502  \u2502                 \u2502  \u2502                 \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Stores in S3    \u2502  \u2502 Sends Discord   \u2502  \u2502 Tracks metrics  \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Queue: RabbitMQ \u2502  \u2502   notifications \u2502  \u2502 Stores: InfluxDB\u2502      \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 \u2502\n\u2502  \u2502                                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 \u2502\n\u2502  \u2502  \u2502 Matchmaking     \u2502  \u2502 Replay Parse    \u2502  \u2502  ELO Service    \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502  Service        \u2502  \u2502  Service        \u2502  \u2502                 \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502 Calculates      \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Creates matches \u2502  \u2502 Parses RL       \u2502  \u2502  ratings        \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Uses Redis      \u2502  \u2502   replays       \u2502  \u2502 Updates on      \u2502      \u2502 \u2502\n\u2502  \u2502  \u2502 Queue: RabbitMQ \u2502  \u2502 Ballchasing API \u2502  \u2502  match end      \u2502      \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 \u2502\n\u2502  \u2502                                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                   \u2502 \u2502\n\u2502  \u2502  \u2502  Submission     \u2502                                                   \u2502 \u2502\n\u2502  \u2502  \u2502   Service       \u2502                                                   \u2502 \u2502\n\u2502  \u2502  \u2502                 \u2502                                                   \u2502 \u2502\n\u2502  \u2502  \u2502 Match submissions\u2502                                                  \u2502 \u2502\n\u2502  \u2502  \u2502 S3 storage      \u2502                                                   \u2502 \u2502\n\u2502  \u2502  \u2502 Queue: RabbitMQ \u2502                                                   \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                               \u2502\n\u2502  Service Communication:                                                     \u2502\n\u2502  - HTTP/GraphQL: Direct service-to-service                                  \u2502\n\u2502  - RabbitMQ: Async messaging between services                              \u2502\n\u2502  - Redis: Shared cache and pub/sub                                         \u2502\n\u2502                                                                               \u2502\n\u2502  Data Storage:                                                              \u2502\n\u2502  - PostgreSQL: All relational data                                          \u2502\n\u2502  - S3: All file storage (images, replays, backups)                          \u2502\n\u2502  - Redis: Cache and sessions                                                \u2502\n\u2502  - InfluxDB: Time-series metrics                                            \u2502\n\u2502  - Neo4j: Graph data (relationships)                                        \u2502\n\u2502                                                                               \u2502\n\u2502  Networks:                                                                  \u2502\n\u2502  - traefik-ingress (from Layer 1)                                          \u2502\n\u2502  - platform-network (overlay, internal)                                    \u2502\n\u2502  - monitoring-network (from Layer 2)                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#network-topology","title":"Network Topology","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#docker-swarm-overlay-networks","title":"Docker Swarm Overlay Networks","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       Network Topology                              \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  traefik-ingress (overlay network)                          \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - Traefik (Layer 1)                                         \u2502  \u2502\n\u2502  \u2502  - All public-facing services (Layer 2 &amp; Platform)          \u2502  \u2502\n\u2502  \u2502  - Vault (for web UI access)                                 \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: All HTTP/HTTPS traffic flows through this network \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  vault-network (overlay network)                            \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - Vault                                                      \u2502  \u2502\n\u2502  \u2502  - Services that need Vault access (most services)          \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: Secret retrieval                                   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  socket-proxy (overlay network)                             \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - Traefik                                                    \u2502  \u2502\n\u2502  \u2502  - Socket Proxy                                              \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: Secure Docker API access for service discovery    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  platform-network (overlay network)                         \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - All Platform services                                     \u2502  \u2502\n\u2502  \u2502  - Redis, RabbitMQ (from Layer 2)                           \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: Internal platform communication                    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  monitoring-network (overlay network)                       \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - InfluxDB, Grafana, Loki, Telegraf                        \u2502  \u2502\n\u2502  \u2502  - Services that push metrics (Platform services)           \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: Metrics and logging                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  chatwoot-network (overlay network)                         \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Connected Services:                                         \u2502  \u2502\n\u2502  \u2502  - Chatwoot (if deployed)                                    \u2502  \u2502\n\u2502  \u2502  - Redis (shared with Platform)                             \u2502  \u2502\n\u2502  \u2502                                                               \u2502  \u2502\n\u2502  \u2502  Purpose: Chatwoot service isolation                         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#network-isolation-strategy","title":"Network Isolation Strategy","text":"<p>Public-Facing (traefik-ingress): - All services with web UIs - All API endpoints - Traefik routes traffic based on Host header</p> <p>Internal (platform-network): - Service-to-service communication - Not exposed externally - RabbitMQ, Redis connections</p> <p>Monitoring (monitoring-network): - Metrics and logging infrastructure - Separated for security and performance</p> <p>Socket Access (socket-proxy): - Highly restricted - Only Traefik can access Docker API - Prevents unauthorized service discovery</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#service-catalog","title":"Service Catalog","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#layer-1-services","title":"Layer 1 Services","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#traefik","title":"Traefik","text":"<p>Purpose: Reverse proxy, ingress controller, TLS termination</p> <p>Image: <code>traefik:v2.6.1</code></p> <p>Configuration: - Static config: <code>/layer_1/src/config/traefik/static.yaml</code> - Dynamic config: Service labels - Certificate storage: Persistent volume (<code>/data</code>)</p> <p>Key Features: - Automatic service discovery via Docker provider - Let's Encrypt integration (HTTP-01 challenge) - Host-based routing - TLS termination - Forward authentication support</p> <p>Routing Example:</p> <pre><code># Service labels\ntraefik.enable: \"true\"\ntraefik.http.routers.sprocket-web.rule: \"Host(`sprocket.mlesports.gg`)\"\ntraefik.http.routers.sprocket-web.entrypoints: \"websecure\"\ntraefik.http.routers.sprocket-web.tls.certresolver: \"lets-encrypt-tls\"\ntraefik.http.services.sprocket-web.loadbalancer.server.port: \"3000\"\n</code></pre> <p>Exposed Ports: - 80 (HTTP) - Redirects to HTTPS - 443 (HTTPS) - Primary traffic</p> <p>Monitoring: - Dashboard: https://traefik.sprocket.mlesports.gg - Metrics: Available via Prometheus endpoint</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#vault","title":"Vault","text":"<p>Purpose: Secrets management, dynamic credentials, encryption</p> <p>Image: <code>vault:1.10.0</code></p> <p>Configuration: - Server config: <code>/layer_1/src/config/vault/vault.hcl</code> - Storage backend: S3-compatible (Digital Ocean Spaces) - Auth methods: GitHub OAuth, Token</p> <p>Key Features: - Auto-initialization on first run - Auto-unseal via bind-mounted keys - S3 backend for state persistence - KV v2 secrets engine - Dynamic PostgreSQL credentials - GitHub team-based access control</p> <p>Mount Points:</p> <pre><code>/infrastructure     - Infrastructure secrets (DB creds, S3 keys)\n/platform          - Application secrets (OAuth, API tokens)\n</code></pre> <p>Initialization Script: <code>/global/services/vault/scripts/auto-initialize.sh</code></p> <p>Access: - Web UI: https://vault.sprocket.mlesports.gg - API: https://vault.sprocket.mlesports.gg/v1/</p> <p>Unseal Keys: Stored in <code>/global/services/vault/unseal-tokens/</code></p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#socket-proxy","title":"Socket Proxy","text":"<p>Purpose: Secure Docker API access for Traefik</p> <p>Image: <code>tecnativa/docker-socket-proxy</code></p> <p>Configuration: - Read-only Docker socket access - Filtered API endpoints - Only allows service discovery</p> <p>Security: - No write access to Docker API - Only Traefik can connect - Separate network (socket-proxy)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#layer-2-services","title":"Layer 2 Services","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#redis","title":"Redis","text":"<p>Purpose: Caching, session storage, pub/sub messaging</p> <p>Why We Use Redis:</p> <p>Without Redis, every user request would hit the PostgreSQL database directly. This creates problems: - Slow response times: Database queries take 10-50ms. Redis queries take &lt;1ms. - Database overload: 100 users making requests = 100+ database queries per second - Scaling bottleneck: Database becomes the constraint for all traffic</p> <p>With Redis:</p> <pre><code>User requests player profile:\n1. Check Redis cache \u2192 Found! (1ms) \u2192 Return data\n\nUser requests again:\n2. Check Redis cache \u2192 Found! (1ms) \u2192 Return data\n   (Database wasn't even queried!)\n\nCache expires after 5 minutes:\n3. Check Redis \u2192 Miss \u2192 Query database (20ms) \u2192 Cache result \u2192 Return data\n4. Next 1000 requests served from cache in 1ms each\n</code></pre> <p>Real Impact: Redis reduces database load by 80-90% and improves response times from 50ms to 1ms for cached data.</p> <p>Image: <code>redis:6-alpine</code></p> <p>Configuration: - Config file: <code>/layer_2/src/config/redis/redis.conf</code> - Password: Retrieved from Vault - Persistence: RDB + AOF (survives restarts)</p> <p>Used By: - All Platform services (caching frequently-accessed data) - Matchmaking service (queue state management) - ELO service (leaderboards and rankings) - API (session storage for logged-in users)</p> <p>Port: 6379</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#rabbitmq","title":"RabbitMQ","text":"<p>Purpose: Message queue for async service communication</p> <p>Why We Use RabbitMQ:</p> <p>Problem without message queues:</p> <pre><code>User submits match \u2192 API needs to:\n1. Save to database\n2. Send Discord notification\n3. Update player stats\n4. Recalculate ELO\n5. Update leaderboards\n6. Generate match image\n\nTotal time: 5-10 seconds (user waits!)\nIf notification service is down: entire request fails\n</code></pre> <p>With RabbitMQ:</p> <pre><code>User submits match:\n1. API saves to database (50ms)\n2. API publishes message to RabbitMQ (5ms)\n3. API responds to user: \"Match saved!\" (55ms total)\n\nIn background (asynchronous):\n- Notification service reads queue \u2192 sends Discord message\n- Analytics service reads queue \u2192 updates stats\n- ELO service reads queue \u2192 recalculates ratings\n- Image service reads queue \u2192 generates image\n\nUser sees instant response.\nServices process independently.\nIf one service is down, others continue working.\nFailed tasks can retry automatically.\n</code></pre> <p>Real Impact: API response time reduced from 5-10 seconds to 50-100ms. Services can fail and retry independently without affecting user experience.</p> <p>Image: <code>rabbitmq:3-management-alpine</code></p> <p>Configuration: - Credentials: Generated and stored in Vault - Virtual hosts: Per environment - Management UI: Enabled</p> <p>Used By: - All microservices (task queues) - Core API (event publishing) - Discord bot (command processing)</p> <p>Queues:</p> <pre><code>{environment}-core          - Core API tasks\n{environment}-bot           - Discord bot tasks\n{environment}-matchmaking   - Matchmaking tasks\n{environment}-analytics     - Analytics tasks\n{environment}-events        - Event streaming\n{environment}-ig            - Image generation tasks\n{environment}-submissions   - Match submissions\n{environment}-notifications - User notifications\n</code></pre> <p>Ports: - 5672 (AMQP) - 15672 (Management UI)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#influxdb","title":"InfluxDB","text":"<p>Purpose: Time-series metrics storage</p> <p>Why We Use InfluxDB:</p> <p>Regular databases like PostgreSQL are designed for transactional data (users, matches, teams). But tracking metrics over time creates problems:</p> <pre><code>Storing 1 metric per second in PostgreSQL:\n- 60 metrics/minute\n- 3,600 metrics/hour\n- 86,400 metrics/day\n- 31,536,000 metrics/year per metric type\n\nWith 50 different metrics = 1.5 BILLION rows per year!\n\nQueries like \"Show me CPU usage over the last 24 hours\" require:\n- Scanning millions of rows\n- Complex aggregations\n- Slow (5-30 seconds)\n- Database locks and performance issues\n</code></pre> <p>With InfluxDB (purpose-built for time-series data):</p> <pre><code>Same 50 metrics at 1/second:\n- Optimized storage format (compressed by 10-100x)\n- Built-in time-based aggregation\n- Query \"last 24 hours\" returns in &lt;100ms\n- Automatic downsampling (1-second \u2192 1-minute after 7 days)\n- Automatic retention (deletes old data)\n</code></pre> <p>Real Impact: - Storage: 10GB vs. 100GB+ for same data - Query speed: 100ms vs. 5-30 seconds - Can track hundreds of metrics without performance degradation</p> <p>What We Track: - Player activity (logins, match participation) - Match statistics (completion rates, duration) - API response times - Error rates - Custom business metrics</p> <p>Image: <code>influxdb:2.7</code></p> <p>Configuration: - Organization: <code>sprocket</code> - Bucket: <code>sprocket_{environment}</code> - Token: Stored in Vault</p> <p>Used By: - Analytics service (writes metrics) - Grafana (reads for visualization) - Telegraf (collects system metrics)</p> <p>Port: 8086</p> <p>Retention: 30 days (configurable)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#grafana-unified-observability-interface","title":"Grafana (Unified Observability Interface)","text":"<p>Purpose: Unified metrics visualization, log querying, and observability dashboards</p> <p>Why We Use Grafana:</p> <p>Grafana serves as our single pane of glass for observability, consolidating metrics and logs:</p> <p>Without Unified Interface:</p> <pre><code>Investigating an incident:\n1. Query InfluxDB for metrics \u2192 See error spike at 3pm\n2. SSH into server \u2192 grep logs for errors\n3. Manually correlate timestamps between metrics and logs\n4. Switch between tools (CLI, metrics UI, log files)\n\u2192 Time: 20-30 minutes to correlate data\n</code></pre> <p>With Grafana Unified Interface:</p> <pre><code>Open Grafana:\n\u2192 Dashboard shows error spike at 3pm\n\u2192 Click time range on graph\n\u2192 Switch to Explore \u2192 Query logs: {service=\"prod-sprocket-core\"} |= \"ERROR\"\n\u2192 Logs show: \"Database connection pool exhausted\"\n\u2192 Click related metrics \u2192 See DB connections maxed out\n\u2192 Time: 2-3 minutes, no SSH needed\n</code></pre> <p>Real Impact: - 10-20x faster issue identification - No SSH required for log analysis (query via Grafana UI) - Correlation built-in: Metrics and logs time-synced in one view - Team collaboration: Share live dashboards vs. pasting log snippets - Historical analysis: What happened during last deployment? - Proactive alerts: Email/Slack when error rate &gt; threshold</p> <p>Unified Features: - Metrics Dashboards: Real-time visualization (InfluxDB/Telegraf data) - Log Querying: LogQL queries against Loki (no SSH to server) - Time Correlation: Sync time ranges between metrics and logs - Split View: Compare metrics and logs side-by-side - Alerting: Unified alerts for metrics or log patterns</p> <p>Our Dashboards: - System health (CPU, memory, disk, network) - Service metrics (response times, error rates, request rates) - Business metrics (active players, matches created, completion rates) - Database performance (query times, connection pool usage) - Log explorer (centralized log search across all services)</p> <p>Common Workflows:</p> <pre><code># 1. Metrics-First Investigation\nDashboard \u2192 See CPU spike \u2192 Explore logs for that time \u2192 Find cause\n\n# 2. Logs-First Investigation\nExplore \u2192 Query logs for errors \u2192 See timestamp \u2192 Check metrics dashboard\n\n# 3. Correlation\nSplit view \u2192 Metrics on left \u2192 Logs on right \u2192 Synchronized time range\n</code></pre> <p>Image: <code>grafana/grafana:latest</code></p> <p>Data Sources: - InfluxDB: System and application metrics - Loki: Centralized logs from all Docker services - PostgreSQL: Direct database queries (optional)</p> <p>Access: https://grafana.sprocket.mlesports.gg</p> <p>Authentication: admin /  <p>How to Use: 1. Dashboards \u2192 Pre-built metric visualizations 2. Explore \u2192 Ad-hoc log queries (LogQL) or metric queries 3. Alerts \u2192 Configure notifications based on thresholds 4. Correlate \u2192 Click metric spike \u2192 View logs for same time</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#n8n","title":"N8n","text":"<p>Purpose: Workflow automation</p> <p>Image: <code>n8nio/n8n:latest</code></p> <p>Configuration: - Database: Managed PostgreSQL - Storage: S3 for file uploads</p> <p>Use Cases: - Automated data exports - Integration workflows - Scheduled tasks</p> <p>Access: https://n8n.sprocket.mlesports.gg</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#neo4j","title":"Neo4j","text":"<p>Purpose: Graph database for relationship data</p> <p>Image: <code>neo4j:latest</code></p> <p>Configuration: - Bolt protocol: 7687 - HTTP: 7474 - Auth: Username/password in Vault</p> <p>Use Cases: - Player relationships - Team hierarchies - Social graphs</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#gatus","title":"Gatus","text":"<p>Purpose: Service health monitoring and uptime tracking</p> <p>Image: <code>twinproduction/gatus:latest</code></p> <p>Configuration: - Config file: <code>/layer_2/src/config/gatus/config.yml</code> - Check interval: 30 seconds</p> <p>Monitored Services: - All public endpoints - Database connectivity - External APIs</p> <p>Access: https://gatus.sprocket.mlesports.gg</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#loki","title":"Loki","text":"<p>Purpose: Log aggregation and querying</p> <p>Image: <code>grafana/loki:latest</code></p> <p>Configuration: - Storage: Local filesystem (future: S3) - Retention: 7 days</p> <p>Log Sources: - Docker log driver (all services) - Manual pushes from applications</p> <p>Access: Queried via Grafana</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#telegraf","title":"Telegraf","text":"<p>Purpose: Metrics collection and forwarding</p> <p>Image: <code>telegraf:latest</code></p> <p>Configuration: - Inputs: Docker, system, custom - Output: InfluxDB</p> <p>Collected Metrics: - CPU, memory, disk, network - Docker container stats - Custom application metrics</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#platform-services","title":"Platform Services","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#sprocket-web","title":"Sprocket Web","text":"<p>Purpose: Main web UI for users</p> <p>Why Next.js:</p> <p>Framework Options: - Plain React: Client-side only, slow first load, bad SEO - Create React App: Simple setup, but no server-side rendering - Next.js: Server-side rendering, excellent SEO, fast initial load, image optimization, API routes</p> <p>What Next.js Provides: 1. Server-Side Rendering (SSR): Pages load with content already rendered (not blank until JavaScript loads) 2. Static Generation: Build pages at deploy time for instant loads 3. Image Optimization: Automatic image resizing/WebP conversion 4. Code Splitting: Only load JavaScript needed for current page 5. Built-in Routing: No need for React Router</p> <p>Impact on User Experience:</p> <pre><code>Without SSR (plain React):\n1. User visits page\n2. Browser downloads blank HTML\n3. Browser downloads 500KB JavaScript\n4. React renders page (1-2 seconds)\n5. Page fetches data from API\n6. Page shows content (2-4 seconds total)\n\nWith Next.js SSR:\n1. User visits page\n2. Server renders page with data\n3. Browser receives complete HTML (300ms)\n4. Page is interactive immediately\n5. Background JavaScript loads for navigation\n(0.5-1 second total)\n</code></pre> <p>Image: <code>asaxplayinghorse/sprocket-web:{tag}</code></p> <p>Framework: Next.js (React)</p> <p>Features: - User authentication (OAuth) - Player profiles - Match browsing - Team management - Statistics and leaderboards</p> <p>Configuration: - Config file: <code>/platform/src/config/services/web.json</code> - Environment: Production - Port: 3000</p> <p>Route: https://sprocket.mlesports.gg</p> <p>Secrets: - Chatwoot HMAC key</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#sprocket-api","title":"Sprocket API","text":"<p>Purpose: GraphQL API backend</p> <p>Why GraphQL (vs REST):</p> <p>REST API Problem (what we used to have):</p> <pre><code>To show a player profile page, we need:\n1. GET /api/players/123 \u2192 Player info\n2. GET /api/players/123/stats \u2192 Player stats\n3. GET /api/players/123/matches \u2192 Recent matches\n4. GET /api/players/123/teams \u2192 Team memberships\n\n4 HTTP requests = 4 round-trips = 200-400ms\nOver-fetching: Each endpoint returns fields we don't need\nUnder-fetching: Need multiple requests to get complete data\n</code></pre> <p>GraphQL Solution (what we use now):</p> <pre><code>query PlayerProfile($id: ID!) {\n  player(id: $id) {\n    name\n    stats { wins losses winRate }\n    recentMatches(limit: 5) { id date opponent score }\n    teams { id name role }\n  }\n}\n\n1 HTTP request = 1 round-trip = 50-100ms\nExact data: Client specifies exactly what fields it needs\nComplete: All related data in one request\n</code></pre> <p>Why NestJS: - Enterprise Framework: Structured, opinionated, scales well - TypeScript: Type safety reduces bugs by ~30% - Dependency Injection: Clean, testable code architecture - Built-in GraphQL: First-class support for GraphQL APIs - Middleware: Authentication, logging, error handling built-in</p> <p>Real Impact: - API response time: 50-100ms (was 200-400ms with REST) - Frontend code: 50% less boilerplate for data fetching - Type safety: Catch bugs at compile time instead of runtime - Development speed: 30-40% faster feature development</p> <p>Image: <code>asaxplayinghorse/sprocket-core:{tag}</code></p> <p>Framework: NestJS (Node.js)</p> <p>Features: - GraphQL API - Authentication/Authorization - Database operations - Business logic - RabbitMQ message handling</p> <p>Configuration: - Port: 3001 - Database: All queries - Cache: Redis - Queue: RabbitMQ</p> <p>Route: https://api.sprocket.mlesports.gg</p> <p>Secrets: - JWT secret - OAuth credentials (Google, Discord, Epic, Steam) - Database password - Redis password - S3 credentials</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#discord-bot","title":"Discord Bot","text":"<p>Purpose: Discord integration</p> <p>Why a Dedicated Bot Service:</p> <p>The Problem: Our community lives on Discord. Users don't want to leave Discord to: - Check match schedules - Submit match results - View team rosters - Register for tournaments</p> <p>Solution: Bring functionality to Discord via bot commands.</p> <p>Why Separate Service (vs. integrating into API):</p> <pre><code>Integrated into API:\n- API must handle HTTP requests AND Discord events\n- Discord.js library loaded in API (memory overhead)\n- Discord connection issues could affect API stability\n- Harder to scale (bot uses WebSocket, API uses HTTP)\n\nDedicated Bot Service:\n- Single responsibility: Discord interactions\n- Independent scaling (bot needs 1 replica, API might need 3)\n- Discord issues don't affect API\n- Can restart bot without affecting API\n- Bot can use RabbitMQ to trigger backend operations\n</code></pre> <p>Architecture Pattern:</p> <pre><code>User sends Discord command: \"/match-schedule\"\n    \u2193\nDiscord Bot receives command\n    \u2193\nBot publishes message to RabbitMQ: \"fetch-match-schedule\"\n    \u2193\nAPI consumes message, fetches from database\n    \u2193\nAPI publishes response to RabbitMQ: \"schedule-data\"\n    \u2193\nBot consumes response, formats for Discord\n    \u2193\nBot sends Discord message with schedule\n</code></pre> <p>Benefits: - User Convenience: Stay in Discord workflow - Engagement: Notifications for matches, results, signups - Automation: Bot can send reminders, announcements - Accessibility: No need to remember website URL</p> <p>Real Impact: - 70% of user interactions happen via Discord bot (vs. website) - Notification open rate: 85% (Discord) vs. 20% (email) - User feedback: \"I never have to leave Discord\" = positive UX</p> <p>Image: <code>asaxplayinghorse/sprocket-discord-bot:{tag}</code></p> <p>Features: - Slash commands - Event notifications - Match scheduling - User registration</p> <p>Configuration: - Bot token: Vault - Command prefix: <code>s.</code> (main) or <code>{env}.</code> (others) - Uses RabbitMQ for command queue</p> <p>Secrets: - Discord bot token - S3 credentials (for image uploads)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#image-generation-service","title":"Image Generation Service","text":"<p>Purpose: Generate match images, player cards, etc.</p> <p>Image: <code>asaxplayinghorse/sprocket-image-generation-service:{tag}</code></p> <p>Features: - Dynamic image creation - Template rendering - S3 upload - Queue-based processing</p> <p>Configuration: - Queue: RabbitMQ <code>{env}-ig</code> - Storage: S3 bucket</p> <p>Secrets: - S3 credentials - Database password</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#other-microservices","title":"Other Microservices","text":"<p>Notification Service: - Sends Discord/email notifications - Queue-based</p> <p>Analytics Service: - Tracks player/match metrics - Writes to InfluxDB</p> <p>Matchmaking Service: - Creates balanced matches - Uses Redis for state</p> <p>Replay Parse Service: - Parses Rocket League replays - Integrates with Ballchasing API</p> <p>ELO Service: - Calculates player ratings - Updates on match completion</p> <p>Submission Service: - Handles match submissions - S3 storage for files</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#data-flow","title":"Data Flow","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#data-flow-overview","title":"Data Flow Overview","text":"<p>The diagram above illustrates three main data flows in the Sprocket platform: - Player Authentication Flow (Blue): Player login via OAuth (Discord/Google), user verification in PostgreSQL, and JWT token issuance - Match Scheduling Flow (Green): League admin schedules match via Discord bot, matchmaking service processes request, teams are notified - Match Completion Flow (Orange): Player submits result and replay file, replay is parsed via Ballchasing API, ELO ratings updated, stats recorded, match card image generated, and Discord notifications sent</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#request-flow","title":"Request Flow","text":"<pre><code>User Browser\n    \u2502\n    \u2502 HTTPS Request\n    \u2502 Host: sprocket.mlesports.gg\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Traefik         \u2502\n\u2502  (Layer 1)          \u2502\n\u2502                     \u2502\n\u2502  1. TLS termination \u2502\n\u2502  2. Route matching  \u2502\n\u2502  3. Load balancing  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 HTTP (internal)\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Sprocket Web       \u2502\n\u2502  (Platform)         \u2502\n\u2502                     \u2502\n\u2502  1. Render page     \u2502\n\u2502  2. API calls       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 GraphQL Query\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Sprocket API       \u2502\n\u2502  (Platform)         \u2502\n\u2502                     \u2502\n\u2502  1. Auth check      \u2502\n\u2502  2. Business logic  \u2502\n\u2502  3. DB queries      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502\n       \u251c\u2500\u2500\u2500 PostgreSQL \u2500\u2500\u2500\u2510\n       \u2502                  \u2502\n       \u251c\u2500\u2500\u2500 Redis \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502                  \u2502\n       \u251c\u2500\u2500\u2500 RabbitMQ \u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502                  \u2502\n       \u2514\u2500\u2500\u2500 S3 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#secret-retrieval-flow","title":"Secret Retrieval Flow","text":"<pre><code>Doppler (Source of Truth)\n    \u2502\n    \u2502 Bootstrap Script\n    \u2502 (manual, one-time)\n    \u2502\n    \u25bc\nVault\n    \u2502\n    \u2502 Pulumi reads secrets at deploy time\n    \u2502\n    \u25bc\nDocker Secrets\n    \u2502\n    \u2502 Mounted into containers\n    \u2502\n    \u25bc\nApplication Services\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#async-processing-flow","title":"Async Processing Flow","text":"<pre><code>User Action (e.g., \"Create Match\")\n    \u2502\n    \u25bc\nSprocket API\n    \u2502\n    \u2502 Publishes message\n    \u2502\n    \u25bc\nRabbitMQ Queue\n    \u2502\n    \u25bc\nMatchmaking Service (consumer)\n    \u2502\n    \u2502 1. Process matchmaking\n    \u2502 2. Write to DB\n    \u2502 3. Publish result event\n    \u2502\n    \u25bc\nRabbitMQ Events Queue\n    \u2502\n    \u251c\u2500\u2500\u2500 Notification Service \u2500\u2500\u2500&gt; Send Discord notification\n    \u2502\n    \u251c\u2500\u2500\u2500 Analytics Service \u2500\u2500\u2500\u2500\u2500\u2500&gt; Record metrics\n    \u2502\n    \u2514\u2500\u2500\u2500 Discord Bot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; Update Discord channels\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#security-architecture","title":"Security Architecture","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#secret-management-hierarchy","title":"Secret Management Hierarchy","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Doppler (Source of Truth)              \u2502\n\u2502  - OAuth credentials                                        \u2502\n\u2502  - API tokens                                              \u2502\n\u2502  - Application secrets                                      \u2502\n\u2502  - Team access control                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Manual bootstrap\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Vault (Runtime Distribution)           \u2502\n\u2502  - Receives secrets from Doppler (via script)              \u2502\n\u2502  - Distributes to services at runtime                      \u2502\n\u2502  - GitHub team-based access control                        \u2502\n\u2502  - Audit logging                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Pulumi reads at deploy time\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Docker Secrets                         \u2502\n\u2502  - Created by Pulumi from Vault values                     \u2502\n\u2502  - Mounted into containers as files                        \u2502\n\u2502  - Readable only by specific services                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Read from filesystem\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Application Services                   \u2502\n\u2502  - Read secrets from /app/secret/*.txt                     \u2502\n\u2502  - Never log secret values                                 \u2502\n\u2502  - Rotate as needed                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#vault-access-control","title":"Vault Access Control","text":"<p>GitHub OAuth Integration:</p> <pre><code>auth \"github\" {\n  organization = \"SprocketBot\"\n\n  team_mappings = {\n    \"github-admin\" = \"admins\"\n    \"github-readonly\" = \"readonly\"\n  }\n}\n</code></pre> <p>Policies: - <code>admins</code>: Full access to all secrets - <code>readonly</code>: Read-only access to secrets - <code>developers</code>: Access to application secrets only - <code>ops</code>: Access to infrastructure secrets only</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#application-authentication","title":"Application Authentication","text":"<p>OAuth Providers: - Google - Discord - Epic Games - Steam</p> <p>Flow: 1. User clicks \"Login with Google\" 2. Redirects to OAuth provider 3. Provider callback with authorization code 4. API exchanges code for user info 5. API creates JWT token 6. Frontend stores JWT 7. Subsequent requests include JWT</p> <p>JWT Structure:</p> <pre><code>{\n  \"sub\": \"user-id\",\n  \"email\": \"user@example.com\",\n  \"roles\": [\"user\"],\n  \"iat\": 1234567890,\n  \"exp\": 1234654290\n}\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#network-security","title":"Network Security","text":"<p>Firewall Rules:</p> <pre><code>Inbound:\n  Port 80 (HTTP) - Allow (redirects to HTTPS)\n  Port 443 (HTTPS) - Allow\n  Port 22 (SSH) - Allow (restrict to admin IPs)\n  All others - Deny\n\nOutbound:\n  Port 443 (HTTPS) - Allow (for external APIs)\n  Port 80 (HTTP) - Allow (for package managers)\n  Port 25060 (PostgreSQL) - Allow (Digital Ocean DB)\n  Port 5432 (PostgreSQL) - Allow (Digital Ocean DB)\n  All others - Allow (for now, should be restricted)\n</code></pre> <p>TLS Configuration: - Minimum version: TLS 1.2 - Cipher suites: Modern, secure ciphers only - HSTS: Enabled (Strict-Transport-Security header) - Certificate provider: Let's Encrypt - Auto-renewal: Yes</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#data-security","title":"Data Security","text":"<p>At Rest: - PostgreSQL: Encrypted by Digital Ocean - S3: Encrypted by cloud provider - Vault state: Encrypted in S3 - Docker volumes: Not encrypted (future improvement)</p> <p>In Transit: - External traffic: TLS 1.2+ (via Traefik) - Internal traffic: Unencrypted (within Docker overlay networks) - Database connections: SSL required (managed PostgreSQL)</p> <p>Secrets: - Never in git (even encrypted) - Never in logs - Never in error messages - Rotated regularly (manual process, should be automated)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#current-limitations-single-node","title":"Current Limitations (Single Node)","text":"<p>Bottlenecks: 1. Single point of failure: If node goes down, everything goes down 2. Resource constraints: All services share node resources 3. No horizontal scaling: Can't add more nodes for capacity 4. Persistent volumes: Tied to single node</p> <p>Current Node Requirements: - CPU: 4+ cores - RAM: 8GB+ (16GB recommended) - Disk: 100GB+ SSD - Network: 1Gbps+</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#vertical-scaling-current","title":"Vertical Scaling (Current)","text":"<p>Approach: Upgrade node resources</p> <p>Pros: - Simple (no architecture changes) - Works well for moderate growth</p> <p>Cons: - Limited by hardware maximums - Expensive at high end - Still single point of failure</p> <p>When to do: - Current node at &gt;70% CPU/memory - Performance degradation observed - Before major traffic events</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#horizontal-scaling-future","title":"Horizontal Scaling (Future)","text":"<p>Approach: Add more nodes to Docker Swarm</p> <p>Changes Required: 1. Vault: Migrate to cloud KMS auto-unseal 2. Volumes: Use NFS or cloud volumes (not local) 3. Traefik: Multiple replicas with shared cert storage 4. Database: Already using managed PostgreSQL (good!) 5. Load Balancer: Add external LB in front of Swarm</p> <p>Pros: - True redundancy - Can scale to traffic - No single point of failure</p> <p>Cons: - More complex - Higher costs - Requires refactoring</p> <p>When to do: - Consistently high traffic (&gt;1000 concurrent users) - Need high availability (99.9%+ uptime) - Budget allows (~3x current costs)</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#service-level-scaling","title":"Service-Level Scaling","text":"<p>Approach: Scale individual services</p> <p>Already Scalable: - All Platform microservices (stateless) - Can run multiple replicas</p> <p>Needs Work: - Redis (would need Redis Cluster) - RabbitMQ (would need cluster) - Vault (would need HA setup)</p> <p>How to Scale a Service:</p> <pre><code># Increase replicas for a service\ndocker service scale sprocket-web=3\n\n# Or via Pulumi\nmode: {\n  replicated: {\n    replicas: 3\n  }\n}\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#performance-optimization","title":"Performance Optimization","text":"<p>Current Optimizations: - Redis caching (reduces DB load) - CDN for static assets (not yet implemented) - Database connection pooling - Gzip compression (Traefik) - HTTP/2 (Traefik)</p> <p>Future Optimizations: - CDN integration (Cloudflare) - Image optimization (WebP, lazy loading) - Database query optimization (indexes) - RabbitMQ prefetch tuning - Service-specific resource limits</p>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#future-architectural-improvements","title":"Future Architectural Improvements","text":""},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#high-priority","title":"High Priority","text":"<ol> <li>Disaster Recovery Testing</li> <li>Document full recovery procedure</li> <li>Test from-scratch rebuild</li> <li>Test database restore</li> <li> <p>Test Vault recovery</p> </li> <li> <p>Monitoring &amp; Alerting</p> </li> <li>Set up PagerDuty</li> <li>Configure critical alerts</li> <li> <p>Create runbooks for common issues</p> </li> <li> <p>Security Hardening</p> </li> <li>Enable Docker secrets encryption</li> <li>Implement WAF (Cloudflare)</li> <li>Add rate limiting</li> <li>Security audit</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#medium-priority","title":"Medium Priority","text":"<ol> <li>Multi-Environment Setup</li> <li>Staging environment</li> <li>Development environment</li> <li> <p>Proper environment isolation</p> </li> <li> <p>CI/CD Pipeline</p> </li> <li>Automated testing</li> <li>Automated deployments</li> <li> <p>Rollback capability</p> </li> <li> <p>Backup Automation</p> </li> <li>Automated database backups</li> <li>Automated volume backups</li> <li>Off-site backup storage</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#low-priority","title":"Low Priority","text":"<ol> <li>Multi-Node Setup</li> <li>3-node Docker Swarm</li> <li>External load balancer</li> <li> <p>Shared volume storage</p> </li> <li> <p>Service Mesh</p> </li> <li>Consider Istio or Linkerd</li> <li>Mutual TLS between services</li> <li>Advanced traffic management</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/ARCHITECTURE/#conclusion","title":"Conclusion","text":"<p>The Sprocket infrastructure is built on solid foundations with clear separation of concerns, managed services for critical components, and comprehensive observability. The three-layer architecture provides good organization and deployment control.</p> <p>Strengths: - Infrastructure as Code (repeatable) - Managed services (reliable) - Comprehensive secret management - Good observability - TLS everywhere</p> <p>Areas for Improvement: - Single node (no HA) - Manual disaster recovery - No CI/CD pipeline - Security hardening needed - Monitoring alerts needed</p> <p>Suitable For: - Current scale (&lt;1000 concurrent users) - Small to medium teams - Moderate budget constraints</p> <p>Will Need Upgrade When: - Traffic exceeds single node capacity - High availability required (99.9%+) - Team size increases significantly</p> <p>This architecture document should serve as a reference for understanding the current infrastructure and planning future improvements.</p> <p>Document Version: 1.0 Last Updated: November 8, 2025 Next Review: February 2026</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/","title":"Summary: Claude Conversation Analysis for Sprocket Infrastructure","text":"<p>Analysis Completed: November 10, 2025 Documents Created: 2 new supplementary documents Source: <code>~/.claude.json</code> conversation history (260 lines of project context)</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#what-was-discovered","title":"What Was Discovered","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#1-rich-technical-context","title":"1. Rich Technical Context","text":"<p>The Claude conversation history provided invaluable insights into the iterative problem-solving process that wasn't captured in git commits alone. Key discoveries:</p> <ul> <li>Vault unsealing challenge: Required 5+ attempts over 3 days (Sept 18-21)</li> <li>Multi-environment routing: Complex 4-access-pattern solution (local, LAN, Tailscale, production)</li> <li>Service connectivity issues: Network layer problems between Layer 2 and Platform services</li> <li>Environment variable management: Ongoing issues with <code>generate-env.sh</code> script</li> <li>Storage migration strategy: Phased approach from MinIO to cloud S3</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#2-emotional-journey-and-persistence","title":"2. Emotional Journey and Persistence","text":"<p>The conversations reveal the human side of infrastructure development: - Moments of frustration: \"I feel like I'm losing my mind here\" - Breakthrough celebrations: \"vault actually unseals!\", \"Sprocket is alive!\" - Persistent problem-solving: Multiple iterations on Vault automation - Mid-project documentation requests: Recognition of knowledge capture needs</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#3-technical-decision-making-process","title":"3. Technical Decision-Making Process","text":"<p>Evidence of systematic decision-making: - Managed services migration (PostgreSQL, S3) driven by operational pain - Security vs automation trade-offs in Vault unsealing - Production-first design philosophy for routing - Iterative testing and validation approach</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#documents-created","title":"Documents Created","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#1-claude_conversation_analysismd","title":"1. <code>CLAUDE_CONVERSATION_ANALYSIS.md</code>","text":"<p>Purpose: High-level analysis of conversation patterns and insights Key Sections: - Executive summary of findings - Detailed technical challenges with conversation evidence - Critical breakthrough moments - Technical patterns and lessons learned - Operational insights and recommendations</p> <p>Value: Provides context and reasoning behind technical decisions that aren't visible in code alone.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#2-technical_challenges_from_claude_conversationsmd","title":"2. <code>TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS.md</code>","text":"<p>Purpose: Practical operational supplement to the runbook Key Sections: - Vault automation and unsealing procedures - Multi-environment routing configuration - Service network connectivity troubleshooting - Environment variable management - Storage migration procedures - Certificate and HTTPS management - Emergency procedures and recovery - Monitoring and alerting setup</p> <p>Value: Step-by-step solutions for problems that were solved during development, ready for operational use.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#how-this-enhances-existing-documentation","title":"How This Enhances Existing Documentation","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#1-complements-the-postmortem","title":"1. Complements the Postmortem","text":"<p>The existing <code>POSTMORTEM.md</code> provides the what and when of the project. The conversation analysis adds the how and why:</p> <ul> <li>Postmortem: \"Vault unsealing was challenging and took 3 days\"</li> <li>Conversations: Shows the 5+ failed attempts, user frustration, and eventual breakthrough</li> <li>Postmortem: \"Multi-environment routing was complex\"</li> <li>Conversations: Reveals the systematic testing of different access patterns and the IP-based routing workaround</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#2-enhances-the-operations-runbook","title":"2. Enhances the Operations Runbook","text":"<p>The existing <code>OPERATIONS_RUNBOOK.md</code> provides comprehensive operational procedures. The technical challenges document adds:</p> <ul> <li>Specific troubleshooting steps for issues encountered during development</li> <li>Emergency procedures tested during the rebuild process</li> <li>Real-world examples of service failures and recoveries</li> <li>Configuration examples that were proven to work</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#3-provides-historical-context","title":"3. Provides Historical Context","text":"<p>Future maintainers can understand: - Why decisions were made: Not just what was implemented, but the reasoning - What didn't work: Failed attempts that should be avoided - Evolution of solutions: How problems were iteratively solved - Operational pain points: Issues that drove architectural changes</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#key-insights-for-future-projects","title":"Key Insights for Future Projects","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#1-technical-insights","title":"1. Technical Insights","text":"<ul> <li>Vault automation: Local bind mounts are acceptable for single-node deployments</li> <li>Multi-environment routing: IP-based routing is a workable hack for development</li> <li>Managed services: Clear operational benefits justify migration costs</li> <li>Network connectivity: Cross-layer communication requires careful network design</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#2-process-insights","title":"2. Process Insights","text":"<ul> <li>Iterative problem-solving: Complex infrastructure requires multiple attempts</li> <li>Documentation timing: Capturing context during development is invaluable</li> <li>Testing approach: Systematic verification of each layer before proceeding</li> <li>Persistence: Technical challenges often require sustained effort over days/weeks</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#3-operational-insights","title":"3. Operational Insights","text":"<ul> <li>Emergency procedures: Tested recovery processes for critical services</li> <li>Monitoring importance: Early detection of issues prevents larger problems</li> <li>Configuration management: Environment variables require careful handling</li> <li>Backup strategies: Multiple backup types needed for different components</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#integration-with-existing-documentation","title":"Integration with Existing Documentation","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#recommended-usage","title":"Recommended Usage","text":"<ol> <li>For new team members: Start with <code>POSTMORTEM.md</code> for overview, then read <code>CLAUDE_CONVERSATION_ANALYSIS.md</code> for context</li> <li>For operations: Use <code>OPERATIONS_RUNBOOK.md</code> as primary reference, supplemented by <code>TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS.md</code> for specific issues</li> <li>For troubleshooting: Technical challenges document provides step-by-step solutions for known problems</li> <li>For planning: Conversation analysis reveals decision-making patterns for future architectural choices</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#cross-references-added","title":"Cross-References Added","text":"<ul> <li>Links to specific sections in existing documents</li> <li>References to git commits mentioned in conversations</li> <li>Connections between technical challenges and operational procedures</li> <li>Historical context for current configuration choices</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#value-proposition","title":"Value Proposition","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#1-preserves-institutional-knowledge","title":"1. Preserves Institutional Knowledge","text":"<p>The conversation history captures the tribal knowledge that would otherwise be lost, including: - Why certain approaches were abandoned - What troubleshooting steps actually worked - How long problems took to solve - What the team learned from failures</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#2-accelerates-future-problem-solving","title":"2. Accelerates Future Problem-Solving","text":"<p>Future issues can be resolved faster because: - Solutions are documented with specific commands and examples - Failed approaches are identified to avoid repetition - Root causes are explained, not just symptoms - Multiple solution options are provided where applicable</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#3-improves-operational-reliability","title":"3. Improves Operational Reliability","text":"<p>Operations teams benefit from: - Proven troubleshooting procedures - Emergency recovery processes tested during development - Configuration examples that are known to work - Understanding of interdependencies between services</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#4-enables-better-decision-making","title":"4. Enables Better Decision-Making","text":"<p>Technical leaders can make better decisions with: - Historical context for architectural choices - Understanding of trade-offs made during development - Knowledge of what caused operational pain - Insight into the effort required for different approaches</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATIONS_SUMMARY/#conclusion","title":"Conclusion","text":"<p>The Claude conversation analysis provides a unique window into the development process that complements the technical documentation with human context and practical solutions. It transforms the project from a series of commits into a learning resource that captures both the technical and experiential knowledge gained during the infrastructure rebuild.</p> <p>These supplementary documents ensure that the hard-won knowledge from the 8-week infrastructure rebuild project is preserved and accessible to future team members, operators, and decision-makers.</p> <p>Next Steps: 1. Review and integrate with existing documentation 2. Update operational procedures based on new insights 3. Share with team members for feedback and validation 4. Use as template for future project documentation 5. Regular updates as new challenges are encountered</p> <p>This analysis demonstrates the value of capturing conversational context during complex technical projects. The insights gained from the Claude conversations provide operational gold that would otherwise be lost in the git history.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/","title":"Claude Conversation Analysis: Sprocket Infrastructure Project","text":"<p>Analysis Date: November 10, 2025 Source: <code>~/.claude.json</code> conversation history Project: Sprocket Infrastructure Rebuild (Sept-Nov 2025)</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#executive-summary","title":"Executive Summary","text":"<p>This analysis extracts key technical insights from Claude conversation history during the Sprocket infrastructure rebuild project. The conversations reveal the iterative problem-solving process, critical breakthroughs, and technical challenges that were resolved through collaboration with Claude.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#key-technical-challenges-identified","title":"Key Technical Challenges Identified","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-vault-automation-and-unsealing-phase-2-the-vault-struggles","title":"1. Vault Automation and Unsealing (Phase 2 - The Vault Struggles)","text":"<p>Challenge: Automating Vault initialization and unsealing in a secure, repeatable way.</p> <p>Conversation Evidence: - Multiple attempts to solve the chicken-and-egg problem of Vault unsealing - User expressed frustration: \"I feel like I'm losing my mind here\" regarding Vault issues - Breakthrough moment: \"vault actually unseals!\" (commit <code>5057afc</code>)</p> <p>Technical Solution: - Local bind mount for storing unseal tokens: <code>global/services/vault/unseal-tokens/</code> - Auto-initialization script: <code>global/services/vault/scripts/auto-initialize.sh</code> - S3 backend for Vault state persistence - Automated unsealing using 3 of 5 unseal keys</p> <p>Key Insight: The conversation shows iterative problem-solving with multiple failed attempts before finding the working solution.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-multi-environment-routing-complexity-phase-5-routing-hell","title":"2. Multi-Environment Routing Complexity (Phase 5 - Routing Hell)","text":"<p>Challenge: Supporting multiple access patterns (localhost, LAN IP, Tailscale, production domain) with a single codebase.</p> <p>Conversation Evidence: - User: \"The platform needed to be accessible via: Local development (.localhost domains), LAN access (Direct IP), Tailscale access, Production (Real domain)\" - Recognition that \"Traefik routing is based on Host header\" was the root cause - Solution implemented in commit <code>7486e02</code>: \"add localhost-based routing and cloud deployment documentation\"</p> <p>Technical Solution: - Conditional routing rules based on configuration - Host-based routing with fallback to IP-based access - Configuration-driven approach separating local vs production settings</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-service-dependency-and-network-issues","title":"3. Service Dependency and Network Issues","text":"<p>Challenge: Services failing to connect to dependencies like RabbitMQ and Redis across different layers.</p> <p>Conversation Evidence: - \"services in this layer_3 compose file to connect to rabbitmq, but now they're getting ACCESS_REFUSED\" - \"Did we forget to connect the networks across the layers?\" - \"layer2_rabbitmq host. Did we forget to connect the networks across the layers?\"</p> <p>Technical Solutions Implemented: - Network connectivity verification between layers - Proper Docker network configuration across layers - Service discovery and DNS resolution fixes</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#4-environment-variable-and-configuration-management","title":"4. Environment Variable and Configuration Management","text":"<p>Challenge: Managing complex environment variables and ensuring consistency across deployments.</p> <p>Conversation Evidence: - Multiple references to <code>generate-env.sh</code> script issues - Problems with JWT_SECRET generation: \"JWT_SECRET always comes out as a poorly formed string\" - Issues with duplicate environment variables: \"The generate-env script still results in us having two REDIS_PASSWORD env vars\" - Token formatting issues: \"INFLUX_ADMIN_TOKEN and FORWARD_AUTH_SECRET always seem to end up with a line break in the middle\"</p> <p>Technical Solutions: - Environment variable standardization - Proper secret generation and formatting - Removal of duplicate variables from Doppler integration</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#5-storage-migration-from-minio-to-cloud-s3","title":"5. Storage Migration from MinIO to Cloud S3","text":"<p>Challenge: Migrating from self-hosted MinIO to managed cloud S3 storage.</p> <p>Conversation Evidence: - Recognition that MinIO was \"causing more problems than it solved\" - Phased migration approach to reduce risk - Dual operation period for safe transition</p> <p>Technical Implementation: - Updated <code>SprocketMinioProvider</code> to support direct cloud credentials - Migration of Vault backend to Digital Ocean Spaces first - Application services migration followed by MinIO removal</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#critical-breakthrough-moments","title":"Critical Breakthrough Moments","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-vault-unsealing-success-september-19-2025","title":"1. Vault Unsealing Success (September 19, 2025)","text":"<p>Conversation: \"vault actually unseals!\" Significance: This was the most challenging technical problem, requiring 5+ attempts and different approaches.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-platform-services-running-october-26-2025","title":"2. Platform Services Running (October 26, 2025)","text":"<p>Conversation: \"Sprocket is alive!\" Significance: All platform services successfully deployed after months of infrastructure work.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-production-deployment-complete-november-8-2025","title":"3. Production Deployment Complete (November 8, 2025)","text":"<p>Conversation: \"Sprocket v1 is finally up and running completely.\" Significance: End of 8-week infrastructure rebuild project.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#technical-patterns-and-lessons-learned","title":"Technical Patterns and Lessons Learned","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-iterative-problem-solving-approach","title":"1. Iterative Problem-Solving Approach","text":"<p>The conversations show a pattern of: - Identify problem - Attempt solution - Test and fail - Learn from failure - Try different approach - Eventually succeed</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-documentation-during-development","title":"2. Documentation During Development","text":"<p>Key insight: User requested documentation creation mid-project: \"I would like you to create a notes file for me on this project... I want to be able to share this with my team\"</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-managed-services-strategy","title":"3. Managed Services Strategy","text":"<p>Clear evolution from self-hosted to managed services: - PostgreSQL: Self-hosted \u2192 Digital Ocean Managed - Storage: MinIO \u2192 Cloud S3 - Result: Reduced operational complexity</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#4-security-vs-automation-trade-offs","title":"4. Security vs Automation Trade-offs","text":"<p>Vault unsealing required balancing: - Security (not storing unseal keys in code/Pulumi) - Automation (no manual intervention required) - Final solution: Local bind mount (acceptable for single-node deployment)</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#technical-debt-and-future-considerations","title":"Technical Debt and Future Considerations","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-environment-variable-complexity","title":"1. Environment Variable Complexity","text":"<p>Multiple conversations about <code>generate-env.sh</code> issues suggest this script became a source of technical debt that required ongoing maintenance.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-ip-based-routing-workaround","title":"2. IP-Based Routing Workaround","text":"<p>The multi-environment routing solution, while functional, is described as a \"workaround\" that may need revisiting for more elegant solutions.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-local-bind-mount-security","title":"3. Local Bind Mount Security","text":"<p>Vault unseal keys stored on local filesystem - acceptable for current deployment but would need cloud KMS for multi-node or higher security requirements.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#operational-insights","title":"Operational Insights","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-deployment-verification","title":"1. Deployment Verification","text":"<p>User consistently used verification scripts like <code>quick-test.sh</code> to validate deployments, showing good operational practices.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-service-health-monitoring","title":"2. Service Health Monitoring","text":"<p>Regular checking of service logs and health status throughout the development process.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-network-troubleshooting","title":"3. Network Troubleshooting","text":"<p>Systematic approach to diagnosing network connectivity issues between services and layers.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#recommendations-for-future-projects","title":"Recommendations for Future Projects","text":""},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#1-start-with-managed-services","title":"1. Start with Managed Services","text":"<p>The migration pattern shows clear benefits of starting with managed services rather than migrating later.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#2-design-for-production-first","title":"2. Design for Production First","text":"<p>Conversations reveal that designing for production domains first, then adding development overrides, is more effective than the reverse.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#3-document-decisions-in-real-time","title":"3. Document Decisions in Real-Time","text":"<p>The request for mid-project documentation shows the value of capturing context while it's fresh.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#4-expect-multiple-iterations","title":"4. Expect Multiple Iterations","text":"<p>The Vault unsealing challenge required 5+ attempts - complex infrastructure problems often need persistent iteration.</p>"},{"location":"departments/development/systems/production-infrastructure/CLAUDE_CONVERSATION_ANALYSIS/#conclusion","title":"Conclusion","text":"<p>The Claude conversation history reveals a complex, iterative infrastructure rebuild project with significant technical challenges. The most valuable insights are:</p> <ol> <li>Persistence pays off - Complex problems (Vault unsealing) require multiple attempts</li> <li>Managed services reduce complexity - Clear pattern of migrating to managed services</li> <li>Documentation during development - Capturing context while fresh is invaluable</li> <li>Iterative problem-solving - Small changes, frequent testing, learning from failures</li> </ol> <p>The conversations show a thoughtful, systematic approach to infrastructure challenges with good operational practices and a willingness to iterate until solutions work reliably.</p> <p>This analysis is based on conversation history from <code>~/.claude.json</code> and represents the collaborative problem-solving process between the user and Claude during the Sprocket infrastructure rebuild project.</p>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/","title":"Sprocket Infrastructure - Context Summary","text":"<p>This document summarizes the current state of the Sprocket infrastructure deployment on the production node, to be referenced when continuing documentation work on a different machine.</p>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#current-deployment-state","title":"Current Deployment State","text":"<p>Status: \u2705 PRODUCTION DEPLOYMENT COMPLETE AND RUNNING</p> <ul> <li>Date Completed: November 8, 2025 (commit a2862ea)</li> <li>Deployment Location: Digital Ocean production node</li> <li>Primary Domain: sprocket.mlesports.gg</li> <li>Infrastructure: Docker Swarm with 3-layer architecture</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#architecture-overview","title":"Architecture Overview","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#three-layer-deployment-structure","title":"Three-Layer Deployment Structure","text":"<ol> <li>Layer 1 (Core Infrastructure) - <code>layer_1/</code></li> <li>Traefik (reverse proxy/ingress with Let's Encrypt)</li> <li>Vault (secrets management with S3 backend on Digital Ocean Spaces)</li> <li>Socket Proxy (secure Docker API access)</li> <li> <p>Base networking</p> </li> <li> <p>Layer 2 (Data Services) - <code>layer_2/</code></p> </li> <li>~~PostgreSQL~~ REMOVED - Now hosted on Digital Ocean Managed Database</li> <li>Redis</li> <li>RabbitMQ</li> <li>InfluxDB</li> <li>Grafana</li> <li>N8n (workflow automation)</li> <li>Neo4j (graph database)</li> <li>Gatus (monitoring)</li> <li>Loki (log aggregation)</li> <li> <p>Telegraf (metrics collection)</p> </li> <li> <p>Platform (Application Services) - <code>platform/</code></p> </li> <li>Sprocket web application</li> <li>Sprocket API</li> <li>Image generation service</li> <li>Discord bot</li> <li>Legacy bot</li> <li>All microservices</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#key-infrastructure-decisions","title":"Key Infrastructure Decisions","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#1-database-migration-to-digital-ocean-managed-postgresql","title":"1. Database Migration to Digital Ocean Managed PostgreSQL","text":"<p>Critical Decision: Removed PostgreSQL from Pulumi-managed infrastructure</p> <ul> <li>Why: Better reliability, automated backups, managed maintenance</li> <li>Impact: PostgreSQL is now an external dependency, not managed by Pulumi</li> <li>Connection: All services connect to <code>sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com:25060</code></li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#2-vault-backend-digital-ocean-spaces-s3-compatible","title":"2. Vault Backend: Digital Ocean Spaces (S3-compatible)","text":"<ul> <li>Vault data persists to Digital Ocean Spaces instead of local storage</li> <li>Bucket: <code>vault-secrets</code></li> <li>Endpoint: <code>https://nyc3.digitaloceanspaces.com</code></li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#3-minio-aws-s3-migration","title":"3. MinIO \u2192 AWS S3 Migration","text":"<ul> <li>Originally used MinIO (self-hosted S3)</li> <li>Migrated to AWS S3 for better reliability</li> <li>Commit: 3b45f27 (October 8, 2025)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#critical-external-dependencies","title":"Critical External Dependencies","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#1-doppler-secrets-management","title":"1. Doppler (Secrets Management)","text":"<p>IMPORTANT: Most secrets and configuration values are stored in Doppler, NOT in Pulumi config.</p> <ul> <li>Doppler project contains all sensitive credentials</li> <li>OAuth secrets (Google, Discord, Epic, Steam)</li> <li>API tokens (Ballchasing, etc.)</li> <li>SMTP credentials</li> <li>Database passwords</li> <li>Application secrets</li> </ul> <p>Without Doppler access, deployment cannot be completed.</p>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#2-github-organization","title":"2. GitHub Organization","text":"<p>Required for: Vault access control and authentication</p> <ul> <li>GitHub teams determine Vault policies</li> <li>GitHub OAuth used for Vault authentication</li> <li>Affects who can access secrets and infrastructure</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#3-digital-ocean-resources","title":"3. Digital Ocean Resources","text":"<ul> <li>Managed PostgreSQL Database: Primary data store</li> <li>Spaces (S3): Vault backend + application storage</li> <li>Droplet/Node: Production deployment target</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#4-dns-configuration","title":"4. DNS Configuration","text":"<ul> <li>Domain: <code>spr.ocket.cloud</code> or <code>sprocket.mlesports.gg</code></li> <li>DNS records must point to production node IP</li> <li>Required for Let's Encrypt certificate provisioning</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#deployment-journey-timeline","title":"Deployment Journey Timeline","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-1-initial-setup-sept-14-19-2025","title":"Phase 1: Initial Setup (Sept 14-19, 2025)","text":"<ul> <li>Started with basic Pulumi infrastructure</li> <li>Layer 1 configuration and Vault setup</li> <li>Major Challenge: Vault bootstrapping and unsealing</li> <li>Multiple iterations to get repeatable Vault initialization</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-2-vault-struggles-sept-19-23-2025","title":"Phase 2: Vault Struggles (Sept 19-23, 2025)","text":"<ul> <li>Vault unsealing automation</li> <li>Secret provisioning from Doppler</li> <li>Integration with services</li> <li>Key Commits:</li> <li><code>5057afc</code>: Vault actually unseals!</li> <li><code>2103358</code>: All tokens working now</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-3-storage-migration-sept-30-oct-8-2025","title":"Phase 3: Storage Migration (Sept 30 - Oct 8, 2025)","text":"<ul> <li>MinIO \u2192 Cloud S3 migration (commit 3f32a74)</li> <li>S3-compatible storage configuration</li> <li>Final MinIO removal (commit 3b45f27)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-4-database-externalization-nov-1-3-2025","title":"Phase 4: Database Externalization (Nov 1-3, 2025)","text":"<ul> <li>Critical Change: Removed PostgreSQL from Pulumi management</li> <li>Migrated to Digital Ocean Managed Database</li> <li>Updated all service configurations to use managed DB</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-5-routing-and-networking-oct-26-27-2025","title":"Phase 5: Routing and Networking (Oct 26-27, 2025)","text":"<ul> <li>Localhost-based routing for development</li> <li>IP-based routing for LAN/Tailscale access</li> <li>Host-based routing for production</li> <li>Commit 7486e02: Major documentation of routing strategies</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#phase-6-production-deployment-nov-4-8-2025","title":"Phase 6: Production Deployment (Nov 4-8, 2025)","text":"<ul> <li>Cloud deployment with real DNS</li> <li>Let's Encrypt certificate provisioning</li> <li>Service health verification</li> <li>Final Commit a2862ea: \"Sprocket v1 is finally up and running completely\"</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#key-configuration-files","title":"Key Configuration Files","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#pulumi-stack-configurations","title":"Pulumi Stack Configurations","text":"<p>Layer 1 (<code>layer_1/Pulumi.layer_1.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\npostgres-port: \"25060\"\nvault-s3-endpoint: https://nyc3.digitaloceanspaces.com\nvault-s3-bucket: vault-secrets\n</code></pre> <p>Layer 2 (<code>layer_2/Pulumi.layer_2.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\ns3-endpoint: nyc3.digitaloceanspaces.com\n</code></pre> <p>Platform (<code>platform/Pulumi.prod.yaml</code>):</p> <pre><code>hostname: sprocket.mlesports.gg\nsubdomain: \"main\"\nimage-tag: main\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\nserver-ip: 192.168.4.39  # LAN access\ntailscale-ip: 100.110.185.84  # Tailscale access\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#project-structure","title":"Project Structure","text":"<pre><code>sprocket-infra/\n\u251c\u2500\u2500 global/                    # Shared code and service definitions\n\u2502   \u251c\u2500\u2500 providers/            # Custom Pulumi providers\n\u2502   \u251c\u2500\u2500 services/             # Service component definitions\n\u2502   \u251c\u2500\u2500 helpers/              # Utility functions\n\u2502   \u2514\u2500\u2500 refs/                 # Cross-stack references\n\u251c\u2500\u2500 layer_1/                  # Core infrastructure\n\u251c\u2500\u2500 layer_2/                  # Data services\n\u251c\u2500\u2500 platform/                 # Application services\n\u251c\u2500\u2500 scripts/                  # Bootstrap and helper scripts\n\u2502   \u2514\u2500\u2500 vault-secrets/        # Vault secret initialization\n\u251c\u2500\u2500 README.md                 # Basic setup instructions\n\u251c\u2500\u2500 CLOUD_DEPLOYMENT.md       # Cloud deployment guide\n\u251c\u2500\u2500 LOCAL_ACCESS.md           # Local development guide\n\u251c\u2500\u2500 verify-deployment.sh      # Comprehensive verification script\n\u2514\u2500\u2500 quick-test.sh            # Quick health check script\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#major-services","title":"Major Services","text":""},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#infrastructure-services-layer-1","title":"Infrastructure Services (Layer 1)","text":"<ul> <li>Traefik: HTTPS routing, Let's Encrypt certificates, reverse proxy</li> <li>Vault: Secrets management with GitHub auth</li> <li>Socket Proxy: Secure Docker socket access for Traefik</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#data-services-layer-2","title":"Data Services (Layer 2)","text":"<ul> <li>Redis: Caching and pub/sub</li> <li>RabbitMQ: Message queue</li> <li>InfluxDB: Time-series metrics</li> <li>Grafana: Metrics visualization</li> <li>Neo4j: Graph database for relationships</li> <li>N8n: Workflow automation</li> <li>Gatus: Service monitoring</li> <li>Loki: Log aggregation</li> <li>Telegraf: Metrics collection</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#application-services-platform","title":"Application Services (Platform)","text":"<ul> <li>Sprocket Web: Main web UI (Next.js)</li> <li>Sprocket API: Backend API</li> <li>Image Generation: Image creation service</li> <li>Discord Bot: Main Discord integration</li> <li>Legacy Bot: Backward compatibility bot</li> <li>Chatwoot: Customer support chat</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#current-deployment-method","title":"Current Deployment Method","text":"<ol> <li>Prerequisites: Doppler access, GitHub org membership, Pulumi backend access</li> <li>Deploy Layer 1: <code>cd layer_1 &amp;&amp; pulumi up</code></li> <li>Initialize Vault Secrets: <code>cd scripts &amp;&amp; ./bootstrap-vault-secrets.sh</code></li> <li>Deploy Layer 2: <code>cd layer_2 &amp;&amp; pulumi up</code></li> <li>Deploy Platform: <code>cd platform &amp;&amp; pulumi up</code></li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#known-issues-and-gotchas","title":"Known Issues and Gotchas","text":"<ol> <li>Vault Unsealing: Vault must be unsealed after deployment using stored unseal keys</li> <li>DNS Propagation: Let's Encrypt certificate provisioning requires DNS to be correct first</li> <li>Service Discovery: Traefik takes 30-60 seconds to discover new services</li> <li>Doppler Dependency: Cannot deploy without Doppler secrets access</li> <li>Database Connection: All services need managed PostgreSQL credentials</li> <li>Node Version: Requires Node.js 16 for some Pulumi operations</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#whats-working","title":"What's Working","text":"<p>\u2705 All services running on production node \u2705 HTTPS with Let's Encrypt certificates \u2705 Vault secrets management \u2705 Database connections to managed PostgreSQL \u2705 Service mesh and networking \u2705 Monitoring and logging infrastructure \u2705 Application services serving users</p>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#documentation-gaps-to-be-filled","title":"Documentation Gaps (To Be Filled)","text":"<p>The following documentation needs to be created:</p> <ol> <li>Postmortem Documentation</li> <li>Detailed deployment journey</li> <li>Technical challenges and solutions</li> <li> <p>Lessons learned</p> </li> <li> <p>Architecture Documentation</p> </li> <li>Detailed architecture diagrams</li> <li>Service dependency graphs</li> <li>Network topology</li> <li> <p>Security architecture</p> </li> <li> <p>Deployment Guide</p> </li> <li>Step-by-step with expected outputs</li> <li>Troubleshooting for each step</li> <li>Configuration reference</li> <li> <p>Verification procedures</p> </li> <li> <p>Component Documentation</p> </li> <li>Deep dive on each service</li> <li>Configuration options</li> <li>Integration points</li> <li> <p>Health checks</p> </li> <li> <p>Operations Runbook</p> </li> <li>Day 2 operations</li> <li>Backup and restore</li> <li>Scaling procedures</li> <li> <p>Incident response</p> </li> <li> <p>Troubleshooting Guide</p> </li> <li>Common issues and solutions</li> <li>Debugging techniques</li> <li>Log locations</li> <li>Recovery procedures</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#next-steps-for-documentation","title":"Next Steps for Documentation","text":"<ol> <li>Move to workstation with better editing environment</li> <li>Use this context to write comprehensive documentation</li> <li>Organize into MkDocs-ready markdown files</li> <li>Include diagrams and examples</li> <li>Make it detailed enough for someone new to rebuild from scratch</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/CONTEXT_SUMMARY/#notes-for-future-maintainers","title":"Notes for Future Maintainers","text":"<ul> <li>This infrastructure has been built iteratively over several months</li> <li>Many decisions were made based on trial and error</li> <li>The git history is invaluable for understanding \"why\"</li> <li>Always check Doppler for the latest secrets</li> <li>The managed PostgreSQL database is the single source of truth for data</li> <li>Vault is critical - protect those unseal keys!</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/","title":"Sprocket Infrastructure Deployment Guide","text":"<p>Version: 1.1 Last Updated: December 4, 2025 Difficulty: Advanced Estimated Time: 4-6 hours (first-time deployment)</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Pre-Deployment Checklist</li> <li>Environment Setup</li> <li>Layer 1 Deployment</li> <li>Secret Provisioning</li> <li>Layer 2 Deployment</li> <li>Platform Deployment</li> <li>Verification</li> <li>Troubleshooting</li> <li>Rollback Procedures</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#deployment-architecture-overview","title":"Deployment Architecture Overview","text":"<p>\u26a0\ufe0f Important Context: The deployment diagram below shows a generic cloud architecture pattern. Our actual deployment uses Docker Swarm on Digital Ocean, not GCP Cloud Run. The diagram illustrates the conceptual architecture that could apply to cloud deployments, but the specific technologies differ from our current stack.</p> <p></p> <p>Our Actual Deployment: - Platform: Docker Swarm on a Digital Ocean Droplet (single-node, can scale to multi-node) - Services: Containerized microservices (Sprocket Web, API, Discord Bot, background workers) - Data: Managed PostgreSQL (Digital Ocean), self-hosted Redis, S3-compatible storage (Digital Ocean Spaces) - Infrastructure: Traefik (reverse proxy), Vault (secrets), monitoring stack (Grafana, InfluxDB) - CI/CD: Pulumi (Infrastructure as Code), manual deployments via CLI</p> <p>The diagram represents a future cloud-native deployment option or a conceptual reference architecture.</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#business-context-planning","title":"Business Context &amp; Planning","text":"<p>For Decision Makers &amp; Project Managers</p> <p>Before beginning a deployment, understand the resource investment required. This section provides non-technical context about what a deployment entails.</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#time-investment","title":"Time Investment","text":"Deployment Stage First-Time Experienced Engineer Notes Prerequisites Setup 2-3 hours 30-60 minutes One-time setup per engineer External Services 1-2 hours 15-30 minutes Creating managed database, S3 buckets Layer 1 Deployment 1-2 hours 15-30 minutes Core infrastructure (Vault, Traefik) Secret Provisioning 30-60 minutes 10-15 minutes Copying secrets from Doppler to Vault Layer 2 Deployment 1-2 hours 20-40 minutes Data services (Redis, databases, monitoring) Platform Deployment 1-2 hours 20-40 minutes Application services Verification &amp; Testing 1-2 hours 30-60 minutes Confirming everything works Total Estimated Time 6-12 hours 2-4 hours Spread across 1-2 days typically <p>Why the Time Range? - First deployment: Includes learning curve, troubleshooting, understanding architecture - Subsequent deployments: Familiarity with process significantly reduces time - Team deployment: With 2 engineers working together, can complete in 4-6 hours</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#required-skills","title":"Required Skills","text":"<p>You'll need engineers with experience in:</p> Skill Area Importance Why It's Needed Docker &amp; Docker Swarm Critical All services run in Docker containers orchestrated by Swarm Command Line / Bash Critical Most operations happen via terminal commands Infrastructure as Code (Pulumi/Terraform) High All infrastructure defined in Pulumi TypeScript code Networking Basics Medium Understanding DNS, ports, proxies helpful for troubleshooting Linux System Administration Medium SSH access, file permissions, system troubleshooting Cloud Services (Digital Ocean/AWS) Medium Setting up managed database, S3 storage <p>Minimum Team Size: 1 experienced engineer can do the deployment, but 2 is recommended for first deployment.</p> <p>Skill Level: This is not a beginner-friendly deployment. Engineer(s) should have: - 2+ years of DevOps/infrastructure experience - Previous experience with Docker - Comfort with command-line operations - Ability to debug when things don't work as expected</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#cost-breakdown","title":"Cost Breakdown","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#one-time-setup-costs","title":"One-Time Setup Costs","text":"Item Cost Notes Domain Name $10-15/year If not already owned (e.g., <code>sprocket.mlesports.gg</code>) SSL Certificate $0 Let's Encrypt provides free certificates Initial Engineering Time $800-2,400 8-12 hours @ $100-200/hour (varies by location/seniority) Total One-Time $810-2,415 Mostly engineering time"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#monthly-recurring-costs","title":"Monthly Recurring Costs","text":"Component Monthly Cost Annual Cost Can Optimize? Digital Ocean Droplet (4 vCPUs, 8GB RAM) $96 $1,152 \u2713 Yes - can downsize to $72/mo if usage is low Managed PostgreSQL (Basic tier) $15 $180 \u2717 No - critical service Block Storage Volumes (100GB total) $15 $180 ~ Maybe - review usage after 1 month Spaces Storage (S3) $5 $60 ~ Maybe - implement cleanup policy Domain Name ~$1 $12 \u2717 No Total Monthly $132 $1,584/year <p>Potential Savings: - If actual usage is consistently &lt;50% CPU/memory, could downsize Droplet to $72/month - Estimated potential savings: $15-30/month ($180-360/year)</p> <p>Cost Comparison to Alternatives:</p> Alternative Approach Est. Monthly Cost Trade-offs Current Setup (managed services) $132/month Low maintenance, reliable, documented All Self-Hosted (no managed DB) $100/month Saves $30/mo but adds 10-15 hours/month maintenance Full AWS/GCP (managed everything) $300-500/month More features, auto-scaling, but 3-4x cost Serverless (Vercel/Netlify/Lambda) $150-300/month Different architecture, would require code rewrite <p>ROI Analysis: Even at $100/hour, 10 hours of maintenance costs more than the $30/month saved by self-hosting the database. Managed services are cost-effective for teams with limited DevOps bandwidth.</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#risk-factors","title":"Risk Factors","text":"<p>What Could Extend Timeline?</p> Risk Probability Impact Mitigation DNS propagation delays Medium +1-4 hours Set up DNS 24 hours before deployment Let's Encrypt rate limits Low +1-24 hours Use staging certs for testing, production once Missing credentials/access Medium +1-2 hours Complete access checklist before starting Docker Hub image pull failures Low +30-60 min Verify image access beforehand First-time Pulumi issues Medium (first-timers) +1-3 hours Review Pulumi docs, have backup support Network/firewall issues Low +1-2 hours Test connectivity, document firewall rules Vault unsealing problems Low +30-60 min Follow Vault initialization carefully <p>Highest Impact Risks: 1. Not having required access credentials - Have all API keys, passwords, tokens ready before starting 2. Unfamiliarity with tools - First deployment should be paired with someone experienced 3. Production vs. test environment confusion - Use local/staging for first attempt</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#prerequisites-checklist-for-project-managers","title":"Prerequisites Checklist (for Project Managers)","text":"<p>Before scheduling a deployment, ensure:</p> <ul> <li>[ ] Budget Approved: $132/month recurring + ~$1,000 one-time engineering</li> <li>[ ] Engineering Time Allocated: Block 8-12 hours for first deployment (can span 1-2 days)</li> <li>[ ] Access Granted:</li> <li>[ ] Digital Ocean account with billing configured</li> <li>[ ] Doppler account with secrets populated</li> <li>[ ] GitHub organization membership</li> <li>[ ] Docker Hub access to private repositories</li> <li>[ ] DNS provider access (to create A records)</li> <li>[ ] Server Ready: Production server provisioned (or local dev environment ready)</li> <li>[ ] Backup Engineer Available: In case primary engineer gets blocked</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#what-happens-after-deployment","title":"What Happens After Deployment?","text":"<p>Immediate (First 24 hours): - Monitor service health - Watch for errors in logs - Verify all features work (login, match creation, Discord bot) - Test external integrations (OAuth, Ballchasing API)</p> <p>First Week: - Monitor resource usage (CPU, memory, disk) - Identify any services using excessive resources - Fine-tune service replica counts if needed - Set up monitoring alerts</p> <p>First Month: - Review costs vs. budget - Analyze usage patterns - Right-size infrastructure if possible (downsize droplet if underutilized) - Implement any missing monitoring/alerting</p> <p>Ongoing: - Monthly: Review costs, check for security updates - Quarterly: Test disaster recovery procedure - As Needed: Deploy application updates (typically 15-30 minutes per deploy)</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#when-not-to-deploy","title":"When NOT to Deploy","text":"<p>Delay deployment if: - Required credentials/access not available (will cause delays mid-deployment) - Engineer unfamiliar with Docker and no backup support available - Production server doesn't meet minimum specs (4 CPU, 8GB RAM) - Budget not yet approved for monthly recurring costs - During critical business periods (deploy during low-usage windows) - Friday afternoon or before long weekend (no time to address issues)</p> <p>Best Time to Deploy: - Monday-Wednesday (full week to address issues) - Morning/early afternoon (full day available if problems arise) - Low-usage period for your application - When 2 engineers available (one primary, one backup)</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#prerequisites","title":"Prerequisites","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#required-access","title":"Required Access","text":"<p>Before starting, ensure you have access to:</p> <ul> <li>[ ] Doppler project with all required secrets</li> <li>[ ] GitHub Organization (<code>SprocketBot</code>) membership</li> <li>[ ] Digital Ocean account with:</li> <li>[ ] Managed PostgreSQL database provisioned</li> <li>[ ] Spaces (S3) access configured</li> <li>[ ] Droplet or production node access</li> <li>[ ] Pulumi Backend access (Pulumi Cloud or S3 backend)</li> <li>[ ] Docker Hub credentials for private images</li> <li>[ ] DNS Provider access (for production deployment)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#required-software","title":"Required Software","text":"<p>Install the following on your deployment machine:</p> <pre><code># Node.js 16.x (required for Pulumi operations)\nnvm install 16\nnvm use 16\n\n# Pulumi CLI\ncurl -fsSL https://get.pulumi.com | sh\n\n# Vault CLI\nwget https://releases.hashicorp.com/vault/1.10.0/vault_1.10.0_linux_amd64.zip\nunzip vault_1.10.0_linux_amd64.zip\nsudo mv vault /usr/local/bin/\n\n# jq (for JSON processing)\nsudo apt-get install jq  # Debian/Ubuntu\n# or\nbrew install jq  # macOS\n\n# Doppler CLI\ncurl -Ls https://cli.doppler.com/install.sh | sh\n\n# Verify installations\nnode --version   # Should be v16.x.x\npulumi version\nvault version\njq --version\ndoppler --version\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#production-node-requirements","title":"Production Node Requirements","text":"<p>Minimum Specifications: - CPU: 4 cores - RAM: 8GB (16GB recommended) - Disk: 100GB SSD - Network: 1Gbps - OS: Ubuntu 20.04+ or similar</p> <p>Software Requirements:</p> <pre><code># Docker (with Swarm mode)\ncurl -fsSL https://get.docker.com | sh\ndocker swarm init\n\n# Verify Docker Swarm is active\ndocker info | grep Swarm\n# Should show: Swarm: active\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#1-external-services-configuration","title":"1. External Services Configuration","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#a-digital-ocean-managed-postgresql","title":"A. Digital Ocean Managed PostgreSQL","text":"<p>Create Database: 1. Log into Digital Ocean console 2. Navigate to Databases \u2192 Create Database 3. Choose PostgreSQL, select version 13+ 4. Choose plan (Basic $15/month minimum) 5. Choose datacenter region (NYC3 recommended) 6. Name: <code>sprocketbot-postgres</code> 7. Wait for provisioning (~5 minutes)</p> <p>Get Connection Details:</p> <pre><code>Host: sprocketbot-postgres-XXXXX-do-user-XXXXX-0.j.db.ondigitalocean.com\nPort: 25060\nDatabase: defaultdb\nUser: doadmin\nPassword: [from console]\nSSL Mode: require\n</code></pre> <p>Save these details - you'll need them for Pulumi configuration.</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#b-digital-ocean-spaces-s3","title":"B. Digital Ocean Spaces (S3)","text":"<p>Create Spaces: 1. Navigate to Spaces \u2192 Create Space 2. Choose datacenter (NYC3 recommended) 3. Create two spaces:    - <code>vault-secrets</code> (for Vault backend)    - <code>sprocket-storage</code> (for application files) 4. Generate Spaces access key:    - API \u2192 Spaces Access Keys \u2192 Generate New Key    - Save Access Key and Secret Key</p> <p>Configuration:</p> <pre><code>Endpoint: nyc3.digitaloceanspaces.com\nAccess Key: DO004ZHPV38R8C9Q46XG\nSecret Key: [from console]\nBucket (Vault): vault-secrets\nBucket (App): sprocket-storage\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#c-doppler-setup","title":"C. Doppler Setup","text":"<p>Login and Configure:</p> <pre><code># Login to Doppler\ndoppler login\n\n# Set up project\ndoppler setup\n\n# Verify secrets\ndoppler secrets\n</code></pre> <p>Required Secrets in Doppler:</p> <pre><code># OAuth Credentials\nGOOGLE_CLIENT_ID\nGOOGLE_CLIENT_SECRET\nDISCORD_CLIENT_ID\nDISCORD_CLIENT_SECRET\nEPIC_CLIENT_ID\nEPIC_CLIENT_SECRET\nSTEAM_API_KEY\n\n# API Tokens\nBALLCHASING_API_TOKEN\n\n# Application Secrets\nCHATWOOT_HMAC_KEY\n\n# Storage Credentials\nMINIO_ACCESS_KEY  # For S3 access\nMINIO_SECRET_KEY  # For S3 secret\n</code></pre> <p>Export for Bootstrap Script:</p> <pre><code># Export all secrets to environment\neval $(doppler secrets download --no-file --format env-no-quotes)\n\n# Verify exports\necho $GOOGLE_CLIENT_ID  # Should show value\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#d-dns-configuration-production-only","title":"D. DNS Configuration (Production Only)","text":"<p>For Production Deployment:</p> <p>Required DNS records pointing to your production node IP:</p> <pre><code>A     sprocket.mlesports.gg            \u2192 &lt;production-ip&gt;\nA     api.sprocket.mlesports.gg        \u2192 &lt;production-ip&gt;\nA     vault.sprocket.mlesports.gg      \u2192 &lt;production-ip&gt;\nA     traefik.sprocket.mlesports.gg    \u2192 &lt;production-ip&gt;\n\n# Or use wildcard\nA     *.sprocket.mlesports.gg          \u2192 &lt;production-ip&gt;\n</code></pre> <p>Verify DNS Propagation:</p> <pre><code># Wait 5-10 minutes after creating records\nnslookup sprocket.mlesports.gg\n# Should return your production IP\n\ndig sprocket.mlesports.gg\n# Should show A record pointing to your IP\n</code></pre> <p>For Local Development:</p> <pre><code># Add to /etc/hosts instead\necho '127.0.0.1 sprocket.localhost api.sprocket.localhost vault.localhost' | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#2-firewall-configuration","title":"2. Firewall Configuration","text":"<p>Production Node Firewall Rules:</p> <pre><code># Allow HTTP (for Let's Encrypt)\nsudo ufw allow 80/tcp\n\n# Allow HTTPS\nsudo ufw allow 443/tcp\n\n# Allow SSH (restrict to your IP)\nsudo ufw allow from YOUR_IP to any port 22\n\n# Enable firewall\nsudo ufw enable\n</code></pre> <p>Cloud Provider Firewall (if using Digital Ocean, AWS, etc.): - Allow inbound: 80/tcp, 443/tcp - Allow outbound: all (or specific ports)</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#3-repository-setup","title":"3. Repository Setup","text":"<pre><code># Clone repository\ngit clone git@github.com:SprocketBot/sprocket-infra.git\ncd sprocket-infra\n\n# Verify structure\nls -la\n# Should see: layer_1/, layer_2/, platform/, global/, scripts/\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#4-pulumi-backend-configuration","title":"4. Pulumi Backend Configuration","text":"<p>Option A: Pulumi Cloud (Recommended for teams)</p> <pre><code># Login to Pulumi Cloud\npulumi login\n\n# Verify\npulumi whoami\n</code></pre> <p>Option B: S3 Backend (Self-hosted)</p> <pre><code># Create ~/.aws/credentials\ncat &gt; ~/.aws/credentials &lt;&lt;EOF\n[default]\naws_access_key_id = YOUR_ACCESS_KEY\naws_secret_access_key = YOUR_SECRET_KEY\nregion = us-east-1\nEOF\n\n# Login to S3 backend\npulumi login \"s3://your-bucket/pulumi?endpoint=your-endpoint\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#environment-setup","title":"Environment Setup","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#configuration-files-overview","title":"Configuration Files Overview","text":"<p>The infrastructure uses three Pulumi stacks, each with its own configuration:</p> <pre><code>layer_1/Pulumi.layer_1.yaml    - Layer 1 config\nlayer_2/Pulumi.layer_2.yaml    - Layer 2 config\nplatform/Pulumi.prod.yaml      - Platform config\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#layer-1-configuration","title":"Layer 1 Configuration","text":"<pre><code>cd layer_1\n\n# Select or create stack\npulumi stack select layer_1\n# If stack doesn't exist:\n# pulumi stack init layer_1\n\n# Configure hostname\npulumi config set hostname sprocket.mlesports.gg\n# For local development:\n# pulumi config set hostname localhost\n\n# Configure PostgreSQL connection (from Digital Ocean)\npulumi config set postgres-host sprocketbot-postgres-XXXXX-do-user-XXXXX-0.j.db.ondigitalocean.com\npulumi config set postgres-port 25060\n\n# Configure Vault S3 backend (from Digital Ocean Spaces)\npulumi config set vault-s3-bucket vault-secrets\npulumi config set vault-s3-endpoint https://nyc3.digitaloceanspaces.com\npulumi config set --secret vault-s3-access-key DO004ZHPV38R8C9Q46XG\npulumi config set --secret vault-s3-secret-key YOUR_SECRET_KEY_HERE\n\n# Configure Docker Hub access (for private images)\npulumi config set docker-username asaxplayinghorse\npulumi config set --secret docker-access-token YOUR_DOCKER_TOKEN_HERE\n\n# Configure PostgreSQL password\npulumi config set --secret postgres-password YOUR_DB_PASSWORD_HERE\n\n# View configuration\npulumi config\n</code></pre> <p>Expected Output:</p> <pre><code>KEY                      VALUE\nhostname                 sprocket.mlesports.gg\npostgres-host            sprocketbot-postgres-...\npostgres-port            25060\nvault-s3-bucket          vault-secrets\nvault-s3-endpoint        https://nyc3.digitaloceanspaces.com\nvault-s3-access-key      [secret]\nvault-s3-secret-key      [secret]\ndocker-username          asaxplayinghorse\ndocker-access-token      [secret]\npostgres-password        [secret]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#layer-2-configuration","title":"Layer 2 Configuration","text":"<pre><code>cd ../layer_2\n\n# Select or create stack\npulumi stack select layer_2\n# If doesn't exist:\n# pulumi stack init layer_2\n\n# Configure hostname (must match Layer 1)\npulumi config set hostname sprocket.mlesports.gg\n\n# Configure PostgreSQL (must match Layer 1)\npulumi config set postgres-host sprocketbot-postgres-XXXXX-do-user-XXXXX-0.j.db.ondigitalocean.com\n\n# Configure S3 endpoint for applications\npulumi config set s3-endpoint nyc3.digitaloceanspaces.com\n\n# View configuration\npulumi config\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#platform-configuration","title":"Platform Configuration","text":"<pre><code>cd ../platform\n\n# Select or create stack\npulumi stack select prod\n# If doesn't exist:\n# pulumi stack init prod\n\n# Configure hostname (must match Layer 1 &amp; 2)\npulumi config set hostname sprocket.mlesports.gg\n\n# Configure subdomain (environment name)\npulumi config set subdomain main\n\n# Configure image tag (Docker image version to deploy)\npulumi config set image-tag main\n\n# Configure PostgreSQL (must match Layer 1 &amp; 2)\npulumi config set postgres-host sprocketbot-postgres-XXXXX-do-user-XXXXX-0.j.db.ondigitalocean.com\n\n# Optional: For local development, add IP-based access\n# pulumi config set server-ip 192.168.4.39\n# pulumi config set tailscale-ip 100.110.185.84\n\n# View configuration\npulumi config\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#layer-1-deployment","title":"Layer 1 Deployment","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-1-deploy-layer-1","title":"Step 1: Deploy Layer 1","text":"<p>Purpose: Deploy core infrastructure (Traefik, Vault, Socket Proxy)</p> <pre><code>cd layer_1\n\n# Preview changes\npulumi preview\n\n# Expected output:\n# + traefik-d3127fd\n# + vault-service\n# + socket-proxy\n# + traefik-ingress network\n# + vault-network\n# + 3 configs, 2 volumes\n\n# Deploy\npulumi up\n\n# Confirm when prompted:\n# Do you want to perform this update? yes\n</code></pre> <p>Expected Duration: 2-5 minutes</p> <p>Success Indicators:</p> <pre><code>Updating (layer_1)\n\n     Type                     Name              Status\n +   pulumi:pulumi:Stack      layer_1-layer_1   created\n +   \u251c\u2500 docker:index:Network  traefik-network   created\n +   \u251c\u2500 docker:index:Network  vault-network     created\n +   \u251c\u2500 docker:index:Volume   traefik-data      created\n +   \u251c\u2500 docker:index:Service  traefik           created\n +   \u251c\u2500 docker:index:Service  vault             created\n +   \u2514\u2500 docker:index:Service  socket-proxy      created\n\nResources:\n    + 7 created\n\nDuration: 3m15s\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-2-verify-layer-1-deployment","title":"Step 2: Verify Layer 1 Deployment","text":"<pre><code># Check services are running\ndocker service ls\n\n# Expected output:\n# ID             NAME            MODE         REPLICAS   IMAGE\n# abc123...      traefik-...     replicated   1/1        traefik:v2.6.1\n# def456...      vault-service   replicated   1/1        vault:1.10.0\n# ghi789...      socket-proxy    replicated   1/1        tecnativa/docker-socket-proxy\n\n# Check Traefik is accessible\ncurl -I http://localhost:80\n# HTTP/1.1 404 (404 is expected - no routes configured yet)\n\ncurl -k -I https://localhost:443\n# HTTP/2 404\n\n# Check Vault is accessible\ncurl -k https://localhost/vault\n# Or if using proper domain:\n# curl https://vault.sprocket.mlesports.gg\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-3-vault-initialization","title":"Step 3: Vault Initialization","text":"<p>Check Vault Status:</p> <pre><code># Get Vault container\nVAULT_CONTAINER=$(docker ps -q -f name=vault)\n\n# Check Vault logs\ndocker logs $VAULT_CONTAINER\n\n# Should see:\n# \"Vault initialized\"\n# \"Vault unsealed\"\n</code></pre> <p>Locate Unseal Keys:</p> <pre><code># Unseal keys are stored at:\ncat global/services/vault/unseal-tokens/unseal_tokens.txt\n\n# Save these keys securely!\n# You'll need them if you ever need to manually unseal Vault\n</code></pre> <p>Get Root Token:</p> <pre><code># Extract root token\ncat global/services/vault/unseal-tokens/root_token.txt\n\n# Or from unseal_tokens.txt:\ngrep 'Initial Root Token' global/services/vault/unseal-tokens/unseal_tokens.txt\n</code></pre> <p>Set Vault Environment Variables:</p> <pre><code># For production\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg\n\n# For local development\n# export VAULT_ADDR=http://vault.localhost\n\n# Set root token\nexport VAULT_TOKEN=$(cat global/services/vault/unseal-tokens/root_token.txt)\n\n# Verify Vault access\nvault status\n\n# Expected output:\n# Sealed: false\n# Key Shares: 5\n# Key Threshold: 3\n# Initialized: true\n</code></pre> <p>Common Vault Issues:</p> <p>Issue: Vault is sealed</p> <pre><code># Check status\nvault status\n# Sealed: true\n\n# Unseal manually\nvault operator unseal\n# Enter first unseal key from unseal_tokens.txt\n# Repeat 2 more times with different keys\n</code></pre> <p>Issue: Can't connect to Vault</p> <pre><code># Check service is running\ndocker service ps vault-service\n\n# Check logs\ndocker service logs vault-service\n\n# Verify network\ndocker network inspect traefik-ingress\n# Should show vault service connected\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#secret-provisioning","title":"Secret Provisioning","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-1-prepare-secrets-from-doppler","title":"Step 1: Prepare Secrets from Doppler","text":"<pre><code>cd scripts\n\n# Login to Doppler (if not already)\ndoppler login\n\n# Select correct project/config\ndoppler setup\n\n# Export secrets to environment\neval $(doppler secrets download --no-file --format env-no-quotes)\n\n# Verify exports\necho $GOOGLE_CLIENT_ID  # Should show value\necho $DISCORD_CLIENT_ID  # Should show value\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-2-run-bootstrap-script","title":"Step 2: Run Bootstrap Script","text":"<p>Purpose: Provision secrets from Doppler into Vault</p> <pre><code># Ensure Vault environment variables are set\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg  # or http://vault.localhost\nexport VAULT_TOKEN=$(cat ../global/services/vault/unseal-tokens/root_token.txt)\n\n# Run bootstrap script\n./bootstrap-vault-secrets.sh\n</code></pre> <p>Expected Output:</p> <pre><code>Bootstrapping Vault secrets for environment: sprocket\nVault address: https://vault.sprocket.mlesports.gg\n\nChecking required environment variables...\n\u2713 All required environment variables are set!\n\nCreating secrets in Vault...\n\nCreating/updating secret at: platform/sprocket/manual/oauth/google\n\u2713 Successfully created/updated: platform/sprocket/manual/oauth/google\n\nCreating/updating secret at: platform/sprocket/manual/oauth/discord\n\u2713 Successfully created/updated: platform/sprocket/manual/oauth/discord\n\nCreating/updating secret at: platform/sprocket/manual/oauth/epic\n\u2713 Successfully created/updated: platform/sprocket/manual/oauth/epic\n\nCreating/updating secret at: platform/sprocket/manual/oauth/steam\n\u2713 Successfully created/updated: platform/sprocket/manual/oauth/steam\n\nCreating/updating secret at: platform/ballchasing\n\u2713 Successfully created/updated: platform/ballchasing\n\nCreating/updating secret at: platform/sprocket/chatwoot\n\u2713 Successfully created/updated: platform/sprocket/chatwoot\n\nCreating/updating secret at: infrastructure/data/minio/root\n\u2713 Successfully created/updated: infrastructure/data/minio/root\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\u2713 All secrets have been successfully bootstrapped!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-3-verify-secrets","title":"Step 3: Verify Secrets","text":"<pre><code># Verify secrets were created\nvault kv get platform/sprocket/manual/oauth/google\n# Should show clientId and clientSecret\n\nvault kv get platform/sprocket/manual/oauth/discord\n# Should show client_id and client_secret\n\nvault kv get platform/ballchasing\n# Should show token\n\n# List all secrets\nvault kv list platform/sprocket/manual/oauth\n# Should show: discord, epic, google, steam\n</code></pre> <p>Common Issues:</p> <p>Issue: <code>Error making API request... 403 Forbidden</code></p> <pre><code># Your Vault token doesn't have permission\n# Use root token:\nexport VAULT_TOKEN=$(cat ../global/services/vault/unseal-tokens/root_token.txt)\n\n# Or create a policy with appropriate permissions\n</code></pre> <p>Issue: Missing environment variable</p> <pre><code># Check which variable is missing from error message\n# Export from Doppler again:\neval $(doppler secrets download --no-file --format env-no-quotes)\n\n# Or manually export:\nexport GOOGLE_CLIENT_ID=\"your-value-here\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#layer-2-deployment","title":"Layer 2 Deployment","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-1-deploy-layer-2","title":"Step 1: Deploy Layer 2","text":"<p>Purpose: Deploy data services (Redis, RabbitMQ, InfluxDB, etc.)</p> <pre><code>cd ../layer_2\n\n# Preview changes\npulumi preview\n\n# Expected resources:\n# + Redis service\n# + RabbitMQ service\n# + InfluxDB service\n# + Grafana service\n# + N8n service\n# + Neo4j service\n# + Gatus service\n# + Loki service\n# + Telegraf service\n# + Vault policies\n# + Multiple networks, volumes, configs\n\n# Deploy\npulumi up\n</code></pre> <p>Expected Duration: 3-7 minutes</p> <p>Success Indicators:</p> <pre><code>Updating (layer_2)\n\n     Type                              Name                 Status\n +   pulumi:pulumi:Stack               layer_2-layer_2      created\n +   \u251c\u2500 SprocketBot:Services:Redis     layer2redis          created\n +   \u251c\u2500 SprocketBot:Services:RabbitMQ  rabbitmq             created\n +   \u251c\u2500 SprocketBot:Services:InfluxDB  influx               created\n +   \u251c\u2500 SprocketBot:Services:Grafana   grafana              created\n +   \u251c\u2500 SprocketBot:Services:N8n       n8n                  created\n +   \u251c\u2500 SprocketBot:Services:Neo4j     neo4j                created\n +   \u251c\u2500 SprocketBot:Services:Gatus     gatus-internal       created\n +   \u2514\u2500 ... (additional resources)\n\nResources:\n    + 25 created\n\nDuration: 5m42s\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-2-verify-layer-2-deployment","title":"Step 2: Verify Layer 2 Deployment","text":"<pre><code># Check all services are running\ndocker service ls | grep layer_2\n\n# Expected output (similar to):\n# layer2redis-...        replicated   1/1        redis:6-alpine\n# rabbitmq-...           replicated   1/1        rabbitmq:3-management\n# influx-...             replicated   1/1        influxdb:2.7\n# grafana-...            replicated   1/1        grafana/grafana\n# ...\n\n# Check service health\ndocker service ps layer2redis-redis-primary\n# Should show \"Running\" state\n\n# Check logs for errors\ndocker service logs layer2redis-redis-primary --tail 50\n# Should show successful startup\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-3-test-layer-2-services","title":"Step 3: Test Layer 2 Services","text":"<p>Redis:</p> <pre><code># Get Redis container\nREDIS_CONTAINER=$(docker ps -q -f name=layer2redis)\n\n# Test Redis connection\ndocker exec $REDIS_CONTAINER redis-cli ping\n# Expected: PONG\n\n# Test with auth (if password configured)\nREDIS_PASSWORD=$(vault kv get -field=password infrastructure/data/redis)\ndocker exec $REDIS_CONTAINER redis-cli -a $REDIS_PASSWORD ping\n</code></pre> <p>RabbitMQ:</p> <pre><code># Access RabbitMQ management UI (if exposed)\n# https://rabbitmq.sprocket.mlesports.gg\n# Or check via API\ncurl -k https://localhost/rabbitmq/api/overview\n</code></pre> <p>InfluxDB:</p> <pre><code># Access InfluxDB UI (if exposed)\n# https://influx.sprocket.mlesports.gg\n\n# Or check health\nINFLUX_CONTAINER=$(docker ps -q -f name=influx)\ndocker exec $INFLUX_CONTAINER influx ping\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#platform-deployment","title":"Platform Deployment","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-1-deploy-platform","title":"Step 1: Deploy Platform","text":"<p>Purpose: Deploy application services (web, API, Discord bot, microservices)</p> <pre><code>cd ../platform\n\n# Preview changes\npulumi preview\n\n# Expected resources:\n# + Sprocket Web service\n# + Sprocket API service\n# + Discord Bot service\n# + Image Generation services (frontend + backend)\n# + Multiple microservices (notification, analytics, matchmaking, etc.)\n# + Platform network\n# + Multiple secrets, configs\n\n# Deploy\npulumi up\n</code></pre> <p>Expected Duration: 5-10 minutes</p> <p>Success Indicators:</p> <pre><code>Updating (prod)\n\n     Type                                   Name                        Status\n +   pulumi:pulumi:Stack                    platform-prod               created\n +   \u251c\u2500 SprocketBot:Platform                prod                        created\n +   \u2502  \u251c\u2500 SprocketBot:Service              prod-sprocket-web           created\n +   \u2502  \u251c\u2500 SprocketBot:Service              prod-sprocket-core          created\n +   \u2502  \u251c\u2500 SprocketBot:Service              prod-discord-bot            created\n +   \u2502  \u251c\u2500 SprocketBot:Service              prod-image-generation       created\n +   \u2502  \u251c\u2500 SprocketBot:Service              prod-notification-service   created\n +   \u2502  \u2514\u2500 ... (additional services)\n\nResources:\n    + 35 created\n\nDuration: 8m23s\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-2-verify-platform-deployment","title":"Step 2: Verify Platform Deployment","text":"<pre><code># Check all platform services are running\ndocker service ls | grep prod\n\n# Expected output:\n# prod-sprocket-web-service                 replicated   1/1   asaxplayinghorse/sprocket-web:main\n# prod-sprocket-core-service                replicated   1/1   asaxplayinghorse/sprocket-core:main\n# prod-discord-bot-service                  replicated   1/1   asaxplayinghorse/sprocket-discord-bot:main\n# ...\n\n# Check specific service status\ndocker service ps prod-sprocket-web-service\n\n# Should show:\n# ID             NAME                     IMAGE                              NODE      DESIRED STATE   CURRENT STATE\n# abc123...      prod-sprocket-web...     asaxplayinghorse/sprocket-web...   node1     Running         Running 2 minutes ago\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#step-3-monitor-service-startup","title":"Step 3: Monitor Service Startup","text":"<p>Services may take several minutes to fully start. Monitor logs:</p> <pre><code># Watch web service logs\ndocker service logs -f prod-sprocket-web-service\n\n# Watch API service logs\ndocker service logs -f prod-sprocket-core-service\n\n# Watch Discord bot logs\ndocker service logs -f prod-discord-bot-service\n</code></pre> <p>Successful Startup Indicators:</p> <p>Web Service:</p> <pre><code>[Nest] INFO [NestFactory] Starting Nest application...\n[Nest] INFO [InstanceLoader] AppModule dependencies initialized\n[Nest] INFO Server listening on port 3000\n</code></pre> <p>API Service:</p> <pre><code>[Nest] INFO [NestFactory] Starting Nest application...\n[Nest] INFO [GraphQLModule] Mapped {/graphql, POST} route\n[Nest] INFO Database connection established\n[Nest] INFO Server listening on port 3001\n</code></pre> <p>Discord Bot:</p> <pre><code>INFO: Discord bot logged in as SprocketBot#1234\nINFO: Connected to Discord gateway\nINFO: Ready to receive commands\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#verification","title":"Verification","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#automated-verification","title":"Automated Verification","text":"<pre><code># Run quick test script\n./quick-test.sh\n\n# Run comprehensive verification\n./verify-deployment.sh\n</code></pre> <p>Expected Output (quick-test.sh):</p> <pre><code>=== Quick Sprocket Deployment Test ===\n\nTesting Traefik on port 80... OK (HTTP 301)\nTesting Traefik on port 443... OK (HTTP 200)\nChecking DNS for sprocket.mlesports.gg... Resolves to 127.0.0.1\nChecking web service container... Running (abc123...)\nChecking Traefik router for web service... Router configured\n\n=== Summary ===\nServices are configured! Access at:\n  http://sprocket.mlesports.gg\n  https://sprocket.mlesports.gg\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#manual-verification","title":"Manual Verification","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#1-test-httphttps-redirect","title":"1. Test HTTP\u2192HTTPS Redirect","text":"<pre><code>curl -I http://sprocket.mlesports.gg\n\n# Expected output:\n# HTTP/1.1 301 Moved Permanently\n# Location: https://sprocket.mlesports.gg/\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#2-test-https-access","title":"2. Test HTTPS Access","text":"<pre><code>curl -I https://sprocket.mlesports.gg\n\n# Expected output:\n# HTTP/2 200\n# content-type: text/html\n# ... (headers)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#3-test-api-endpoint","title":"3. Test API Endpoint","text":"<pre><code>curl -X POST https://api.sprocket.mlesports.gg/graphql \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ __schema { types { name } } }\"}'\n\n# Expected: GraphQL schema response (JSON)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#4-test-service-discovery","title":"4. Test Service Discovery","text":"<pre><code># Check Traefik router configuration\nTRAEFIK_CONTAINER=$(docker ps -q -f name=traefik)\ndocker exec $TRAEFIK_CONTAINER wget -qO- http://localhost:8080/api/http/routers | jq\n\n# Should show routers for:\n# - sprocket-web\n# - sprocket-core (API)\n# - image-generation\n# - vault\n# - traefik dashboard\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#5-test-database-connectivity","title":"5. Test Database Connectivity","text":"<pre><code># From API service container\nAPI_CONTAINER=$(docker ps -q -f name=prod-sprocket-core)\n\ndocker exec $API_CONTAINER node -e \"\nconst pg = require('pg');\nconst client = new pg.Client({\n  host: 'sprocketbot-postgres-...',\n  port: 25060,\n  database: 'defaultdb',\n  user: 'doadmin',\n  password: process.env.DB_PASSWORD,\n  ssl: { rejectUnauthorized: false }\n});\nclient.connect()\n  .then(() =&gt; console.log('\u2713 Database connected'))\n  .catch(err =&gt; console.error('\u2717 Database error:', err.message));\n\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#6-test-s3-storage","title":"6. Test S3 Storage","text":"<pre><code># Test S3 connectivity\naws s3 ls --endpoint-url https://nyc3.digitaloceanspaces.com s3://sprocket-storage/\n\n# Or using aws-cli in a service container\ndocker exec $API_CONTAINER aws s3 ls s3://sprocket-storage/ --endpoint-url https://nyc3.digitaloceanspaces.com\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#7-browser-tests","title":"7. Browser Tests","text":"<p>Web UI: - Navigate to https://sprocket.mlesports.gg - Should see Sprocket homepage - Try logging in with Google/Discord OAuth</p> <p>Traefik Dashboard: - Navigate to https://traefik.sprocket.mlesports.gg - Should see Traefik dashboard (requires auth) - Check routers, services, middleware</p> <p>Vault UI: - Navigate to https://vault.sprocket.mlesports.gg - Should see Vault login page - Can login with GitHub OAuth or root token</p> <p>Grafana (Unified Logs &amp; Metrics): - Navigate to https://grafana.sprocket.mlesports.gg - Should see Grafana login - Default credentials (if not changed): admin/admin - Unified Interface: Access both metrics (InfluxDB/Telegraf) and logs (Loki) from single dashboard - Use Explore view to query logs with LogQL: <code>{service=\"prod-sprocket-core-service\"} |= \"ERROR\"</code> - Use Dashboards to view real-time metrics and system health</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#8-direct-master-node-access-advanced-debugging","title":"8. Direct Master Node Access (Advanced Debugging)","text":"<p>For low-level debugging, you can access the Digital Ocean master node directly:</p> <p>Via Digital Ocean Console: 1. Log into https://cloud.digitalocean.com 2. Navigate to: Droplets \u2192 Select your droplet 3. Click \"Console\" (web-based SSH terminal) 4. Login and run debugging commands:    <code>bash    docker service ls              # List all services    docker service logs &lt;service&gt;  # View service logs    docker stats --no-stream       # Check resource usage    docker system df               # Check disk usage</code></p> <p>Via SSH:</p> <pre><code>ssh root@&lt;droplet-ip&gt;\n# Or with key: ssh -i ~/.ssh/your-key root@&lt;droplet-ip&gt;\n</code></pre> <p>Common Debugging Commands:</p> <pre><code># Exec into a running container\nCONTAINER_ID=$(docker ps -q -f name=prod-sprocket-core)\ndocker exec -it $CONTAINER_ID sh\n\n# Check system resources\ndf -h                  # Disk usage\nfree -h                # Memory usage\ndocker system df       # Docker storage\n\n# Network debugging\ndocker network inspect traefik-ingress\nnslookup vault.sprocket.mlesports.gg\n</code></pre> <p>See OPERATIONS_RUNBOOK.md for detailed debugging procedures.</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#common-issues","title":"Common Issues","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#issue-1-services-not-starting","title":"Issue 1: Services Not Starting","text":"<p>Symptoms:</p> <pre><code>docker service ls\n# Shows 0/1 replicas\n</code></pre> <p>Diagnosis:</p> <pre><code># Check service status\ndocker service ps &lt;service-name&gt; --no-trunc\n\n# Common failure reasons:\n# - Image pull failure\n# - Resource constraints\n# - Health check failure\n# - Configuration error\n</code></pre> <p>Solutions:</p> <p>Image Pull Failure:</p> <pre><code># Check Docker Hub authentication\ndocker login\n# Enter credentials\n\n# Verify image exists\ndocker pull asaxplayinghorse/sprocket-web:main\n\n# Update service\ndocker service update --force &lt;service-name&gt;\n</code></pre> <p>Resource Constraints:</p> <pre><code># Check node resources\ndocker node ls\ndocker node inspect self | jq '.[0].Description.Resources'\n\n# Check running containers\ndocker stats --no-stream\n\n# If low on resources:\n# - Scale down non-critical services\n# - Upgrade node (more CPU/RAM)\n# - Add more nodes to swarm\n</code></pre> <p>Health Check Failure:</p> <pre><code># Check service logs\ndocker service logs &lt;service-name&gt; --tail 100\n\n# Common causes:\n# - Database not accessible\n# - Missing secrets\n# - Port already in use\n# - Dependency not ready\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#issue-2-404-not-found","title":"Issue 2: 404 Not Found","text":"<p>Symptoms:</p> <pre><code>curl https://sprocket.mlesports.gg\n# HTTP/2 404\n</code></pre> <p>Diagnosis:</p> <pre><code># Check Traefik routers\nTRAEFIK_CONTAINER=$(docker ps -q -f name=traefik)\ndocker exec $TRAEFIK_CONTAINER wget -qO- http://localhost:8080/api/http/routers | jq '.[] | select(.rule | contains(\"sprocket\"))'\n\n# Should show router with rule matching your domain\n</code></pre> <p>Solutions:</p> <p>Router Not Found:</p> <pre><code># Check service labels\ndocker service inspect prod-sprocket-web-service | jq '.[0].Spec.TaskTemplate.ContainerSpec.Labels'\n\n# Should include Traefik labels like:\n# \"traefik.enable\": \"true\"\n# \"traefik.http.routers.sprocket-web.rule\": \"Host(`sprocket.mlesports.gg`)\"\n\n# If missing, service needs to be redeployed\ncd platform\npulumi up\n</code></pre> <p>Wrong Hostname:</p> <pre><code># Check Pulumi config\npulumi config get hostname\n# Should match your domain\n\n# Update if wrong\npulumi config set hostname sprocket.mlesports.gg\npulumi up\n</code></pre> <p>Orphaned Service Tasks:</p> <pre><code># Sometimes old service versions linger\ndocker service update --force prod-sprocket-web-service\n\n# Or recreate service\ncd platform\npulumi destroy --target &lt;service-resource&gt;\npulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#issue-3-certificate-errors","title":"Issue 3: Certificate Errors","text":"<p>Symptoms:</p> <pre><code>curl https://sprocket.mlesports.gg\n# SSL certificate problem: unable to get local issuer certificate\n</code></pre> <p>Diagnosis:</p> <pre><code># Check certificate issuer\necho | openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg 2&gt;/dev/null | openssl x509 -noout -issuer\n\n# Should show:\n# issuer=C=US, O=Let's Encrypt, CN=R3\n\n# If shows \"TRAEFIK DEFAULT CERT\":\n# - Let's Encrypt hasn't issued cert yet\n# - DNS not pointing to server\n# - Let's Encrypt rate limited\n</code></pre> <p>Solutions:</p> <p>DNS Not Configured:</p> <pre><code># Verify DNS\nnslookup sprocket.mlesports.gg\n# Should return your server IP\n\n# If not, update DNS records\n# Wait 5-10 minutes for propagation\n</code></pre> <p>Let's Encrypt Rate Limited:</p> <pre><code># Check Traefik logs\ndocker service logs traefik-... | grep -i \"acme\\|letsencrypt\"\n\n# If rate limited:\n# - Wait 1 hour (soft limit resets)\n# - Use staging environment temporarily\n# - Check you're not requesting too many certs\n</code></pre> <p>Firewall Blocking Port 80:</p> <pre><code># Let's Encrypt HTTP-01 challenge needs port 80\n# Check firewall\nsudo ufw status\n\n# Should show:\n# 80/tcp ALLOW\n\n# If blocked:\nsudo ufw allow 80/tcp\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#issue-4-vault-sealed","title":"Issue 4: Vault Sealed","text":"<p>Symptoms:</p> <pre><code>vault status\n# Sealed: true\n</code></pre> <p>Solution:</p> <pre><code># Unseal Vault manually\nvault operator unseal\n# Enter first unseal key from unseal_tokens.txt\n\nvault operator unseal\n# Enter second unseal key\n\nvault operator unseal\n# Enter third unseal key\n\n# Check status\nvault status\n# Sealed: false\n</code></pre> <p>Prevent Future Sealing:</p> <pre><code># Check auto-unseal script\ncat global/services/vault/scripts/auto-initialize.sh\n\n# Verify bind mount is working\ndocker service inspect vault-service | jq '.[0].Spec.TaskTemplate.ContainerSpec.Mounts'\n\n# Should show:\n# {\n#   \"Type\": \"bind\",\n#   \"Source\": \"/root/sprocket-infra/global/services/vault/unseal-tokens\",\n#   \"Target\": \"/vault/unseal-tokens\"\n# }\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#issue-5-database-connection-failed","title":"Issue 5: Database Connection Failed","text":"<p>Symptoms:</p> <pre><code># Service logs show:\n# Error: connect ECONNREFUSED\n# Or: FATAL: password authentication failed\n</code></pre> <p>Solutions:</p> <p>Check Connection Details:</p> <pre><code># Verify Pulumi config\ncd layer_1\npulumi config get postgres-host\npulumi config get postgres-port\n\n# Should match Digital Ocean connection info\n</code></pre> <p>Check Password:</p> <pre><code># Password should be in Pulumi config\npulumi config get postgres-password --show-secrets\n\n# Or in Vault\nvault kv get infrastructure/data/postgres\n</code></pre> <p>Test Connection from Node:</p> <pre><code># Install PostgreSQL client\nsudo apt-get install postgresql-client\n\n# Test connection\nPGPASSWORD='your-password' psql -h sprocketbot-postgres-... -p 25060 -U doadmin -d defaultdb\n\n# If successful, connection details are correct\n# If fails, check:\n# - IP whitelist on Digital Ocean\n# - Network connectivity\n# - Password\n</code></pre> <p>Update IP Whitelist:</p> <pre><code># In Digital Ocean console:\n# 1. Go to your database\n# 2. Settings \u2192 Trusted Sources\n# 3. Add your production node IP\n# 4. Save\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#rollback-platform","title":"Rollback Platform","text":"<pre><code>cd platform\n\n# Export current state (backup)\npulumi stack export &gt; backup-platform-$(date +%Y%m%d-%H%M%S).json\n\n# Preview destroy\npulumi destroy --preview\n\n# Destroy platform\npulumi destroy\n\n# Or rollback to previous state\npulumi stack import &lt; backup-platform-TIMESTAMP.json\npulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#rollback-layer-2","title":"Rollback Layer 2","text":"<pre><code>cd layer_2\n\n# Export state\npulumi stack export &gt; backup-layer2-$(date +%Y%m%d-%H%M%S).json\n\n# Destroy\npulumi destroy\n</code></pre> <p>\u26a0\ufe0f Warning: Destroying Layer 2 will delete data services. Ensure backups exist!</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#rollback-layer-1","title":"Rollback Layer 1","text":"<pre><code>cd layer_1\n\n# Export state\npulumi stack export &gt; backup-layer1-$(date +%Y%m%d-%H%M%S).json\n\n# Destroy\npulumi destroy\n</code></pre> <p>\u26a0\ufe0f Warning: Destroying Layer 1 will make entire platform inaccessible!</p>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#emergency-rollback","title":"Emergency Rollback","text":"<p>If deployment is completely broken:</p> <pre><code># Stop all services\ndocker service rm $(docker service ls -q)\n\n# Clean up networks\ndocker network prune -f\n\n# Clean up volumes (\u26a0\ufe0f DELETES DATA!)\ndocker volume prune -f\n\n# Start fresh deployment\ncd layer_1 &amp;&amp; pulumi up\ncd ../layer_2 &amp;&amp; pulumi up\ncd ../platform &amp;&amp; pulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#post-deployment","title":"Post-Deployment","text":""},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#security-checklist","title":"Security Checklist","text":"<ul> <li>[ ] Change default passwords (Grafana, etc.)</li> <li>[ ] Rotate Vault root token</li> <li>[ ] Enable Vault audit logging</li> <li>[ ] Configure firewall rules</li> <li>[ ] Set up SSL certificate monitoring</li> <li>[ ] Review service logs for errors</li> <li>[ ] Test disaster recovery procedure</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#monitoring-setup","title":"Monitoring Setup","text":"<ul> <li>[ ] Configure Grafana dashboards</li> <li>[ ] Set up alerting (PagerDuty, etc.)</li> <li>[ ] Configure log retention (Loki)</li> <li>[ ] Set up uptime monitoring (Gatus)</li> <li>[ ] Enable metric collection (Telegraf)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#backup-setup","title":"Backup Setup","text":"<ul> <li>[ ] Configure automated database backups</li> <li>[ ] Test database restore</li> <li>[ ] Document Vault recovery procedure</li> <li>[ ] Export Pulumi state regularly</li> <li>[ ] Store secrets backup securely</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DEPLOYMENT_GUIDE/#next-steps","title":"Next Steps","text":"<p>After successful deployment:</p> <ol> <li>Test Application Functionality</li> <li>User registration</li> <li>OAuth login</li> <li>API queries</li> <li> <p>Discord bot commands</p> </li> <li> <p>Performance Tuning</p> </li> <li>Monitor resource usage</li> <li>Optimize database queries</li> <li>Configure caching</li> <li> <p>Adjust service replicas if needed</p> </li> <li> <p>Documentation</p> </li> <li>Document custom configurations</li> <li>Create runbooks for common operations</li> <li> <p>Update team wiki</p> </li> <li> <p>Team Onboarding</p> </li> <li>Share access credentials</li> <li>Provide architecture overview</li> <li>Conduct deployment walkthrough</li> </ol> <p>Deployment Complete! \ud83c\udf89</p> <p>Your Sprocket infrastructure is now live and serving users.</p> <p>For operational procedures, see OPERATIONS_RUNBOOK.md.</p> <p>For troubleshooting, see the Troubleshooting section above or TECHNICAL_CHALLENGES.md.</p> <p>Document Version: 1.1 Last Updated: December 4, 2025 Maintained By: Infrastructure Team</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/","title":"Documentation Review: Outsider Perspective Analysis","text":"<p>Review Date: November 10, 2025 Reviewer Perspective: Complete outsider with no prior context Status: Improvements Implemented</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#executive-summary","title":"Executive Summary","text":"<p>The documentation package is comprehensive and well-structured for someone familiar with the project. However, from an outsider's perspective, there were critical context gaps that could cause significant confusion. The primary issues have been addressed.</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#issues-identified-resolved","title":"Issues Identified &amp; Resolved","text":"<p>\u2705 Fixed: Architecture Diagram Mismatch - Problem: Generated diagrams referenced Unity game, FastAPI services - completely different tech stack - Impact: Readers would be confused about what Sprocket actually is - Solution: Regenerated diagrams with correct Rocket League platform context (Next.js, GraphQL, Discord Bot)</p> <p>\u2705 Fixed: Missing \"What is Sprocket\" Context in Technical Docs - Problem: ARCHITECTURE.md jumped straight into technical details - Impact: Outsiders don't know they're reading about a league management platform, not a game - Solution: Added \"What You're Looking At\" section explaining Sprocket vs. Rocket League distinction</p> <p>\u2705 Fixed: GCP/Cloud Run Diagrams Don't Match Actual Stack - Problem: Deployment/Monitoring diagrams show GCP Cloud Run, but we use Docker Swarm on Digital Ocean - Impact: Readers implementing from docs would use wrong technologies - Solution: Added prominent warnings explaining diagrams are conceptual, listed actual tech stack</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#additional-recommendations-for-future-improvement","title":"Additional Recommendations (For Future Improvement)","text":""},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#1-add-a-readers-guide-section-to-00-start-heremd","title":"1. Add a \"Reader's Guide\" Section to 00-START-HERE.md","text":"<p>Current State: Good, but assumes some baseline knowledge</p> <p>Suggested Addition:</p> <pre><code>## Who Should Read This Documentation?\n\n### \u2705 This Documentation is For You If:\n- You're joining the Sprocket infrastructure team\n- You need to deploy/maintain the Sprocket platform\n- You're a stakeholder wanting to understand costs and architecture\n- You're troubleshooting a production issue\n- You're planning infrastructure changes or scaling\n\n### \u274c This Documentation is NOT For You If:\n- You want to learn how to play Rocket League (see: Rocket League tutorials)\n- You want to use Sprocket as a player (see: sprocket.mlesports.gg user docs)\n- You're building application features (see: sprocket-core repo docs)\n- You're looking for game development docs (Sprocket doesn't make games)\n\n### \ud83e\udd14 Not Sure Which Documentation You Need?\n- **Using Sprocket (player)**: Visit sprocket.mlesports.gg and check the help section\n- **Developing features (engineer)**: See the sprocket-core repository\n- **Running/deploying infrastructure (ops/devops)**: You're in the right place! Keep reading.\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#2-add-consistent-assumed-knowledge-sections","title":"2. Add Consistent \"Assumed Knowledge\" Sections","text":"<p>Problem: Each document assumes different levels of expertise without stating it.</p> <p>Recommendation: Add to the top of each major doc:</p> <p>Example for ARCHITECTURE.md:</p> <pre><code>## Assumed Knowledge for This Document\n\n**Required** (you must know these):\n- \u2705 What Docker containers are\n- \u2705 Basic networking concepts (IP addresses, ports, DNS)\n- \u2705 What databases and caches do\n- \u2705 How web applications work (client/server model)\n\n**Helpful** (makes it easier but not required):\n- Docker Swarm orchestration\n- Infrastructure as Code (IaC) concepts\n- Microservices architecture patterns\n\n**Not Required** (we explain these):\n- Specific services (Traefik, Vault, Grafana)\n- Pulumi syntax\n- Our specific architecture decisions\n\n**If you're missing required knowledge**: Read GLOSSARY.md first, or refer to it as you read this document.\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#3-add-visual-you-are-here-maps","title":"3. Add Visual \"You Are Here\" Maps","text":"<p>Problem: Long documents lose readers - they don't know how sections relate.</p> <p>Recommendation: Add navigation context at the start of major sections:</p> <pre><code>## Layer 2: Data Services\n\n\ud83d\udccd **Where We Are in the Architecture**:\n</code></pre> <p>Layer 1 (Infrastructure) \u2190 YOU DEPLOYED THIS     \u2193 Layer 2 (Data Services) \u2190 YOU ARE HERE     \u2193 Platform (Applications) \u2190 COMING NEXT</p> <pre><code>\n**What You Need Running Before This**:\n- \u2705 Layer 1 deployed and healthy\n- \u2705 Vault unsealed and accessible\n- \u2705 Traefik routing working\n\n**What Depends on This Layer**:\n- \u23f3 Platform services (they need Redis, RabbitMQ, etc.)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#4-add-why-this-matters-context-to-technical-sections","title":"4. Add \"Why This Matters\" Context to Technical Sections","text":"<p>Problem: Outsiders don't understand why we made certain choices.</p> <p>Current: \"We use Redis for caching.\" Better: \"We use Redis for caching because without it, every user request would hit PostgreSQL directly, causing 10-50ms response times instead of 1ms.\"</p> <p>Good Examples Already in Docs: - \u2705 Redis section explains \"Real Impact: 80-90% reduction in database load\" - \u2705 RabbitMQ section explains \"API response time: 50ms vs. 5-10 seconds\" - \u2705 Why NestJS section compares to alternatives</p> <p>Recommendation: Ensure every major technology choice has a \"Why This Matters\" or \"Real Impact\" callout.</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#5-add-common-misconceptions-section-to-00-start-heremd","title":"5. Add \"Common Misconceptions\" Section to 00-START-HERE.md","text":"<p>Recommendation:</p> <pre><code>## Common Misconceptions About Sprocket\n\n### \u274c Misconception: \"Sprocket is a game\"\n**Reality**: Sprocket is a *platform* for managing esports leagues. Players play Rocket League (the game), but use Sprocket to register, schedule matches, track stats, etc. Think ESPN.com for Rocket League leagues.\n\n### \u274c Misconception: \"This infrastructure runs the Rocket League game\"\n**Reality**: Rocket League runs on players' computers/consoles (made by Psyonix/Epic). This infrastructure runs the *league management website and Discord bot*.\n\n### \u274c Misconception: \"We're deploying to Google Cloud (GCP)\"\n**Reality**: Some diagrams show GCP as a conceptual reference, but we actually deploy to Digital Ocean using Docker Swarm.\n\n### \u274c Misconception: \"I can just deploy this and start using Sprocket\"\n**Reality**: You need external dependencies (Doppler, Digital Ocean account, DNS provider, GitHub org access). See EXTERNALITIES_AND_DEPENDENCIES.md first.\n\n### \u274c Misconception: \"This is a beginner-friendly deployment\"\n**Reality**: This is an advanced deployment requiring 2+ years DevOps experience. Not suitable for beginners.\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#6-add-decision-trees-for-navigation","title":"6. Add Decision Trees for Navigation","text":"<p>Problem: Readers don't know which document to read.</p> <p>Recommendation: Add to 00-START-HERE.md:</p> <pre><code>## Documentation Decision Tree\n\n**Start Here** \u2192 Answer these questions:\n\n1. **What do you need to do?**\n   - \ud83d\udd27 Deploy Sprocket from scratch \u2192 [DEPLOYMENT_GUIDE.md](#)\n   - \ud83d\udcca Understand how it all works \u2192 [ARCHITECTURE.md](#)\n   - \ud83d\udea8 Fix a production issue \u2192 [OPERATIONS_RUNBOOK.md](#) \u2192 Incident Response\n   - \ud83d\udcc8 Plan for scaling/growth \u2192 [ARCHITECTURE.md](#) \u2192 Scalability section\n   - \ud83d\udcb0 Understand costs \u2192 [00-START-HERE.md](#) \u2192 Cost Breakdown\n   - \ud83c\udf93 Onboard as new team member \u2192 [Recommended reading order below](#)\n\n2. **What's your experience level?**\n   - \ud83d\udc76 Junior engineer / New to infrastructure \u2192 Start with [GLOSSARY.md](#)\n   - \ud83e\uddd1\u200d\ud83d\udcbc Non-technical manager \u2192 [ARCHITECTURE.md](#) \u2192 Executive Summary\n   - \ud83d\udc68\u200d\ud83d\udcbb Experienced engineer \u2192 Jump to [ARCHITECTURE.md](#) directly\n   - \ud83c\udfd7\ufe0f Infrastructure architect \u2192 [POSTMORTEM.md](#) for decisions + [TECHNICAL_CHALLENGES.md](#)\n\n3. **How much time do you have?**\n   - \u26a1 15 minutes \u2192 [CONTEXT_SUMMARY.md](#)\n   - \ud83d\udd50 1 hour \u2192 [ARCHITECTURE.md](#) \u2192 Executive Summary + Overview\n   - \ud83d\udcda Half day \u2192 [Full recommended reading order](#)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#7-add-prerequisites-checklist-at-document-start","title":"7. Add \"Prerequisites Checklist\" at Document Start","text":"<p>Problem: Readers start reading without ensuring they have access to needed systems.</p> <p>Recommendation: Add to DEPLOYMENT_GUIDE.md and OPERATIONS_RUNBOOK.md:</p> <pre><code>## Before You Begin - Access Checklist\n\n**Stop! Don't proceed until you can check \u2705 for all required items.**\n\n### Required Access (Cannot proceed without these):\n- [ ] SSH access to production Droplet (root or sudo)\n- [ ] GitHub organization access (SprocketBot)\n- [ ] Doppler project access (sprocket-infra)\n- [ ] Pulumi access token\n- [ ] Digital Ocean account access (team or personal)\n\n### Required Knowledge:\n- [ ] I understand what Docker containers are\n- [ ] I can use command-line tools (bash, ssh)\n- [ ] I have read the GLOSSARY.md for unfamiliar terms\n\n### Required Local Setup:\n- [ ] Docker installed locally\n- [ ] Pulumi CLI installed\n- [ ] `kubectl` or `docker` CLI access configured\n- [ ] Git configured with SSH keys\n\n### Optional but Helpful:\n- [ ] Vault CLI installed locally\n- [ ] Database client (psql or DBeaver)\n- [ ] VPN access (if required)\n\n**If you can't check all Required items**: Stop and get access first. See EXTERNALITIES_AND_DEPENDENCIES.md for details.\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#what-was-done-well-keep-these-patterns","title":"What Was Done Well (Keep These Patterns)","text":""},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#executive-summaries-for-non-technical-readers","title":"\u2705 Executive Summaries for Non-Technical Readers","text":"<ul> <li>Example: ARCHITECTURE.md starts with \"For Decision Makers &amp; Non-Technical Stakeholders\"</li> <li>Impact: Managers can understand costs, risks, and value without reading technical details</li> <li>Keep doing this: Every major doc should have a non-technical summary</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#why-we-use-x-explanations","title":"\u2705 \"Why We Use X\" Explanations","text":"<ul> <li>Examples:</li> <li>\"Why Redis\" - explains without it, every request hits database (slow)</li> <li>\"Why RabbitMQ\" - explains synchronous vs. asynchronous processing</li> <li>\"Why Managed PostgreSQL\" - shows ROI ($15/month vs. 10 hours maintenance)</li> <li>Keep doing this: Technical decisions should justify themselves</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#real-world-analogies","title":"\u2705 Real-World Analogies","text":"<ul> <li>Examples:</li> <li>Docker containers = shipping containers (standardized)</li> <li>Redis = keeping frequently-used files on your desk vs. filing cabinet</li> <li>Reverse proxy = receptionist routing visitors to right department</li> <li>Keep doing this: Makes concepts accessible to junior engineers</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#code-examples-with-expected-outputs","title":"\u2705 Code Examples with Expected Outputs","text":"<ul> <li>Example: DEPLOYMENT_GUIDE shows commands AND what you should see</li> <li>Impact: Readers know if they did it right</li> <li>Keep doing this: Every operational command should show expected output</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#time-estimates","title":"\u2705 Time Estimates","text":"<ul> <li>Examples:</li> <li>\"Time to Read: 45-60 minutes\"</li> <li>\"First-time deployment: 6-12 hours, Experienced: 2-4 hours\"</li> <li>Impact: Readers can plan their time</li> <li>Keep doing this: Helps with resource planning</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#comprehensive-glossary","title":"\u2705 Comprehensive Glossary","text":"<ul> <li>GLOSSARY.md is excellent - plain-language definitions</li> <li>Real-world analogies help non-experts</li> <li>Categorized by topic makes it easy to find terms</li> <li>Keep doing this: Update glossary as new terms are introduced</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#completed-november-10-2025","title":"\u2705 Completed (November 10, 2025)","text":"<ul> <li>[x] Fixed architecture diagram mismatch (Unity \u2192 Sprocket/Rocket League)</li> <li>[x] Added \"What You're Looking At\" to ARCHITECTURE.md</li> <li>[x] Added warnings about GCP diagrams vs. actual Docker Swarm stack</li> <li>[x] Regenerated diagrams with correct context</li> <li>[x] Updated diagram descriptions in all docs</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#recommended-for-future-priority-order","title":"\ud83d\udccb Recommended for Future (Priority Order)","text":"<p>High Priority (Do within 1 week): 1. [ ] Add \"Common Misconceptions\" section to 00-START-HERE.md 2. [ ] Add \"Who Should Read This\" section to 00-START-HERE.md 3. [ ] Add \"Prerequisites Checklist\" to DEPLOYMENT_GUIDE.md</p> <p>Medium Priority (Do within 1 month): 4. [ ] Add \"Assumed Knowledge\" sections to each major doc 5. [ ] Add decision tree for navigation to 00-START-HERE.md 6. [ ] Ensure every tech choice has \"Why This Matters\" explanation</p> <p>Low Priority (Do within 3 months): 7. [ ] Add \"You Are Here\" navigation aids to long documents 8. [ ] Create visual flowcharts for common tasks 9. [ ] Add troubleshooting decision trees</p>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#metrics-documentation-quality","title":"Metrics: Documentation Quality","text":""},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#before-improvements","title":"Before Improvements","text":"<ul> <li>\u274c Architecture diagrams: Incorrect tech stack (Unity/FastAPI vs. Next.js/NestJS)</li> <li>\u274c Context assumption: Readers had to infer Sprocket = league platform, not game</li> <li>\u274c Stack mismatch: GCP diagrams vs. Docker Swarm reality</li> <li>\u26a0\ufe0f Navigation: No clear \"who/what/why\" at document starts</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#after-improvements","title":"After Improvements","text":"<ul> <li>\u2705 Architecture diagrams: Accurate Rocket League platform context</li> <li>\u2705 Clear distinction: \"Sprocket is NOT a game\" stated explicitly</li> <li>\u2705 Stack clarity: Warnings about conceptual diagrams, actual stack documented</li> <li>\u2705 Context bridges: Each doc explains what you're reading and why</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#remaining-gaps-recommendations-above","title":"Remaining Gaps (Recommendations Above)","text":"<ul> <li>\ud83d\udccb Prerequisites checklists (readers start without verifying access)</li> <li>\ud83d\udccb Common misconceptions (prevent confusion before it happens)</li> <li>\ud83d\udccb Decision trees (help readers find right doc)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/DOCUMENTATION_REVIEW_RECOMMENDATIONS/#conclusion","title":"Conclusion","text":"<p>The documentation package is production-ready and high-quality for the target audience (infrastructure engineers joining the team). The improvements made address the most critical gaps for outsiders:</p> <ol> <li>\u2705 Diagrams now match actual technology stack</li> <li>\u2705 Clear explanation of what Sprocket is (platform, not game)</li> <li>\u2705 Warnings about conceptual vs. actual deployment</li> </ol> <p>Recommended next steps: Implement the \"High Priority\" items from the checklist above to make the documentation even more accessible to complete outsiders and junior team members.</p> <p>Review Status: \u2705 Complete Documentation Package Status: \u2705 Production-Ready (with recommended future improvements) Last Updated: November 10, 2025</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/","title":"External Dependencies and Requirements","text":"<p>This document details all the external systems, services, and requirements needed to successfully deploy and operate the Sprocket infrastructure. These are critical dependencies that must be in place before deployment.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#critical-external-dependencies","title":"Critical External Dependencies","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#1-doppler-secrets-management","title":"1. Doppler (Secrets Management)","text":"<p>Priority: \ud83d\udd34 CRITICAL - Cannot deploy without Doppler access</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-is","title":"What It Is","text":"<p>Doppler is a secrets management platform that serves as the source of truth for all application secrets in the Sprocket infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#why-its-required","title":"Why It's Required","text":"<ul> <li>OAuth credentials (Google, Discord, Epic Games, Steam)</li> <li>API tokens (Ballchasing API, etc.)</li> <li>SMTP credentials for email</li> <li>Application-specific secrets</li> <li>Database passwords (non-infrastructure)</li> <li>Service integration keys</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#whats-stored-in-doppler","title":"What's Stored in Doppler","text":"<p>OAuth Providers:</p> <pre><code>GOOGLE_CLIENT_ID\nGOOGLE_CLIENT_SECRET\nDISCORD_CLIENT_ID\nDISCORD_CLIENT_SECRET\nEPIC_GAMES_CLIENT_ID\nEPIC_GAMES_CLIENT_SECRET\nSTEAM_API_KEY\n</code></pre> <p>API Tokens:</p> <pre><code>BALLCHASING_API_TOKEN\nCHATWOOT_HMAC_KEY\n</code></pre> <p>Email Configuration:</p> <pre><code>SMTP_HOST\nSMTP_PORT\nSMTP_USERNAME\nSMTP_PASSWORD\n</code></pre> <p>Application Secrets:</p> <pre><code>SESSION_SECRET\nJWT_SECRET\nENCRYPTION_KEY\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#how-its-used","title":"How It's Used","text":"<ol> <li>During Bootstrap: <code>scripts/bootstrap-vault-secrets.sh</code> reads from Doppler</li> <li>Secret Provisioning: Loads secrets into Vault at specific paths</li> <li>Runtime: Services retrieve secrets from Vault (which came from Doppler)</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-requirements","title":"Access Requirements","text":"<ul> <li>Doppler project membership</li> <li>Read access to all required secrets</li> <li>Doppler CLI installed and authenticated</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#setup-instructions","title":"Setup Instructions","text":"<pre><code># Install Doppler CLI\ncurl -Ls https://cli.doppler.com/install.sh | sh\n\n# Login to Doppler\ndoppler login\n\n# Select project\ndoppler setup\n\n# Verify access\ndoppler secrets\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-doppler","title":"What Happens Without Doppler","text":"<p>\u274c Deployment will fail at secret provisioning step - OAuth authentication won't work - API integrations will fail - Services can't authenticate to external systems - No email notifications</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#2-github-organization","title":"2. GitHub Organization","text":"<p>Priority: \ud83d\udd34 CRITICAL - Required for Vault authentication and authorization</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-is_1","title":"What It Is","text":"<p>The GitHub Organization (<code>SprocketBot</code>) manages team-based access control for Vault.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#why-its-required_1","title":"Why It's Required","text":"<ul> <li>Vault Authentication: Uses GitHub OAuth for user authentication</li> <li>Vault Authorization: GitHub teams map to Vault policies</li> <li>Access Control: Team membership determines who can access which secrets</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#github-teams-vault-policies-mapping","title":"GitHub Teams \u2192 Vault Policies Mapping","text":"GitHub Team Vault Policy Access Level <code>github-admin</code> Admin access Full access to all secrets, can manage policies <code>github-readonly</code> Read-only access Can read but not modify secrets <code>developers</code> Developer access Can read app secrets, not infrastructure secrets <code>ops</code> Operations access Can manage infrastructure secrets"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#vault-policy-examples","title":"Vault Policy Examples","text":"<p>github-admin.hcl:</p> <pre><code># Full access to all secret paths\npath \"secret/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\n\n# Can manage auth methods\npath \"auth/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\n</code></pre> <p>github-readonly.hcl:</p> <pre><code># Read-only access to secrets\npath \"secret/*\" {\n  capabilities = [\"read\", \"list\"]\n}\n\n# Cannot write or delete\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#github-oauth-configuration","title":"GitHub OAuth Configuration","text":"<p>Vault is configured to use GitHub OAuth:</p> <pre><code>auth \"github\" {\n  organization = \"SprocketBot\"\n\n  team_mappings = {\n    \"github-admin\" = \"admins\"\n    \"github-readonly\" = \"readonly\"\n  }\n}\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-requirements_1","title":"Access Requirements","text":"<ul> <li>Membership in SprocketBot GitHub organization</li> <li>Assignment to appropriate GitHub teams</li> <li>GitHub OAuth app configured for Vault</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#setup-instructions_1","title":"Setup Instructions","text":"<ol> <li>Join GitHub Organization: Must be invited by org admin</li> <li>Join Required Teams: Request team membership</li> <li>GitHub OAuth: Authenticate to Vault via GitHub    <code>bash    vault login -method=github</code></li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-github-org-access","title":"What Happens Without GitHub Org Access","text":"<p>\u274c Cannot authenticate to Vault - No access to secrets - Cannot provision new secrets - Cannot manage infrastructure - Team-based access control doesn't work</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#3-digital-ocean-account","title":"3. Digital Ocean Account","text":"<p>Priority: \ud83d\udd34 CRITICAL - Required for managed services and infrastructure</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-provides","title":"What It Provides","text":"<ol> <li>Managed PostgreSQL Database</li> <li>Spaces (S3-compatible object storage)</li> <li>Compute instances (Droplets) (if used)</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#31-digital-ocean-managed-postgresql","title":"3.1 Digital Ocean Managed PostgreSQL","text":"<p>Why It's Used: - Production-grade database with automated backups - Point-in-time recovery - High availability option - Automated maintenance and updates - Better performance than self-hosted</p> <p>Connection Details:</p> <pre><code>Host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\nPort: 25060\nDatabase: defaultdb\nConnection Pool: Yes\nSSL Mode: Required\n</code></pre> <p>What's Stored: - All application data (users, games, teams, matches, etc.) - Chatwoot data - N8n workflows - Grafana dashboards - Application state</p> <p>Backup Strategy: - Daily automated backups (7-day retention) - Point-in-time recovery (PITR) available - Can restore to any point in last 7 days</p> <p>Access Requirements: - Database credentials (username/password) - SSL certificate (automatic) - Firewall rules allowing production node IP</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#32-digital-ocean-spaces-object-storage","title":"3.2 Digital Ocean Spaces (Object Storage)","text":"<p>Primary Use: Vault backend storage</p> <p>Bucket: <code>vault-secrets</code> Endpoint: <code>https://nyc3.digitaloceanspaces.com</code> Region: NYC3</p> <p>Why It's Used: - Reliable S3-compatible storage - Vault persistence across container restarts - Automatic replication - Cheaper than managing local storage</p> <p>Secondary Use: Application file storage - User uploads - Generated images - Replay files - Backups</p> <p>Access Requirements: - Spaces access key - Spaces secret key - Bucket read/write permissions</p> <p>Configuration:</p> <pre><code># In Pulumi config\nvault-s3-bucket: vault-secrets\nvault-s3-endpoint: https://nyc3.digitaloceanspaces.com\nvault-s3-access-key: DO801Q8P44QRYP9HEQVL\nvault-s3-secret-key: [secure]\n\ns3-endpoint: nyc3.digitaloceanspaces.com\ns3-access-key: DO004ZHPV38R8C9Q46XG\ns3-secret-key: [secure]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-digital-ocean","title":"What Happens Without Digital Ocean","text":"<p>\u274c Cannot deploy infrastructure - No database (all apps fail) - Vault state not persistent - File uploads don't work - Backup/restore not possible</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#4-dns-provider","title":"4. DNS Provider","text":"<p>Priority: \ud83d\udfe1 HIGH - Required for production, optional for local dev</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-provides_1","title":"What It Provides","text":"<p>DNS records for the Sprocket platform domains.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#required-dns-records","title":"Required DNS Records","text":"<p>For Production (sprocket.mlesports.gg):</p> <pre><code>A     sprocket.mlesports.gg           \u2192 &lt;production-node-ip&gt;\nA     main.sprocket.mlesports.gg      \u2192 &lt;production-node-ip&gt;\nA     api.sprocket.mlesports.gg       \u2192 &lt;production-node-ip&gt;\nA     vault.sprocket.mlesports.gg     \u2192 &lt;production-node-ip&gt;\nA     traefik.sprocket.mlesports.gg   \u2192 &lt;production-node-ip&gt;\n</code></pre> <p>Or using wildcard:</p> <pre><code>A     *.sprocket.mlesports.gg         \u2192 &lt;production-node-ip&gt;\nA     sprocket.mlesports.gg           \u2192 &lt;production-node-ip&gt;\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#why-its-required_2","title":"Why It's Required","text":"<ul> <li>Let's Encrypt requires public DNS for certificate issuance</li> <li>HTTP-01 challenge verification</li> <li>Users access via domain names</li> <li>Service discovery and routing</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-requirements_2","title":"Access Requirements","text":"<ul> <li>DNS provider account (Cloudflare, Route53, etc.)</li> <li>Ability to create/modify A records</li> <li>DNS propagation time (~5-60 minutes)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#verification","title":"Verification","text":"<pre><code># Check DNS resolution\nnslookup sprocket.mlesports.gg\ndig sprocket.mlesports.gg\n\n# Should return production node IP\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-dns","title":"What Happens Without DNS","text":"<p>\u274c Production deployment not possible - Let's Encrypt certificates won't issue - Users can't access services - Traefik routing won't work (hostname-based)</p> <p>Workaround for Local Dev: - Use <code>/etc/hosts</code> file modifications - Use <code>.localhost</code> domains - Accept self-signed certificates</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#5-pulumi-backend","title":"5. Pulumi Backend","text":"<p>Priority: \ud83d\udd34 CRITICAL - Required for infrastructure state management</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-provides_2","title":"What It Provides","text":"<p>Stores Pulumi state for all infrastructure stacks.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#backend-options","title":"Backend Options","text":"<p>Option 1: Pulumi Cloud (Current):</p> <pre><code>pulumi login\n</code></pre> <ul> <li>State stored in Pulumi's cloud</li> <li>Team collaboration</li> <li>State history</li> <li>Web UI for viewing resources</li> </ul> <p>Option 2: S3 Backend:</p> <pre><code>pulumi login \"s3://[bucket]/pulumi?endpoint=[endpoint]\"\n</code></pre> <ul> <li>Self-hosted state</li> <li>S3-compatible storage</li> <li>More control</li> <li>Requires S3 credentials</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#required-configuration","title":"Required Configuration","text":"<pre><code># AWS credentials for S3 backend\n~/.aws/credentials:\n[default]\naws_access_key_id = [your key]\naws_secret_access_key = [your secret]\nregion = us-east-1\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#whats-stored","title":"What's Stored","text":"<ul> <li>Infrastructure state for all three layers</li> <li>Resource dependencies</li> <li>Configuration values (encrypted)</li> <li>Output values</li> <li>Deployment history</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-requirements_3","title":"Access Requirements","text":"<ul> <li>Pulumi account (for Pulumi Cloud)</li> <li>OR S3 credentials (for S3 backend)</li> <li>Organization/project membership</li> <li>Write access to state storage</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#stack-names","title":"Stack Names","text":"<ul> <li><code>layer_1</code></li> <li><code>layer_2</code></li> <li><code>prod</code> (platform stack)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-pulumi-backend","title":"What Happens Without Pulumi Backend","text":"<p>\u274c Cannot manage infrastructure - No deployment possible - Can't track resource state - Risk of resource conflicts - No rollback capability</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#6-docker-hub-account","title":"6. Docker Hub Account","text":"<p>Priority: \ud83d\udfe1 HIGH - Required for private images</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-it-provides_3","title":"What It Provides","text":"<p>Access to private Docker images for Sprocket services.</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#private-images","title":"Private Images","text":"<ul> <li><code>asaxplayinghorse/sprocket-web</code></li> <li><code>asaxplayinghorse/sprocket-api</code></li> <li><code>asaxplayinghorse/sprocket-image-generation</code></li> <li><code>asaxplayinghorse/sprocket-discord-bot</code></li> <li>Other microservices</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#why-its-required_3","title":"Why It's Required","text":"<ul> <li>Services are not public</li> <li>Pull from private repositories</li> <li>Specific image tags (e.g., <code>main</code>, <code>staging</code>)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#configuration","title":"Configuration","text":"<pre><code># In Pulumi config\ndocker-username: asaxplayinghorse\ndocker-access-token: [secure]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#used-in","title":"Used In","text":"<ul> <li>Docker service configurations</li> <li>Image pull secrets</li> <li>Registry authentication</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#what-happens-without-docker-hub-access","title":"What Happens Without Docker Hub Access","text":"<p>\u274c Cannot pull application images - Services won't start - Image pull failures - Deployment blocked</p> <p>Workaround: - Use public images (if available) - Build images locally - Use different registry</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#development-tools-and-cli-requirements","title":"Development Tools and CLI Requirements","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#required-on-deployment-machine","title":"Required on Deployment Machine","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#1-nodejs","title":"1. Node.js","text":"<p>Version: 16.x (required for some Pulumi operations)</p> <pre><code># Check version\nnode --version\n\n# Install with nvm\nnvm install 16\nnvm use 16\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#2-pulumi-cli","title":"2. Pulumi CLI","text":"<pre><code># Install\ncurl -fsSL https://get.pulumi.com | sh\n\n# Verify\npulumi version\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#3-docker","title":"3. Docker","text":"<pre><code># Verify\ndocker --version\ndocker info | grep Swarm\n# Should show: Swarm: active\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#4-vault-cli-for-secret-management","title":"4. Vault CLI (for secret management)","text":"<pre><code># Install\nwget https://releases.hashicorp.com/vault/1.15.0/vault_1.15.0_linux_amd64.zip\nunzip vault_1.15.0_linux_amd64.zip\nsudo mv vault /usr/local/bin/\n\n# Verify\nvault version\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#5-git","title":"5. Git","text":"<pre><code># Verify\ngit --version\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#network-requirements","title":"Network Requirements","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#firewall-rules","title":"Firewall Rules","text":"<p>Inbound (Production node):</p> <pre><code>Port 80   (HTTP)  - Let's Encrypt challenges\nPort 443  (HTTPS) - Application traffic\nPort 22   (SSH)   - Management access\n</code></pre> <p>Outbound:</p> <pre><code>Port 443  (HTTPS) - GitHub, Doppler, Docker Hub, Digital Ocean\nPort 80   (HTTP)  - Package managers, Let's Encrypt\nPort 25060 (PostgreSQL) - Managed database connection\nPort 5432 (PostgreSQL) - Managed database connection (alt)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#dns-resolution","title":"DNS Resolution","text":"<pre><code>Access to public DNS servers\nAbility to resolve:\n  - github.com\n  - doppler.com\n  - hub.docker.com\n  - digitalocean.com\n  - nyc3.digitaloceanspaces.com\n  - sprocketbot-postgres-*.db.ondigitalocean.com\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-checklist","title":"Access Checklist","text":"<p>Before attempting deployment, verify you have:</p> <p>Secrets and Configuration: - [ ] Doppler account and project access - [ ] GitHub organization membership - [ ] GitHub team assignments (admin or appropriate level) - [ ] Digital Ocean account access - [ ] Digital Ocean managed PostgreSQL credentials - [ ] Digital Ocean Spaces credentials - [ ] Docker Hub credentials (username + access token) - [ ] Pulumi backend access (Pulumi Cloud or S3)</p> <p>DNS and Network: - [ ] DNS provider access - [ ] A records configured (for production) - [ ] Firewall rules configured - [ ] Production node accessible via SSH</p> <p>Local Tools: - [ ] Node.js 16.x installed - [ ] Pulumi CLI installed and authenticated - [ ] Docker installed (with Swarm initialized) - [ ] Vault CLI installed - [ ] Git configured</p> <p>Documentation Access: - [ ] This repository cloned - [ ] Vault unseal keys location known - [ ] Root token location known (if needed)</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#dependency-graph","title":"Dependency Graph","text":"<pre><code>                    Doppler\n                      \u2502\n                      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u25bc      \u25bc\n                   Vault  Services\n                      \u2502      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2502      \u2502            \u2502\n    \u25bc                 \u25bc      \u25bc            \u25bc\nGitHub Org    Digital Ocean    Docker Hub    DNS Provider\n    \u2502               \u2502                            \u2502\n    \u251c\u2500\u2500\u2500 Auth \u2500\u2500\u2500\u2500\u2500\u2500\u2524                            \u2502\n    \u2502               \u251c\u2500\u2500\u2500 Database                \u2502\n    \u2502               \u251c\u2500\u2500\u2500 Spaces (S3)             \u2502\n    \u2502               \u2514\u2500\u2500\u2500 Droplets                \u2502\n    \u2502                                            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pulumi Backend \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                    Infrastructure\n                      Deployment\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#cost-considerations","title":"Cost Considerations","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#monthly-costs-approximate","title":"Monthly Costs (Approximate)","text":"<p>Digital Ocean: - Managed PostgreSQL (Basic): $15-60/month (depending on size) - Spaces: $5/month + $0.02/GB transfer - Droplet (if used): $6-40/month</p> <p>Pulumi Cloud: - Free tier: Up to 500 resources - Team: $75/month (if needed)</p> <p>Doppler: - Free tier: Up to 5 users - Team: $8/user/month (if needed)</p> <p>Domain Registration: - ~$10-15/year</p> <p>Docker Hub: - Free tier: 1 user, unlimited public repos - Pro: $5/month for private repos</p> <p>Total Estimated: $30-150/month depending on scale</p>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#security-considerations","title":"Security Considerations","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#secret-storage","title":"Secret Storage","text":"<ul> <li>\u2705 Doppler: Encrypted at rest and in transit</li> <li>\u2705 Vault: Encrypted storage on S3</li> <li>\u2705 Pulumi: Secrets encrypted in state files</li> <li>\u26a0\ufe0f Never commit secrets to git</li> <li>\u26a0\ufe0f Rotate credentials regularly</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#access-control","title":"Access Control","text":"<ul> <li>Use least-privilege principle</li> <li>GitHub team-based access</li> <li>Vault policies per team</li> <li>Regular access reviews</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#backup-strategy","title":"Backup Strategy","text":"<ul> <li>Database: Daily automated backups</li> <li>Vault state: S3 replication</li> <li>Pulumi state: Version history</li> <li>Git: Repository backups</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#if-doppler-is-unavailable","title":"If Doppler Is Unavailable","text":"<ol> <li>Have backup export of secrets in secure location</li> <li>Can manually provision to Vault</li> <li>Deploy with partial functionality</li> <li>Restore from Doppler when available</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#if-github-org-is-unavailable","title":"If GitHub Org Is Unavailable","text":"<ol> <li>Use Vault root token for emergency access</li> <li>Temporarily disable GitHub auth</li> <li>Use alternative auth method (userpass)</li> <li>Restore GitHub auth when available</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#if-digital-ocean-database-is-down","title":"If Digital Ocean Database Is Down","text":"<ol> <li>Contact Digital Ocean support</li> <li>Restore from point-in-time backup</li> <li>Consider failover to replica (if configured)</li> <li>Application is fully down until DB restored</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/EXTERNALITIES_AND_DEPENDENCIES/#if-pulumi-state-is-lost","title":"If Pulumi State Is Lost","text":"<p>\u26a0\ufe0f CRITICAL SITUATION 1. Check Pulumi Cloud history 2. Restore from S3 backup (if S3 backend) 3. May need to import existing resources 4. Potentially recreate infrastructure from scratch</p> <p>Prevention: - Regular state exports: <code>pulumi stack export &gt; backup.json</code> - Store backups in multiple locations</p> <p>This document represents the full dependency tree for the Sprocket infrastructure. All of these externalities must be in place and accessible for successful deployment.</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/","title":"Git History Analysis - Deployment Journey","text":"<p>Complete analysis of the git commit history showing the evolution of the Sprocket infrastructure from inception to production deployment.</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#commit-timeline","title":"Commit Timeline","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#foundation-period-sept-14-19-2025","title":"Foundation Period: Sept 14-19, 2025","text":"<p>Goal: Establish basic infrastructure and get Layer 1 working</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#8e2c54d-sept-16-2025","title":"<code>8e2c54d</code> - Sept 16, 2025","text":"<p>\"feat: layer_2 config started\" - Initial Layer 2 configuration - Setting up data services</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#3126c6d-sept-14-2025","title":"<code>3126c6d</code> - Sept 14, 2025","text":"<p>\"feat: layer_1 now runs again!\" - Layer 1 infrastructure working - Core services deploying</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#vault-struggles-period-sept-17-23-2025","title":"Vault Struggles Period: Sept 17-23, 2025","text":"<p>Goal: Get Vault initialization and unsealing working reliably</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#5de5d04-sept-17-2025","title":"<code>5de5d04</code> - Sept 17, 2025","text":"<p>\"feat: bootstrapping problems with minio and other vault secrets\" - Encountering issues with Vault + MinIO integration - Secret bootstrapping not working smoothly</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#cde2961-sept-18-2025","title":"<code>cde2961</code> - Sept 18, 2025","text":"<p>\"feat: Accessing vault now works on localhost\" - Progress on Vault access - Can connect to Vault locally - Still not fully automated</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#5057afc-sept-19-2025","title":"<code>5057afc</code> - Sept 19, 2025","text":"<p>\"feat: vault actually unseals!\" - BREAKTHROUGH: Automatic unsealing working - Major milestone in Vault automation</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#346c2a9-sept-19-2025","title":"<code>346c2a9</code> - Sept 19, 2025","text":"<p>\"docs: update readme with learnings\" - Documented what was learned about Vault - Captured knowledge for future reference</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#fff0c3c-sept-19-2025","title":"<code>fff0c3c</code> - Sept 19, 2025","text":"<p>\"feat: still polishing layer_1 to be repeatable\" - Working on making Layer 1 deployment repeatable - Not just working once, but reliably</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#07ee345-sept-21-2025","title":"<code>07ee345</code> - Sept 21, 2025","text":"<p>\"feat: relative paths in vault script\" - Improving Vault automation scripts - Making them more portable</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#2103358-sept-21-2025","title":"<code>2103358</code> - Sept 21, 2025","text":"<p>\"feat: all tokens working now. Vault should be good!\" - MAJOR MILESTONE: Vault fully functional - All token generation working - Vault considered stable</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#ac9ac81-sept-21-2025","title":"<code>ac9ac81</code> - Sept 21, 2025","text":"<p>\"fix: changed org in refs, and keep the vault container up and running\" - Container stability improvements - Organization references corrected</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#debugging-and-integration-sept-22-23-2025","title":"Debugging and Integration: Sept 22-23, 2025","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#38e8d13-sept-22-2025","title":"<code>38e8d13</code> - Sept 22, 2025","text":"<p>\"feat: Making progress, vault now has secrets and provides them appropriately\" - Vault not just working, but serving secrets to services - Integration with rest of stack</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#1dc12a9-sept-23-2025","title":"<code>1dc12a9</code> - Sept 23, 2025","text":"<p>\"feat: some debugging progress\" - Continued debugging and refinement</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#storage-migration-period-sept-30-oct-8-2025","title":"Storage Migration Period: Sept 30 - Oct 8, 2025","text":"<p>Goal: Move from self-hosted MinIO to cloud S3 storage</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#3f32a74-sept-30-2025","title":"<code>3f32a74</code> - Sept 30, 2025","text":"<p>\"feat(layer_2): migrate to cloud S3 and fix service configurations\"</p> <p>Major changes: - Replaced local MinIO service with cloud S3-compatible storage - Updated SprocketMinioProvider to support direct credentials and SSL configuration - Fixed PostgreSQL provider to set superuser=false for non-superuser connections</p> <p>Service fixes: - N8n: Added N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS env var - N8n: Fixed PostgreSQL port to use config instead of hardcoded 5432 - Gatus: Removed ingress node placement constraint to allow scheduling</p> <p>Configuration: - Added s3-endpoint, s3-access-key, and s3-secret-key config support - Removed postgres duplicate parameter from Monitoring component - Updated layer_2 exports to use S3 endpoint instead of local MinIO</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#de363dd-oct-6-2025","title":"<code>de363dd</code> - Oct 6, 2025","text":"<p>\"org: move deprecated pulumi files\" - Cleanup and organization - Moving old files out of the way</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#3b45f27-oct-8-2025","title":"<code>3b45f27</code> - Oct 8, 2025","text":"<p>\"feat: Move from MinIO to AWS. Remove postgres network.\" - COMPLETED MIGRATION: MinIO fully removed - Using AWS S3 / cloud storage exclusively - Removed PostgreSQL-specific networking</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#platform-stabilization-oct-26-2025","title":"Platform Stabilization: Oct 26, 2025","text":"<p>Goal: Get application platform working</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#6395d01-oct-26-2025","title":"<code>6395d01</code> - Oct 26, 2025","text":"<p>\"feat: Sprocket is alive!\" - MAJOR MILESTONE: Platform is running! - Application services working - First time everything comes together</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#routing-and-access-period-oct-26-27-2025","title":"Routing and Access Period: Oct 26-27, 2025","text":"<p>Goal: Support multiple access patterns (localhost, LAN, cloud)</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#7486e02-oct-27-2025","title":"<code>7486e02</code> - Oct 27, 2025","text":"<p>\"feat: add localhost-based routing and cloud deployment documentation\"</p> <p>This commit includes infrastructure changes and documentation for both local development and cloud production deployments.</p> <p>Configuration Updates: - Set layer_1 hostname to 'localhost' for local development - Set platform hostname to 'localhost' for local development - Added server-ip config support for LAN access (192.168.4.39) - Added tailscale-ip config support for remote access (100.110.185.84)</p> <p>Platform Routing Enhancements: - Modified Platform.ts to support IP-based routing alongside hostname routing - Web service now accepts requests via: localhost domains, LAN IP, and Tailscale IP - Routing rules: <code>Host(\\</code>sprocket.localhost`) || Host(`192.168.4.39`) || Host(`100.110.185.84`)` - Resolved duplicate router issues that were causing 404 errors</p> <p>Documentation: - Added CLOUD_DEPLOYMENT.md with comprehensive cloud deployment guide - Includes DNS configuration, firewall setup, deployment order - Troubleshooting section for common issues - Documents differences between local and cloud deployments</p> <p>Bug Fixes: - Fixed Traefik routing issues caused by duplicate service instances - Resolved 404 errors by cleaning up orphaned Docker service tasks - Fixed hostname configuration mismatch between layer_1 and platform</p> <p>Deployment Verification: - Successfully tested HTTPS access via localhost domains - Verified HTTPS access via LAN IP (192.168.4.39) - Verified HTTPS access via Tailscale IP (100.110.185.84) - All services routing correctly with proper TLS</p> <p>Local Development Access: Services accessible at: - https://sprocket.localhost (main web UI) - https://api.sprocket.localhost (API) - https://image-generation.sprocket.localhost (image generation) - https://192.168.4.39 (LAN access) - https://100.110.185.84 (Tailscale access)</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#final-push-to-production-nov-3-8-2025","title":"Final Push to Production: Nov 3-8, 2025","text":"<p>Goal: Deploy to production with real domain and complete the project</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#acf4e4e-nov-4-2025","title":"<code>acf4e4e</code> - Nov 4, 2025","text":"<p>\"feat: so close!\" - Near completion - Final debugging</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#f0f2ef5-nov-7-2025","title":"<code>f0f2ef5</code> - Nov 7, 2025","text":"<p>\"Final stretch.\" - Last mile work - Production deployment imminent</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#e0d8785-nov-7-2025","title":"<code>e0d8785</code> - Nov 7, 2025","text":"<p>\"Platform is up and running\" - Platform deployed to production - Services running</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#a2862ea-nov-8-2025","title":"<code>a2862ea</code> - Nov 8, 2025  \u2728","text":"<p>\"feat: Sprocket v1 is finally up and running completely.\" - \ud83c\udf89 PROJECT COMPLETE: Full production deployment - All services running - Users being served - Infrastructure stable</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#historical-commits-before-sept-2025","title":"Historical Commits (Before Sept 2025)","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#early-2025-maintenance-updates","title":"Early 2025: Maintenance Updates","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#9429e1c-jan-13-2025","title":"<code>9429e1c</code> - Jan 13, 2025","text":"<p>\"chore: update to match with newest dockerhub api\" - Keeping dependencies current</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#7b0e5cb-jan-13-2025","title":"<code>7b0e5cb</code> - Jan 13, 2025","text":"<p>\"chore: add flake for consistent dev environments\" - Added Nix flake for reproducible environments</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#2024-feature-development","title":"2024: Feature Development","text":"<p>Multiple commits throughout 2024 for: - Playground configuration - Core service updates - Discord token updates - Vault policy adjustments - Service configurations - Monitoring additions (Gatus) - Workflow automation (N8n) - Various bug fixes and features</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#2022-initial-infrastructure-development","title":"2022: Initial Infrastructure Development","text":"<p>The infrastructure was originally built in 2022 with commits showing: - Initial Pulumi setup (<code>640a75f</code> - Apr 5, 2022) - Platform deployments - Service additions (Chatwoot, InfluxDB, Neo4j, etc.) - Vault policies - Authentication setups - Configuration management</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#deployment-phases-summary","title":"Deployment Phases Summary","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#phase-1-foundation-2022","title":"Phase 1: Foundation (2022)","text":"<ul> <li>Initial Pulumi infrastructure</li> <li>Core services defined</li> <li>Basic deployment working</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#phase-2-maintenance-2022-2024","title":"Phase 2: Maintenance (2022-2024)","text":"<ul> <li>Feature additions</li> <li>Service updates</li> <li>Configuration refinements</li> <li>Bug fixes</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#phase-3-major-rebuild-sept-nov-2025","title":"Phase 3: Major Rebuild (Sept-Nov 2025)","text":"<p>The recent commits show a complete infrastructure rebuild:</p> <ol> <li>Week 1 (Sept 14-19): Vault automation breakthrough</li> <li>Week 2 (Sept 22-23): Service integration</li> <li>Weeks 3-4 (Sept 30 - Oct 8): Storage migration (MinIO \u2192 S3)</li> <li>Week 5-6 (Oct 26-27): Platform alive, routing solved</li> <li>Week 7-8 (Nov 3-8): Production deployment completed</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#key-patterns-in-commit-history","title":"Key Patterns in Commit History","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#1-iterative-problem-solving","title":"1. Iterative Problem Solving","text":"<p>Multiple commits on same issue show persistence: - Vault unsealing: 5+ commits over 4 days - Routing issues: Multiple attempts to get right - Storage migration: Phased approach over 2 weeks</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#2-documentation-commits","title":"2. Documentation Commits","text":"<p>Several commits dedicated to documentation: - <code>346c2a9</code>: README updates with learnings - <code>7486e02</code>: Comprehensive deployment documentation - Shows learning was captured along the way</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#3-feature-toggles","title":"3. Feature Toggles","text":"<p>Commits show features being added, tested, sometimes reverted: - Statping-ng added then reverted (multiple commits Dec 2022) - Shows willingness to try and abandon what doesn't work</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#4-configuration-refinement","title":"4. Configuration Refinement","text":"<p>Many \"chore: config\" commits show: - Continuous tuning - Environment-specific adjustments - Learning what works in production</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#5-breaking-changes","title":"5. Breaking Changes","text":"<p>Some commits forced by external changes: - DockerHub API updates - Service version upgrades - Platform migrations</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#commit-message-quality-analysis","title":"Commit Message Quality Analysis","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#excellent-commits","title":"Excellent Commits","text":"<ul> <li><code>7486e02</code>: Full description of changes, why they were made, and how to use</li> <li><code>3f32a74</code>: Detailed breakdown of service fixes and configuration changes</li> <li>These make understanding the project history possible</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#standard-commits","title":"Standard Commits","text":"<ul> <li>Clear feat/fix/chore prefixes</li> <li>Concise descriptions</li> <li>Easy to scan</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#vague-commits","title":"Vague Commits","text":"<ul> <li><code>f0f2ef5</code>: \"Final stretch\" - doesn't say what changed</li> <li><code>1dc12a9</code>: \"some debugging progress\" - lacks specifics</li> <li>These make it hard to understand what was done</li> </ul> <p>Lesson: Detailed commit messages are invaluable for postmortems and future maintenance.</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#what-the-git-history-tells-us","title":"What the Git History Tells Us","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#1-the-problem-was-hard","title":"1. The Problem Was Hard","text":"<ul> <li>Months of work from initial setup to production</li> <li>Multiple false starts and revisions</li> <li>Complex interdependencies between services</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#2-persistence-pays-off","title":"2. Persistence Pays Off","text":"<ul> <li>Many attempts at Vault automation before success</li> <li>Routing issues solved through iteration</li> <li>Migration to managed services was worth the effort</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#3-external-dependencies-matter","title":"3. External Dependencies Matter","text":"<ul> <li>Storage backend changes (MinIO \u2192 S3)</li> <li>Database changes (self-hosted \u2192 managed)</li> <li>These decisions simplified the architecture</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#4-documentation-is-crucial","title":"4. Documentation Is Crucial","text":"<ul> <li>Commits with good messages help future understanding</li> <li>Documentation commits show learning was captured</li> <li>Missing docs would make this postmortem much harder</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#5-evolution-over-revolution","title":"5. Evolution Over Revolution","text":"<ul> <li>Gradual migration of storage</li> <li>Phased deployment approach</li> <li>Small changes, frequent commits</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#recommended-git-practices-based-on-this-history","title":"Recommended Git Practices (Based on This History)","text":""},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#do","title":"DO:","text":"<p>\u2705 Write detailed commit messages for complex changes \u2705 Commit frequently with small, focused changes \u2705 Document learnings in dedicated commits \u2705 Use conventional commit prefixes (feat/fix/docs/chore) \u2705 Include \"why\" in commit messages, not just \"what\"</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#dont","title":"DON'T:","text":"<p>\u274c Use vague messages like \"updates\" or \"changes\" \u274c Commit too many unrelated changes together \u274c Skip documentation of important decisions \u274c Forget to explain context in commit message</p>"},{"location":"departments/development/systems/production-infrastructure/GIT_HISTORY_ANALYSIS/#timeline-visualization","title":"Timeline Visualization","text":"<pre><code>Sept 14 \u2500\u252c\u2500 Layer 1 running\n          \u2502\nSept 17 \u2500\u253c\u2500 Vault struggles begin\n          \u2502\nSept 19 \u2500\u253c\u2500 \u2728 Vault unseals! (breakthrough)\n          \u2502\nSept 21 \u2500\u253c\u2500 \u2728 Vault fully working\n          \u2502\nSept 30 \u2500\u253c\u2500 Storage migration begins\n          \u2502\nOct 8 \u2500\u2500\u253c\u2500 \u2728 MinIO removed, cloud storage only\n          \u2502\nOct 26 \u2500\u253c\u2500 \u2728 Platform alive!\n          \u2502\nOct 27 \u2500\u253c\u2500 Routing and access solved\n          \u2502\nNov 4 \u2500\u2500\u253c\u2500 Almost there\n          \u2502\nNov 7 \u2500\u2500\u253c\u2500 Platform running\n          \u2502\nNov 8 \u2500\u2500\u2534\u2500 \ud83c\udf89 PRODUCTION COMPLETE!\n</code></pre> <p>Total duration of rebuild: ~8 weeks Total commits: 20+ in the rebuild phase Key milestones: 5 major breakthroughs Final status: Production-ready infrastructure serving users</p> <p>This git history represents a journey from a broken/outdated infrastructure to a fully functional, production-ready platform. The commit messages tell a story of persistence, learning, and iterative improvement.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/","title":"Sprocket Infrastructure Glossary","text":"<p>Version: 1.0 Last Updated: November 8, 2025 Audience: Everyone - from non-technical managers to junior engineers</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#how-to-use-this-glossary","title":"How to Use This Glossary","text":"<p>This glossary defines all technical terms used throughout the Sprocket infrastructure documentation. Terms are organized alphabetically within categories for easy reference.</p> <p>For Non-Technical Readers: Start with \"Core Concepts\" to understand the basics. For Junior Engineers: Read through \"Infrastructure Concepts\" and \"Deployment Tools\" sections. For Everyone: Use Ctrl+F / Cmd+F to search for specific terms.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#core-concepts","title":"Core Concepts","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#infrastructure","title":"Infrastructure","text":"<p>The collection of hardware (computers, servers) and software (databases, web servers) needed to run an application. Think of it like the foundation, plumbing, and electrical systems of a building - users don't see it, but nothing works without it.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#cloud-computing","title":"Cloud Computing","text":"<p>Renting computer resources (servers, storage, databases) from companies like Digital Ocean or AWS instead of buying and maintaining your own physical servers. Like renting an apartment instead of buying a house.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#production","title":"Production","text":"<p>The \"real\" environment where actual users interact with the application. As opposed to \"development\" (where engineers test) or \"staging\" (where you test before going live). When something is \"in production,\" it means real users depend on it.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#deployment","title":"Deployment","text":"<p>The process of taking application code and making it run on servers so users can access it. Like publishing a book - writing it is development, but deployment is printing and distributing it to readers.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#high-availability-ha","title":"High Availability (HA)","text":"<p>A system designed to keep running even if individual parts fail. Like having backup generators in a hospital - if main power fails, backups take over automatically.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#scalability","title":"Scalability","text":"<p>The ability to handle more users or traffic by adding more resources. Two types: - Vertical Scaling: Making a single server bigger (more CPU, more RAM) - like adding more floors to a building - Horizontal Scaling: Adding more servers - like building more buildings</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#infrastructure-concepts","title":"Infrastructure Concepts","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#docker","title":"Docker","text":"<p>Software that packages applications and all their dependencies into \"containers\" - standardized units that run the same way everywhere. Think of it like shipping containers: whether you're shipping toys or electronics, the container format is the same, making transport easier.</p> <p>Real-world analogy: Instead of saying \"this app needs Python 3.9, library X version 2.1, library Y version 3.0...\" you package everything together. The container runs the same on your laptop and on the production server.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#docker-swarm","title":"Docker Swarm","text":"<p>Docker's built-in tool for managing containers across multiple servers. It handles: - Running multiple copies of an application for reliability - Distributing containers across servers - Restarting containers if they crash - Load balancing traffic between copies</p> <p>Why we use it: Provides redundancy and easier management than running containers manually on each server.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#container","title":"Container","text":"<p>A running instance of a Docker image. It's isolated from other containers - has its own filesystem, network, and processes. Like apartments in a building - each has its own space but they share the building's infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#service-docker-service","title":"Service (Docker Service)","text":"<p>In Docker Swarm, a service is a definition of how to run containers: - Which image to use - How many copies (replicas) to run - What ports to expose - What networks to connect to</p> <p>Example: The \"Sprocket Web\" service says \"run 1 copy of the sprocket-web image, expose port 3000, connect to the ingress network.\"</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#replica","title":"Replica","text":"<p>A copy of a container. If a service has 3 replicas, it means 3 identical containers are running, usually for redundancy and load balancing.</p> <p>Why multiple replicas? If one crashes, the others keep serving users. Also spreads traffic across multiple containers for better performance.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#image-docker-image","title":"Image (Docker Image)","text":"<p>A template for creating containers. Contains the application code plus everything needed to run it. Like a blueprint for a house - you can build multiple houses (containers) from one blueprint (image).</p> <p>Example: <code>asaxplayinghorse/sprocket-web:main</code> is an image stored in Docker Hub that contains the Sprocket web application.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#volume-docker-volume","title":"Volume (Docker Volume)","text":"<p>Persistent storage for containers. Containers are temporary - when they stop, their data disappears. Volumes persist data even when containers are destroyed.</p> <p>Real-world analogy: Like an external hard drive that survives even if you replace your computer.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#network-overlay-network","title":"Network (Overlay Network)","text":"<p>In Docker Swarm, overlay networks connect containers even across different physical servers. Containers on the same network can communicate using service names instead of IP addresses.</p> <p>Example: A container can connect to Redis by using the hostname \"redis\" instead of needing to know the IP address.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#bind-mount","title":"Bind Mount","text":"<p>A way to share a directory from the host server directly into a container. Changes made in the container show up on the host and vice versa.</p> <p>When we use it: Vault unseal keys are stored on the host server and bind-mounted into the Vault container so it can read them.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#node","title":"Node","text":"<p>In Docker Swarm, a node is a server that runs containers. Can be: - Manager Node: Orchestrates the swarm, assigns containers to workers - Worker Node: Runs containers as assigned by manager</p> <p>Current setup: We have 1 manager node (our single Droplet) running everything.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#networking-security-concepts","title":"Networking &amp; Security Concepts","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#dns-domain-name-system","title":"DNS (Domain Name System)","text":"<p>The system that translates human-readable domain names (like <code>sprocket.mlesports.gg</code>) into IP addresses (like <code>192.168.4.39</code>) that computers use to find each other.</p> <p>Real-world analogy: Like a phone book - you look up \"Pizza Place\" and get the phone number.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#https-http","title":"HTTPS / HTTP","text":"<ul> <li>HTTP: HyperText Transfer Protocol - how web browsers communicate with web servers</li> <li>HTTPS: HTTP Secure - encrypted version so data can't be intercepted</li> </ul> <p>Why it matters: Without HTTPS, passwords and data travel in plain text. Anyone on the network can read them.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#tls-transport-layer-security","title":"TLS (Transport Layer Security)","text":"<p>The encryption technology that makes HTTPS secure. Formerly called SSL (Secure Sockets Layer) - you'll still hear \"SSL certificate\" but it really means TLS.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#certificate-tlsssl-certificate","title":"Certificate (TLS/SSL Certificate)","text":"<p>A digital file that proves a website is who it claims to be and enables HTTPS encryption. Issued by trusted authorities like Let's Encrypt.</p> <p>Real-world analogy: Like a passport that proves your identity and is issued by a trusted government.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#lets-encrypt","title":"Let's Encrypt","text":"<p>A free, automated certificate authority that issues TLS certificates. We use it to get certificates for <code>sprocket.mlesports.gg</code> so users see the padlock icon in their browser.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#port","title":"Port","text":"<p>A numbered endpoint on a computer where network connections are made. Common ports: - 80: HTTP (web traffic, unencrypted) - 443: HTTPS (web traffic, encrypted) - 22: SSH (remote server access) - 5432/25060: PostgreSQL (database)</p> <p>Real-world analogy: Like apartment numbers in a building - the building has one address (IP), but different apartments (ports) serve different purposes.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#firewall","title":"Firewall","text":"<p>Security system that controls what network traffic is allowed in/out of a server. Like a security guard at a building entrance - only lets in authorized visitors.</p> <p>Our firewall: Allows ports 80 (HTTP), 443 (HTTPS), 22 (SSH for admins only). Blocks everything else.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#reverse-proxy","title":"Reverse Proxy","text":"<p>A server that sits in front of your application servers and forwards requests to them. Provides benefits like: - TLS termination (handles HTTPS encryption/decryption) - Load balancing (distributes traffic across multiple servers) - Routing (sends requests to the right application based on URL)</p> <p>Our reverse proxy: Traefik handles all incoming traffic and routes it to the correct service.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#load-balancer","title":"Load Balancer","text":"<p>Distributes incoming network traffic across multiple servers to prevent any one server from becoming overloaded.</p> <p>Example: If you have 3 web servers, the load balancer sends some users to server 1, some to server 2, some to server 3.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#ingress-controller","title":"Ingress Controller","text":"<p>In container environments, the component that handles incoming traffic from outside the cluster and routes it to the appropriate services. Traefik is our ingress controller.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#oauth","title":"OAuth","text":"<p>An authentication method that lets users log in using their Google, Discord, Steam, or Epic Games accounts instead of creating a new username/password.</p> <p>User experience: \"Sign in with Google\" button.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#jwt-json-web-token","title":"JWT (JSON Web Token)","text":"<p>A secure way to transmit user authentication information between client and server. After logging in, the user gets a JWT that proves they're authenticated for future requests.</p> <p>Real-world analogy: Like a wristband at a concert - you show your ID once to get in, then the wristband proves you're allowed inside.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#storage-data-concepts","title":"Storage &amp; Data Concepts","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#database","title":"Database","text":"<p>Software that stores and organizes data so applications can create, read, update, and delete information efficiently.</p> <p>Our database: PostgreSQL, which stores all Sprocket data (users, matches, teams, etc.)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#postgresql-postgres","title":"PostgreSQL (Postgres)","text":"<p>A powerful, reliable, open-source relational database. Organizes data into tables (like spreadsheets) with rows and columns.</p> <p>Why we chose it: Industry standard, handles complex queries, supports millions of records, excellent reliability.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#managed-database","title":"Managed Database","text":"<p>A database service run by a cloud provider (like Digital Ocean) where they handle: - Automatic backups - Updates and security patches - Hardware maintenance - Scaling and performance tuning</p> <p>vs. Self-Hosted: We used to run our own PostgreSQL in a container. Now Digital Ocean manages it for us.</p> <p>Trade-off: Costs $15/month but saves many hours of maintenance work.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#redis","title":"Redis","text":"<p>An in-memory data store used for caching and fast data access. Extremely fast because it stores data in RAM instead of on disk.</p> <p>Use cases in Sprocket: - Session storage (keep users logged in) - Caching (store frequently-accessed data to avoid database queries) - Pub/sub messaging (real-time updates)</p> <p>Real-world analogy: Like keeping frequently-used files on your desk instead of filing them away - faster access.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#s3-simple-storage-service","title":"S3 (Simple Storage Service)","text":"<p>Amazon's object storage service for files. Digital Ocean Spaces is S3-compatible (same API, different company).</p> <p>Use cases in Sprocket: - Storing replay files - Storing generated images - Storing backups - Storing Vault's encrypted data</p> <p>Why S3?: Industry standard, extremely durable (99.999999999% - \"eleven 9s\"), cheap, scalable.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#object-storage","title":"Object Storage","text":"<p>Storage for files (objects) accessed via HTTP API instead of mounting as a filesystem. Each file has a unique URL.</p> <p>vs. File Storage: Instead of <code>/var/www/images/photo.jpg</code>, you access <code>https://s3.amazonaws.com/bucket/photo.jpg</code></p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#bucket","title":"Bucket","text":"<p>In object storage, a container for files. Like a folder, but at the top level.</p> <p>Our buckets: - <code>vault-secrets</code>: Stores Vault's encrypted backend data - <code>sprocket-storage</code>: Stores application files (images, replays)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#backup","title":"Backup","text":"<p>A copy of data stored separately so you can restore it if the original is lost or corrupted.</p> <p>Our backups: - Database: Automatic daily backups by Digital Ocean (7-day retention) - Vault: Manual backups of unseal keys and S3 data - Pulumi state: Manual exports</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#retention","title":"Retention","text":"<p>How long backups are kept before being deleted.</p> <p>Example: 7-day retention means you can restore from backups up to 7 days old, but older backups are deleted.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#application-architecture-concepts","title":"Application Architecture Concepts","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#microservices","title":"Microservices","text":"<p>An architectural pattern where an application is split into small, independent services that each handle one specific function.</p> <p>Example: Instead of one big \"Sprocket\" application, we have separate services for: - Web UI - API - Discord bot - Image generation - Matchmaking - Notifications</p> <p>Benefits: Each service can be updated independently. If one fails, others keep working.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>A way for applications to communicate with each other programmatically. Our GraphQL API lets the web UI, Discord bot, and other services request data.</p> <p>Real-world analogy: Like a waiter at a restaurant - you tell them what you want, they bring it from the kitchen. You don't need to know how the kitchen works.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#graphql","title":"GraphQL","text":"<p>A query language for APIs. Instead of having dozens of different endpoints, clients can request exactly the data they need in a single query.</p> <p>Example: One request can get user info, their recent matches, and team details, rather than making 3 separate requests.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#message-queue","title":"Message Queue","text":"<p>A system for asynchronous communication between services. One service puts a message in the queue, another service processes it later.</p> <p>Our message queue: RabbitMQ</p> <p>Use case: When a match finishes, the API puts a message in the queue. The notification service reads it and sends Discord notifications. The analytics service reads it and records stats.</p> <p>Why queues?: Services don't have to wait for each other. API can return immediately while background tasks process.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#rabbitmq","title":"RabbitMQ","text":"<p>A popular open-source message queue system using the AMQP protocol.</p> <p>Real-world analogy: Like a mailbox - sender drops a letter in, recipient picks it up when ready.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#pubsub-publishsubscribe","title":"Pub/Sub (Publish/Subscribe)","text":"<p>A messaging pattern where: - Publishers send messages to topics - Subscribers listen to topics they're interested in</p> <p>Example: Match results are \"published\" to a topic. Notification service, analytics service, and ELO service all \"subscribe\" to get the results.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#deployment-operations-tools","title":"Deployment &amp; Operations Tools","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>Defining infrastructure (servers, networks, databases) using code files instead of clicking through web interfaces.</p> <p>Benefits: - Version controlled (track changes in git) - Repeatable (deploy the same way every time) - Self-documenting (code shows what exists) - Easy rollback (restore previous version)</p> <p>Our IaC tool: Pulumi</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#pulumi","title":"Pulumi","text":"<p>An Infrastructure as Code tool that lets you define infrastructure using real programming languages (we use TypeScript).</p> <p>What it manages: - Docker services - Networks - Volumes - Secrets - Configurations</p> <p>How it works: You write code describing what you want. Pulumi compares it to what currently exists and makes the necessary changes.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#stack-pulumi-stack","title":"Stack (Pulumi Stack)","text":"<p>An instance of your infrastructure. We have 3 stacks: - <code>layer_1</code>: Core infrastructure (Traefik, Vault) - <code>layer_2</code>: Data services (Redis, databases, monitoring) - <code>prod</code>: Application services (Sprocket web, API, bot)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#terraform","title":"Terraform","text":"<p>Another popular Infrastructure as Code tool (competitor to Pulumi). We don't use it, but you might see it mentioned in industry documentation.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#cicd-continuous-integration-continuous-deployment","title":"CI/CD (Continuous Integration / Continuous Deployment)","text":"<p>Automated processes for testing and deploying code: - CI: Automatically test code when developers commit changes - CD: Automatically deploy tested code to production</p> <p>Current state: We don't have CI/CD yet (manual deployments via Pulumi). Future improvement.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#git","title":"Git","text":"<p>Version control system that tracks changes to code over time. Like \"track changes\" in Word, but much more powerful.</p> <p>Our git repository: <code>sprocket-infra</code> on GitHub</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#github","title":"GitHub","text":"<p>A website that hosts git repositories and provides collaboration tools. Our infrastructure code lives here.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#metrics","title":"Metrics","text":"<p>Numerical data about system performance collected over time: - CPU usage: 45% - Memory usage: 3.2GB - Request rate: 150 requests/minute - Error rate: 0.3%</p> <p>Why it matters: Helps identify problems before users notice. \"CPU is at 90%\" warns you to add resources.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#logs","title":"Logs","text":"<p>Text records of events that happen in applications. Each log entry typically has: - Timestamp - Severity (INFO, WARN, ERROR) - Message describing what happened</p> <p>Example: <code>2025-11-08 10:30:15 ERROR: Failed to connect to database</code></p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#monitoring","title":"Monitoring","text":"<p>Continuously collecting metrics and logs to understand system health.</p> <p>Our monitoring stack: - InfluxDB: Stores time-series metrics - Grafana: Visualizes metrics in dashboards - Loki: Aggregates logs from all services - Gatus: Monitors service health/uptime</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#dashboard","title":"Dashboard","text":"<p>A visual display of key metrics, usually showing graphs and charts in real-time.</p> <p>Example: Our Grafana dashboard shows CPU usage, memory usage, request rate, error rate, database connections - all updating live.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#alert","title":"Alert","text":"<p>An automated notification when metrics exceed thresholds.</p> <p>Examples: - \"CPU &gt; 85% for 10 minutes\" \u2192 Send Slack message - \"Error rate &gt; 5%\" \u2192 Page on-call engineer - \"Service down for 2 minutes\" \u2192 Send PagerDuty alert</p> <p>Current state: Monitoring tools installed, but alerts not fully configured yet.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#uptime","title":"Uptime","text":"<p>Percentage of time a service is available and working correctly.</p> <p>Industry targets: - 99% uptime = ~3.65 days downtime per year - 99.9% uptime = ~8.76 hours downtime per year - 99.99% uptime = ~52 minutes downtime per year</p> <p>Our target: 99.9%+ (aiming for \"three 9s\")</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#sla-service-level-agreement","title":"SLA (Service Level Agreement)","text":"<p>A commitment about service availability and performance. Defines what level of service users should expect.</p> <p>Example SLA: \"The service will be available 99.9% of the time, with response times under 500ms for 95% of requests.\"</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#security-secrets-management","title":"Security &amp; Secrets Management","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#secrets","title":"Secrets","text":"<p>Sensitive information like passwords, API keys, and certificates that must be kept secure.</p> <p>Examples: - Database passwords - OAuth client secrets - API tokens - Encryption keys</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#vault-hashicorp-vault","title":"Vault (HashiCorp Vault)","text":"<p>A tool for securely storing and managing secrets. Provides: - Encrypted storage - Access control (who can read which secrets) - Audit logging (track who accessed what) - Secret rotation capabilities</p> <p>Why we use it: Better than storing secrets in code or environment variables. Centralized, secure, auditable.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#unseal-seal-vault","title":"Unseal / Seal (Vault)","text":"<p>Vault starts in a \"sealed\" state where it can't decrypt any secrets. Unsealing requires providing multiple \"unseal keys\" (like needing multiple keys to open a bank vault).</p> <p>Sealed: Vault is running but all data is encrypted and inaccessible Unsealed: Vault can decrypt and serve secrets</p> <p>Why this security model?: Even if someone steals the Vault server, they can't read secrets without unseal keys.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#doppler","title":"Doppler","text":"<p>A cloud-based secrets management service we use as our \"source of truth\" for secrets. Team members can update secrets here, then they're synced to Vault.</p> <p>Workflow: Doppler (team edits) \u2192 Bootstrap script \u2192 Vault (runtime use)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#environment-variable","title":"Environment Variable","text":"<p>A variable set outside your application code that the application can read. Common way to configure applications without hardcoding values.</p> <p>Example: <code>DATABASE_URL=postgresql://...</code> tells the app where to find the database.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#docker-secret","title":"Docker Secret","text":"<p>A secure way to make secrets available to containers in Docker Swarm. Secrets are encrypted in transit and at rest, mounted as read-only files inside containers.</p> <p>How it works: Pulumi creates Docker secrets from Vault values. Containers read secrets from <code>/app/secret/</code> directory.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#sprocket-specific-terms","title":"Sprocket-Specific Terms","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#sprocket","title":"Sprocket","text":"<p>The gaming platform this infrastructure supports. Provides competitive gaming leagues, match tracking, player statistics, and team management for Rocket League esports.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#mle-minor-league-esports","title":"MLE (Minor League Esports)","text":"<p>The organization that runs Sprocket. Provides competitive Rocket League leagues at various skill levels.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#ballchasing","title":"Ballchasing","text":"<p>An external service that stores and analyzes Rocket League replay files. Sprocket integrates with their API to fetch replay data.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#replay","title":"Replay","text":"<p>A recording of a Rocket League match that can be analyzed for statistics. Stored as <code>.replay</code> files, uploaded to Ballchasing or our S3 storage.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#elo","title":"ELO","text":"<p>A rating system for ranking player skill. Named after Arpad Elo who developed it for chess. Used in Sprocket to match players of similar skill.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#matchmaking","title":"Matchmaking","text":"<p>The process of creating balanced matches by grouping players of similar skill levels.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#service-specific-terms","title":"Service-Specific Terms","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#traefik","title":"Traefik","text":"<p>A modern reverse proxy and load balancer. Acts as the entry point for all HTTP/HTTPS traffic to our infrastructure.</p> <p>What it does: - Routes <code>sprocket.mlesports.gg</code> to the web service - Routes <code>api.sprocket.mlesports.gg</code> to the API service - Handles TLS certificate automation with Let's Encrypt - Terminates HTTPS connections</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#ingress-egress","title":"Ingress / Egress","text":"<ul> <li>Ingress: Traffic coming INTO the system (user requests)</li> <li>Egress: Traffic going OUT of the system (API calls to external services)</li> </ul> <p>Traefik Ingress Network: The Docker network where all public-facing services connect to receive incoming traffic.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#influxdb","title":"InfluxDB","text":"<p>A time-series database optimized for storing metrics that change over time (CPU usage, request rates, temperatures, etc.)</p> <p>vs. PostgreSQL: PostgreSQL stores records (users, matches). InfluxDB stores measurements over time (server CPU at 10:00, 10:01, 10:02...).</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#grafana","title":"Grafana","text":"<p>An open-source platform for visualizing metrics. Creates dashboards with graphs, charts, and alerts.</p> <p>Connects to: InfluxDB (for metrics), Loki (for logs)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#loki","title":"Loki","text":"<p>A log aggregation system developed by Grafana Labs. Collects logs from all services and makes them searchable.</p> <p>Query example: \"Show me all ERROR logs from sprocket-core service in the last hour\"</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#gatus","title":"Gatus","text":"<p>A health monitoring tool that periodically checks if services are up and responding correctly.</p> <p>What it monitors: - HTTP endpoints (is the website accessible?) - Database connectivity - External API availability - Response times</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#n8n","title":"N8n","text":"<p>A workflow automation tool (like Zapier but self-hosted). Used for scheduled tasks and integrations.</p> <p>Use cases: Automated data exports, scheduled reports, webhook integrations</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#neo4j","title":"Neo4j","text":"<p>A graph database - stores data as nodes (entities) and relationships between them. Better than traditional databases for highly connected data.</p> <p>Use case in Sprocket: Player relationships, team hierarchies, social connections</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#telegraf","title":"Telegraf","text":"<p>A metrics collection agent that gathers system stats and forwards them to InfluxDB.</p> <p>What it collects: CPU, memory, disk usage, network I/O, Docker container stats</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#technology-stack","title":"Technology Stack","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#nodejs","title":"Node.js","text":"<p>JavaScript runtime that lets you run JavaScript on servers (not just in browsers). Many of our services are built with Node.js.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#nextjs","title":"Next.js","text":"<p>A React framework for building web applications. Powers our Sprocket Web UI.</p> <p>Why Next.js: Server-side rendering for better performance and SEO.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#nestjs","title":"NestJS","text":"<p>A Node.js framework for building scalable server applications. Powers our Sprocket API.</p> <p>Why NestJS: Great TypeScript support, modular architecture, built-in GraphQL support.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#typescript","title":"TypeScript","text":"<p>A programming language that adds types to JavaScript. Makes code more reliable by catching errors before runtime.</p> <p>Our infrastructure code: Written in TypeScript (Pulumi definitions)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#acronyms-quick-reference","title":"Acronyms Quick Reference","text":"Acronym Full Term Simple Definition API Application Programming Interface How applications talk to each other CDN Content Delivery Network Serves files from servers close to users for speed CI/CD Continuous Integration/Deployment Automated testing and deployment CLI Command Line Interface Text-based way to interact with software DNS Domain Name System Translates domain names to IP addresses HA High Availability System designed to stay running if parts fail HTTP(S) HyperText Transfer Protocol (Secure) How web browsers and servers communicate IaC Infrastructure as Code Defining infrastructure using code JWT JSON Web Token Token proving a user is authenticated OAuth Open Authorization \"Sign in with Google/Discord\" authentication S3 Simple Storage Service Amazon's (and compatible) object storage SLA Service Level Agreement Promise about service availability SSH Secure Shell Encrypted way to access servers remotely SSL/TLS Secure Sockets Layer / Transport Layer Security Encryption for web traffic UI User Interface What users see and interact with URL Uniform Resource Locator Web address (e.g., https://sprocket.mlesports.gg) VM Virtual Machine Software-based computer running inside another computer"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#common-units-measurements","title":"Common Units &amp; Measurements","text":""},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#storage","title":"Storage","text":"<ul> <li>KB (Kilobyte) = 1,024 bytes</li> <li>MB (Megabyte) = 1,024 KB</li> <li>GB (Gigabyte) = 1,024 MB (~1 billion bytes)</li> <li>TB (Terabyte) = 1,024 GB (~1 trillion bytes)</li> </ul> <p>Context: Our database is ~10GB. Our S3 storage costs $5/month for 100GB+.</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#computing","title":"Computing","text":"<ul> <li>CPU (Central Processing Unit): The \"brain\" that executes instructions</li> <li>Core: Modern CPUs have multiple cores (can do multiple things simultaneously)</li> <li>RAM (Random Access Memory): Fast temporary storage for running programs</li> <li>vCPU: Virtual CPU - a portion of a physical CPU allocated to a virtual machine</li> </ul> <p>Our server: 4 vCPUs, 8GB RAM</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#network","title":"Network","text":"<ul> <li>Mbps (Megabits per second): Network speed measurement</li> <li>Latency: Time for data to travel from source to destination (measured in milliseconds)</li> <li>Bandwidth: Maximum data transfer rate</li> <li>Throughput: Actual data transfer rate achieved</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#time","title":"Time","text":"<ul> <li>ms (millisecond) = 0.001 seconds</li> <li>Response Time: How long it takes a server to respond to a request</li> </ul> <p>Our target: &lt;500ms response time for 95% of requests (p95)</p>"},{"location":"departments/development/systems/production-infrastructure/GLOSSARY/#related-resources","title":"Related Resources","text":"<p>For deeper learning: - Docker documentation: https://docs.docker.com/ - Pulumi documentation: https://www.pulumi.com/docs/ - Traefik documentation: https://doc.traefik.io/traefik/</p> <p>Sprocket-specific: - Main repository: https://github.com/SprocketBot/ - Infrastructure repository: https://github.com/SprocketBot/sprocket-infra</p> <p>Glossary Version: 1.0 Last Updated: November 8, 2025 Maintained By: Infrastructure Team</p> <p>Feedback: If you encounter an undefined term, please add it to this glossary or request clarification in #infrastructure.</p> <p>This glossary is a living document. Please update it when new terms are introduced or when definitions need clarification.</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/","title":"Incident Postmortem Template","text":"<p>Purpose: Use this template for documenting incidents and investigations. Keep it concise but comprehensive.</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#incident-information","title":"Incident Information","text":"<p>Date/Time: [When did the incident occur?] Duration: [How long did the incident last?] Severity: [P0 - Critical | P1 - High | P2 - Medium | P3 - Low] Status: [Resolved | In Progress | Monitoring] Author: [Who wrote this postmortem?] Reviewers: [Who reviewed this?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#executive-summary","title":"Executive Summary","text":"<p>[2-3 sentence overview of what happened, the impact, and the resolution]</p> <p>Impact: - [Number of users affected] - [Services impacted] - [Data loss/corruption: Yes/No] - [Financial impact if applicable]</p> <p>Root Cause: [One sentence description of what caused the incident]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#timeline","title":"Timeline","text":"<p>All times in [timezone]</p> Time Event Action Taken HH:MM [What happened] [What was done] HH:MM [Detection/Alert] [Who was notified] HH:MM [Investigation began] [What was checked] HH:MM [Root cause identified] [What was found] HH:MM [Fix implemented] [What was changed] HH:MM [Incident resolved] [Service restored] HH:MM [Monitoring period] [Verified stability] <p>Total Time to Detect: [Time from incident start to detection] Total Time to Resolve: [Time from detection to resolution] Total Impact Duration: [Total time users were affected]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#what-happened","title":"What Happened","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#background","title":"Background","text":"<p>[Provide context about the system/feature involved. What was the normal state before the incident?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#incident-description","title":"Incident Description","text":"<p>[Detailed description of what went wrong. Include symptoms, error messages, and observable behavior.]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#detection","title":"Detection","text":"<p>[How was the incident discovered? Alert? User report? Monitoring?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#investigation","title":"Investigation","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#initial-hypothesis","title":"Initial Hypothesis","text":"<p>[What did we initially think was wrong?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#investigation-steps","title":"Investigation Steps","text":"<ol> <li>[First thing checked]</li> <li>Finding: [What was discovered]</li> <li> <p>Conclusion: [What this told us]</p> </li> <li> <p>[Second thing checked]</p> </li> <li>Finding: [What was discovered]</li> <li> <p>Conclusion: [What this told us]</p> </li> <li> <p>[Continue as needed...]</p> </li> </ol>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#key-evidence","title":"Key Evidence","text":"<ul> <li>[Log entries, metrics, screenshots, or other evidence that helped identify the issue]</li> <li>[Include specific error messages, stack traces, or data points]</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>[Detailed explanation of the actual root cause. Use the \"5 Whys\" technique if helpful:]</p> <ol> <li>Why did [symptom] happen? Because [reason]</li> <li>Why did [reason] happen? Because [deeper reason]</li> <li>Why did [deeper reason] happen? Because [root cause]</li> </ol> <p>Final Root Cause: [Clear statement of the fundamental issue]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#resolution","title":"Resolution","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#immediate-fix","title":"Immediate Fix","text":"<p>[What was done to restore service immediately?]</p> <pre><code># Include relevant commands or code changes\n[example command or configuration]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#verification","title":"Verification","text":"<p>[How did we verify the fix worked?] - [Metric/test 1] - [Metric/test 2] - [User verification]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#rollback-plan","title":"Rollback Plan","text":"<p>[If the fix didn't work, what was the rollback plan? Was it needed?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#prevention-follow-up","title":"Prevention &amp; Follow-up","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#permanent-fix","title":"Permanent Fix","text":"<p>[What needs to be done to permanently solve this? Is the immediate fix sufficient?]</p> <p>Status: [Completed | Scheduled | Pending] Timeline: [When will this be done?] Owner: [Who is responsible?]</p>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#action-items","title":"Action Items","text":"Action Owner Priority Due Date Status [Specific task to prevent recurrence] [Name] [P0-P3] [Date] [Done/In Progress/Pending] [Improve monitoring/alerting] [Name] [P0-P3] [Date] [Done/In Progress/Pending] [Update documentation] [Name] [P0-P3] [Date] [Done/In Progress/Pending] [Add tests/validation] [Name] [P0-P3] [Date] [Done/In Progress/Pending]"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#lessons-learned","title":"Lessons Learned","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#what-went-well","title":"What Went Well","text":"<ul> <li>[Things that worked during incident response]</li> <li>[Effective tools, processes, or communication]</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#what-could-be-improved","title":"What Could Be Improved","text":"<ul> <li>[Detection could have been faster if...]</li> <li>[Response could have been better if...]</li> <li>[Communication could be improved by...]</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>[Important lesson learned]</li> <li>[Important lesson learned]</li> <li>[Important lesson learned]</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#template-usage-notes","title":"Template Usage Notes","text":""},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#when-to-write-a-postmortem","title":"When to Write a Postmortem","text":"<ul> <li>Always: P0 (Critical) and P1 (High) incidents</li> <li>Usually: P2 (Medium) incidents that affected users</li> <li>Sometimes: P3 (Low) incidents if there are important lessons</li> <li>Always: Any incident that could have been prevented with better processes</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#tips-for-writing-good-postmortems","title":"Tips for Writing Good Postmortems","text":"<ol> <li>Be blameless: Focus on systems and processes, not individuals</li> <li>Be specific: Include exact times, commands, and error messages</li> <li>Be honest: Document mistakes and what went wrong</li> <li>Be actionable: Every \"What Could Be Improved\" should have an action item</li> <li>Be concise: Aim for 2-3 pages maximum</li> <li>Write quickly: Complete within 2-3 days of incident resolution</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#severity-definitions","title":"Severity Definitions","text":"Level Definition Example Response Time P0 - Critical Complete service outage or data loss \"Website down for all users\" Immediate (5 min) P1 - High Major functionality broken, significant user impact \"Login broken, payments failing\" 30 minutes P2 - Medium Partial functionality degraded, some users affected \"Search slow, some pages timing out\" 4 hours P3 - Low Minor issue, minimal user impact \"Typo on page, deprecated warning in logs\" Next business day"},{"location":"departments/development/systems/production-infrastructure/INCIDENT_POSTMORTEM_TEMPLATE/#review-process","title":"Review Process","text":"<ol> <li>Author writes first draft within 24-48 hours</li> <li>Technical reviewer validates technical accuracy</li> <li>Team reviews action items in next team meeting</li> <li>Manager approves and archives</li> <li>Follow-up check action items after 30 days</li> </ol> <p>Delete this section when creating actual postmortem</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/","title":"Sprocket Infrastructure Operations Runbook","text":"<p>Version: 1.1 Last Updated: December 4, 2025 Audience: Operations Team, DevOps Engineers</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Daily Operations</li> <li>Service Management</li> <li>Monitoring &amp; Alerts</li> <li>Backup &amp; Recovery</li> <li>Scaling Operations</li> <li>Security Operations</li> <li>Incident Response</li> <li>Maintenance Procedures</li> <li>Common Tasks</li> <li>Emergency Contacts</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#daily-operations","title":"Daily Operations","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#morning-health-check","title":"Morning Health Check","text":"<p>Frequency: Every weekday morning Duration: ~10 minutes Responsibility: On-call engineer</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#checklist","title":"Checklist","text":"<pre><code># 1. Check all services are running\ndocker service ls\n\n# All services should show N/N replicas (e.g., 1/1, 3/3)\n# If any show 0/N or X/N (where X &lt; N), investigate\n\n# 2. Check system resources\ndocker node ls\ndocker stats --no-stream\n\n# CPU/Memory should be &lt; 80% on average\n# If consistently high, consider scaling\n\n# 3. Run health check script\n./quick-test.sh\n\n# Should show all green checks\n\n# 4. Check recent errors\ndocker service logs traefik --since 24h | grep -i error | wc -l\ndocker service logs prod-sprocket-core-service --since 24h | grep -i error | head -20\n\n# Review any ERROR level logs\n\n# 5. Check Gatus dashboard\n# Navigate to https://gatus.sprocket.mlesports.gg\n# All services should be \"UP\"\n\n# 6. Check Grafana dashboards\n# Navigate to https://grafana.sprocket.mlesports.gg\n# Review key metrics:\n# - Request rate\n# - Error rate\n# - Response times\n# - Database connections\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#normal-vs-abnormal-examples","title":"Normal vs Abnormal Examples","text":"<p>\u2705 NORMAL - All Services Running</p> <pre><code>$ docker service ls\nID             NAME                              MODE         REPLICAS   IMAGE\nabc123def456   prod-sprocket-web-service         replicated   1/1        asaxplayinghorse/sprocket-web:main\nbcd234efg567   prod-sprocket-core-service        replicated   1/1        asaxplayinghorse/sprocket-core:main\ncde345fgh678   prod-discord-bot-service          replicated   1/1        asaxplayinghorse/sprocket-discord-bot:main\ndef456ghi789   layer2redis-redis-primary         replicated   1/1        redis:6-alpine\nefg567hij890   rabbitmq-service                  replicated   1/1        rabbitmq:3-management\n\n(All showing N/N replicas = healthy)\n</code></pre> <p>\u274c ABNORMAL - Service Not Running</p> <pre><code>$ docker service ls\nID             NAME                              MODE         REPLICAS   IMAGE\nabc123def456   prod-sprocket-web-service         replicated   0/1        asaxplayinghorse/sprocket-web:main\n                                                              ^^^--- PROBLEM: Service has 0/1 replicas\n\nAction required: Check logs with `docker service logs prod-sprocket-web-service --tail 100`\n</code></pre> <p>\u2705 NORMAL - System Resources</p> <pre><code>$ docker stats --no-stream\nCONTAINER           CPU %     MEM USAGE / LIMIT     MEM %     NET I/O\nsprocket-web        2.5%      450MiB / 8GiB        5.62%     1.2MB / 3.4MB\nsprocket-core       5.3%      680MiB / 8GiB        8.50%     4.5MB / 12MB\nredis               0.8%      120MiB / 8GiB        1.50%     500KB / 800KB\npostgres            3.2%      400MiB / 8GiB        5.00%     2MB / 5MB\n\n(All containers &lt;10% CPU, &lt;15% Memory = healthy)\n</code></pre> <p>\u274c ABNORMAL - High Resource Usage</p> <pre><code>$ docker stats --no-stream\nCONTAINER           CPU %     MEM USAGE / LIMIT     MEM %\nsprocket-core       95.2%     7.2GiB / 8GiB        90.0%\n                    ^^^^      ^^^^^^^^^^^^        ^^^^--- PROBLEM: Near memory limit\n                    High CPU usage\n\nPossible causes:\n- Memory leak (check for gradual growth over time in Grafana)\n- Traffic spike (check request rate in Grafana)\n- Inefficient query (check slow query logs)\n- Infinite loop or runaway process\n\nAction: Check logs, review Grafana metrics, consider restarting service if unresponsive\n</code></pre> <p>\u2705 NORMAL - Error Logs</p> <pre><code>$ docker service logs prod-sprocket-core-service --since 24h | grep -i error\n2025-11-08T10:23:15Z WARN: Retry attempt 1 for external API call\n2025-11-08T11:45:32Z WARN: User authentication failed (invalid token)\n2025-11-08T13:12:08Z INFO: Error handled gracefully, returned 400 to client\n\n(Occasional warnings and handled errors = normal)\n(Total count &lt;50 errors/day = healthy)\n</code></pre> <p>\u274c ABNORMAL - High Error Rate</p> <pre><code>$ docker service logs prod-sprocket-core-service --since 24h | grep -i error | wc -l\n2847\n^^^^--- PROBLEM: 2,847 errors in 24 hours (should be &lt;50)\n\n$ docker service logs prod-sprocket-core-service --since 1h | grep -i error | head -5\n2025-11-08T14:01:23Z ERROR: Database connection timeout\n2025-11-08T14:01:25Z ERROR: Database connection timeout\n2025-11-08T14:01:27Z ERROR: Database connection timeout\n2025-11-08T14:01:29Z ERROR: Database connection timeout\n2025-11-08T14:01:31Z ERROR: Database connection timeout\n                              ^^^--- PROBLEM: Repeated database errors\n\nAction: Check database connectivity and health immediately\n</code></pre> <p>\u2705 NORMAL - Gatus Dashboard</p> <pre><code>Service Status Dashboard (https://gatus.sprocket.mlesports.gg)\n\nSprocket Web          \u2713 UP      Response: 145ms    Uptime: 99.98%\nSprocket API          \u2713 UP      Response: 67ms     Uptime: 99.95%\nDiscord Bot           \u2713 UP      Response: N/A      Uptime: 99.92%\nPostgreSQL            \u2713 UP      Response: 12ms     Uptime: 100%\nRedis                 \u2713 UP      Response: 2ms      Uptime: 99.99%\n\n(All services UP, response times &lt;500ms = healthy)\n</code></pre> <p>\u274c ABNORMAL - Service Down in Gatus</p> <pre><code>Service Status Dashboard\n\nSprocket Web          \u2717 DOWN    Last check: 2min ago    Uptime: 97.23%\n                      ^^^^^^--- PROBLEM: Service unreachable\n\nSprocket API          \u26a0 SLOW    Response: 5,234ms       Uptime: 99.15%\n                      ^^^^^^    ^^^^^^^^^^^^^--- PROBLEM: Very slow response (should be &lt;500ms)\n\nRecent Failures:\n- 14:23: Sprocket Web returned 502 Bad Gateway\n- 14:24: Sprocket Web connection timeout\n- 14:25: Sprocket API response time &gt;5s (threshold: 1s)\n\nAction: Immediately check service health, review logs, check upstream dependencies\n</code></pre> <p>\u2705 NORMAL - Grafana Metrics</p> <pre><code>Request Rate:           350 requests/min       (steady)\nError Rate:             0.2%                   (&lt;1% = healthy)\nP95 Response Time:      245ms                  (&lt;500ms = healthy)\nDatabase Connections:   12/100                 (well below limit)\nCPU Usage:              45%                    (plenty of headroom)\nMemory Usage:           4.2GB / 8GB (52%)      (healthy)\n</code></pre> <p>\u274c ABNORMAL - Grafana Metrics</p> <pre><code>Request Rate:           1,850 requests/min     (normally 350) \u26a0 SPIKE\nError Rate:             15.3%                  (normally &lt;1%) \u274c HIGH\nP95 Response Time:      4,523ms                (normally &lt;500ms) \u274c SLOW\nDatabase Connections:   98/100                 (near limit) \u26a0 WARNING\nCPU Usage:              92%                    (sustained high) \u26a0 WARNING\nMemory Usage:           7.6GB / 8GB (95%)      (near limit) \u274c CRITICAL\n\nPattern suggests: Traffic spike combined with performance degradation\nLikely cause: DDoS attack, viral event, or system degradation\nAction: Check traffic sources, implement rate limiting, scale if legitimate traffic\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#expected-output","title":"Expected Output","text":"<pre><code>\u2713 All services running (15/15)\n\u2713 System resources healthy (&lt;70% utilization)\n\u2713 No critical errors in last 24h\n\u2713 All health checks passing\n\u2713 Metrics within normal range\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#escalation","title":"Escalation","text":"<p>If health check fails: 1. Check Incident Response section 2. Notify team in #infrastructure-alerts Slack channel 3. Begin investigation immediately</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#weekly-review","title":"Weekly Review","text":"<p>Frequency: Every Monday Duration: ~30 minutes Responsibility: Team lead</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#checklist_1","title":"Checklist","text":"<ul> <li>[ ] Review uptime metrics (target: 99.9%+)</li> <li>[ ] Review error rate trends</li> <li>[ ] Check disk usage (alert if &gt;80%)</li> <li>[ ] Review database performance</li> <li>[ ] Check certificate expiration dates</li> <li>[ ] Review pending updates</li> <li>[ ] Check backup status</li> <li>[ ] Review access logs for anomalies</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#tasks","title":"Tasks","text":"<pre><code># 1. Generate uptime report\n# From Gatus dashboard: https://gatus.sprocket.mlesports.gg\n# Export last 7 days uptime data\n\n# 2. Check disk usage\ndocker system df\ndf -h\n\n# If disk usage &gt;80%:\n# - Clean old images: docker image prune -a\n# - Clean old logs: docker system prune\n# - Check volume sizes: docker volume ls\n\n# 3. Check certificate expiration\necho | openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg 2&gt;/dev/null | openssl x509 -noout -dates\n\n# Should show expiration &gt;30 days in future\n# Let's Encrypt auto-renews at 30 days\n\n# 4. Check database metrics\n# In Grafana: PostgreSQL dashboard\n# Review:\n# - Connection count\n# - Query performance\n# - Table sizes\n# - Cache hit ratio\n\n# 5. Check pending updates\ndocker images | grep -v \"latest\\|main\"\n# Review image versions, check for updates\n\n# 6. Verify backups\n# Check Digital Ocean console\n# Verify daily backups are running\n# Last backup should be &lt;24 hours old\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#service-management","title":"Service Management","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#starting-a-service","title":"Starting a Service","text":"<pre><code># Update service if stopped\ndocker service update --force &lt;service-name&gt;\n\n# Or scale from 0 to 1\ndocker service scale &lt;service-name&gt;=1\n\n# Wait for service to start\ndocker service ps &lt;service-name&gt;\n\n# Check logs\ndocker service logs -f &lt;service-name&gt;\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#stopping-a-service","title":"Stopping a Service","text":"<pre><code># Scale to 0 (keeps service definition)\ndocker service scale &lt;service-name&gt;=0\n\n# Or remove completely\ndocker service rm &lt;service-name&gt;\n\n# Note: Removing requires redeployment via Pulumi\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#restarting-a-service","title":"Restarting a Service","text":"<pre><code># Force update (rolling restart)\ndocker service update --force &lt;service-name&gt;\n\n# Or with zero downtime (if replicas &gt; 1)\ndocker service update --force --update-parallelism 1 &lt;service-name&gt;\n\n# Monitor restart\nwatch -n 2 \"docker service ps &lt;service-name&gt; | head -10\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#updating-a-service","title":"Updating a Service","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#update-image-version","title":"Update Image Version","text":"<pre><code>cd platform\n\n# Update image tag in Pulumi config\npulumi config set image-tag v1.2.3\n\n# Preview changes\npulumi preview\n\n# Apply update\npulumi up\n\n# Monitor rollout\ndocker service logs -f prod-sprocket-web-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#update-environment-variables","title":"Update Environment Variables","text":"<pre><code># Update via Pulumi config\ncd platform\npulumi config set some-config-value new-value\n\n# Or update service directly (temporary)\ndocker service update \\\n  --env-add NEW_VAR=value \\\n  &lt;service-name&gt;\n\n# Permanent changes should go through Pulumi\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#rolling-back-a-service","title":"Rolling Back a Service","text":"<pre><code># Roll back to previous image\ndocker service update --rollback &lt;service-name&gt;\n\n# Or via Pulumi\ncd platform\npulumi stack export &gt; backup-before-rollback.json\n# Restore previous stack state\npulumi stack import &lt; previous-backup.json\npulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#monitoring-alerts","title":"Monitoring &amp; Alerts","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#monitoring-architecture","title":"Monitoring Architecture","text":"<p>\u26a0\ufe0f Important Context: The monitoring diagram below shows a generic cloud monitoring pattern using GCP services. Our actual monitoring uses self-hosted open-source tools on our Docker Swarm infrastructure.</p> <p></p> <p>Our Actual Monitoring Stack: - Data Sources: Docker containers (Sprocket services), system metrics (CPU, memory, disk), application logs - Collection: Telegraf (metrics collector), Docker log drivers (log collection) - Storage: InfluxDB (time-series metrics), Loki (log aggregation), PostgreSQL (structured data) - Visualization: Grafana (dashboards and graphs) - Health Monitoring: Gatus (uptime monitoring and health checks) - Alerts: Grafana alerts (email, webhook) - not yet fully configured</p> <p>The diagram represents a future cloud-native monitoring setup or conceptual reference. Our current implementation is fully self-hosted on our infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#service-level-agreements-slas","title":"Service Level Agreements (SLAs)","text":"<p>Target Performance Standards</p> <p>Our infrastructure aims to meet the following service level agreements. These targets guide our monitoring, alerting, and capacity planning decisions.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#availability-targets","title":"Availability Targets","text":"Service Uptime Target Max Downtime/Month Max Downtime/Year Web Application 99.9% 43 minutes 8.76 hours GraphQL API 99.9% 43 minutes 8.76 hours Discord Bot 99.5% 3.6 hours 43.8 hours Database (PostgreSQL) 99.99% 4.3 minutes 52.6 minutes Redis Cache 99.9% 43 minutes 8.76 hours Overall Platform 99.9% 43 minutes 8.76 hours <p>Why these targets? - 99.9% (three nines): Industry standard for SaaS applications, balances reliability with cost - 99.99% (four nines): For managed database - Digital Ocean provides this SLA - 99.5% for Discord bot: Lower target acceptable as Discord integration is supplementary, not critical path</p> <p>Current Performance (as of November 2025): - Web Application: 99.95% (exceeding target) - API: 99.92% (exceeding target) - Discord Bot: 99.87% (exceeding target) - Database: 100% (managed service)</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#performance-targets","title":"Performance Targets","text":"Metric Target Warning Threshold Critical Threshold Notes API Response Time (p95) &lt;500ms &gt;800ms &gt;2000ms 95% of requests should complete in &lt;500ms API Response Time (p99) &lt;1000ms &gt;1500ms &gt;3000ms 99% of requests should complete in &lt;1s Web Page Load Time &lt;2s &gt;3s &gt;5s First contentful paint Error Rate &lt;1% &gt;2% &gt;5% Percentage of requests returning 5xx errors Database Query Time (p95) &lt;50ms &gt;100ms &gt;500ms 95% of queries &lt;50ms Cache Hit Rate (Redis) &gt;80% &lt;70% &lt;50% Percentage of cache hits vs. misses <p>Why these targets? - 500ms p95: Industry research shows users perceive &lt;500ms as \"instant\" - &lt;1% error rate: Allows for transient failures while maintaining quality - &gt;80% cache hit rate: Ensures Redis is effectively reducing database load</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#resource-utilization-targets","title":"Resource Utilization Targets","text":"Resource Normal Range Warning Threshold Critical Threshold Action Required CPU Usage 30-70% &gt;80% sustained &gt;90% sustained Scale up or optimize Memory Usage 40-70% &gt;85% &gt;95% Investigate memory leaks or scale Disk Usage &lt;60% &gt;80% &gt;90% Clean up logs/images or expand storage Database Connections &lt;50/100 &gt;80/100 &gt;95/100 Optimize connection pooling or scale Network Bandwidth &lt;50% &gt;70% &gt;90% Check for DDoS or scale bandwidth <p>Why these ranges? - 30-70% CPU: Leaves headroom for traffic spikes while not wasting resources - &lt;50 DB connections: Managed PostgreSQL has 100 connection limit; staying well below avoids contention - &lt;60% disk: Growth buffer; log retention and image storage can grow quickly</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#recovery-time-targets","title":"Recovery Time Targets","text":"Scenario Target RTO Target RPO Notes Service Restart &lt;5 minutes 0 (no data loss) Automated restart via Docker Swarm Layer Redeployment &lt;30 minutes 0 (no data loss) Using Pulumi with existing state Database Restore &lt;2 hours &lt;24 hours From Digital Ocean automated backups Full Infrastructure Rebuild &lt;6 hours &lt;24 hours From scratch using docs + backups Vault Recovery &lt;30 minutes 0 (no data loss) Unseal keys stored locally <p>RTO (Recovery Time Objective): How long to restore service RPO (Recovery Point Objective): Maximum data loss acceptable</p> <p>Why these targets? - &lt;5min service restart: Automated health checks trigger restarts - &lt;6hr full rebuild: Documented in DEPLOYMENT_GUIDE.md, tested quarterly - &lt;24hr RPO: Daily database backups provide acceptable data loss window</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#alert-response-targets","title":"Alert Response Targets","text":"Alert Severity Response Time Resolution Time Escalation Critical (platform down) &lt;15 minutes &lt;2 hours Immediate: Page on-call engineer High (degraded performance) &lt;30 minutes &lt;4 hours Within 30min: Notify team lead Medium (service issue) &lt;2 hours &lt;8 hours Next business day: Review in daily standup Low (informational) &lt;24 hours &lt;1 week Track in backlog <p>Critical Alerts (page immediately): - Platform completely down - Database unreachable - Error rate &gt;10% - Multiple services down - Security breach detected</p> <p>High Alerts (notify within 30min): - Single service down - Response time &gt;2x normal - Error rate &gt;5% - Disk usage &gt;90% - Memory usage &gt;95%</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#sla-monitoring","title":"SLA Monitoring","text":"<p>Weekly SLA Review:</p> <pre><code># Check uptime from Gatus\n# Navigate to: https://gatus.sprocket.mlesports.gg\n# Export uptime report for last 7 days\n\n# Calculate current month uptime\n# Target: &gt;99.9% (43 minutes downtime budget/month)\n# If &lt;99.9%, investigate root causes\n</code></pre> <p>Monthly SLA Report: - Total downtime this month - Incidents that caused downtime - Performance vs. targets (response times, error rates) - Resource utilization trends - Capacity planning recommendations</p> <p>SLA Breach Process: 1. Document incident (when, what, why, how long) 2. Calculate impact on monthly SLA 3. Conduct post-mortem (see Incident Response) 4. Identify preventive measures 5. Update runbooks/monitoring as needed</p> <p>Current SLA Status:</p> <pre><code>Month: November 2025\nUptime target: 99.9% (43min downtime budget)\nCurrent uptime: 99.95% (21min downtime)\nRemaining budget: 22 minutes\n\n\u2713 On track to meet SLA\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#system-metrics","title":"System Metrics","text":"<ul> <li>CPU Usage: &lt;80% average</li> <li>Memory Usage: &lt;80% average</li> <li>Disk Usage: &lt;80% total</li> <li>Network I/O: Monitor for anomalies</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#application-metrics","title":"Application Metrics","text":"<ul> <li>Request Rate: Baseline ~100 req/min</li> <li>Error Rate: &lt;1% of requests</li> <li>Response Time: p95 &lt;500ms, p99 &lt;1s</li> <li>Active Users: Monitor trends</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#service-health","title":"Service Health","text":"<ul> <li>Service Replicas: All N/N</li> <li>Container Restarts: &lt;3 per day per service</li> <li>Failed Health Checks: 0</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#accessing-monitoring-tools","title":"Accessing Monitoring Tools","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#grafana-unified-logs-and-metrics-interface","title":"Grafana Unified Logs and Metrics Interface","text":"<p>Unified Access Point: https://grafana.sprocket.mlesports.gg</p> <p>Grafana provides a unified interface for both logs and metrics, consolidating monitoring into a single dashboard:</p> <pre><code># URL: https://grafana.sprocket.mlesports.gg\n# Login: admin / &lt;password from Vault&gt;\n\n# Unified Interface Features:\n# - Metrics: Real-time system and application metrics via InfluxDB/Telegraf\n# - Logs: Centralized log aggregation via Loki\n# - Correlation: View metrics and logs side-by-side for troubleshooting\n# - Time-sync: Correlate events across logs and metrics with unified timeline\n\n# Key Dashboards:\n# - System Overview: Node resources, Docker stats, CPU, memory, disk\n# - Application Metrics: Request rates, latencies, error rates\n# - Database: PostgreSQL performance, connection pools, query times\n# - Service Health: Container states, restarts, health checks\n# - Logs Explorer: Query and filter logs from all services\n\n# Log Query Examples (in Explore view):\n# {service=\"prod-sprocket-core-service\"} |= \"ERROR\"\n# {service=\"traefik\"} |= \"404\"\n# {service=\"vault\"} |~ \"unseal|seal\"\n</code></pre> <p>How to Use the Unified Interface:</p> <ol> <li>View Metrics:</li> <li>Navigate to Dashboards \u2192 Select dashboard</li> <li>View real-time graphs and metrics</li> <li> <p>Drill down into specific time ranges</p> </li> <li> <p>Query Logs:</p> </li> <li>Navigate to Explore (compass icon)</li> <li>Select Loki as data source</li> <li>Use LogQL queries to filter logs</li> <li> <p>Example: <code>{service=\"prod-sprocket-core-service\"} |= \"ERROR\"</code></p> </li> <li> <p>Correlate Logs and Metrics:</p> </li> <li>In Explore, add multiple queries</li> <li>Use split view to compare metrics and logs</li> <li>Sync time ranges to correlate events</li> <li> <p>Click on metric spikes to see corresponding logs</p> </li> <li> <p>Common Workflows:</p> </li> <li>Debugging: Select time range on metric graph \u2192 View corresponding logs</li> <li>Performance: Compare response time metrics with error logs</li> <li>Capacity: Monitor resource metrics while reviewing application logs</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#gatus-service-monitoring","title":"Gatus Service Monitoring","text":"<pre><code># URL: https://gatus.sprocket.mlesports.gg\n\n# Shows:\n# - Uptime percentage\n# - Response times\n# - Recent downtime events\n# - Health check history\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#digital-ocean-master-node-access-for-debugging","title":"Digital Ocean Master Node Access for Debugging","text":"<p>The infrastructure runs on a single Digital Ocean Droplet that serves as the Docker Swarm master node. You can access it via the Digital Ocean UI for direct debugging:</p> <p>Access Methods:</p> <ol> <li> <p>Via Digital Ocean Console (Web-based SSH):    <code>bash    # 1. Log into Digital Ocean console: https://cloud.digitalocean.com    # 2. Navigate to: Droplets \u2192 Select your droplet (e.g., \"sprocket-master-node\")    # 3. Click \"Console\" in the top-right (or \"Access\" \u2192 \"Launch Droplet Console\")    # 4. Login with root or your configured user</code></p> </li> <li> <p>Via SSH (Direct):    <code>bash    # From your local machine    ssh root@&lt;droplet-ip&gt;    # Or if using SSH keys:    ssh -i ~/.ssh/your-key root@&lt;droplet-ip&gt;</code></p> </li> </ol> <p>Common Debugging Tasks on the Master Node:</p> <p>1. Check Service Health:</p> <pre><code># List all Docker services\ndocker service ls\n\n# Check specific service status\ndocker service ps prod-sprocket-core-service\n\n# View service logs\ndocker service logs -f prod-sprocket-core-service --tail 100\n\n# Check resource usage\ndocker stats --no-stream\n</code></pre> <p>2. Inspect Container State:</p> <pre><code># List running containers\ndocker ps\n\n# Get container ID for a service\nCONTAINER_ID=$(docker ps -q -f name=prod-sprocket-core)\n\n# Exec into running container for debugging\ndocker exec -it $CONTAINER_ID sh\n# Or bash if available:\ndocker exec -it $CONTAINER_ID bash\n\n# Inside container, check:\nenv                    # Environment variables\nnetstat -tuln         # Network connections\nps aux                # Running processes\ndf -h                 # Disk usage\n</code></pre> <p>3. Check System Resources:</p> <pre><code># Disk usage\ndf -h\ndu -sh /var/lib/docker/*\n\n# Memory usage\nfree -h\n\n# CPU usage\ntop\n# Press 'q' to quit\n\n# Docker storage\ndocker system df\n</code></pre> <p>4. Network Debugging:</p> <pre><code># Test DNS resolution\nnslookup vault.sprocket.mlesports.gg\n\n# Test internal service connectivity\ncurl http://layer2redis-redis-primary:6379\ncurl http://rabbitmq-service:5672\n\n# Check Docker networks\ndocker network ls\ndocker network inspect traefik-ingress\n</code></pre> <p>5. View Docker Swarm Status:</p> <pre><code># Check swarm status\ndocker info | grep Swarm\n\n# List nodes (should show master + workers if multi-node)\ndocker node ls\n\n# View swarm tasks\ndocker service ps $(docker service ls -q)\n</code></pre> <p>Security Note: Access to the master node provides full control over the infrastructure. Only authorized personnel should have SSH access. Consider: - Using SSH key authentication (disable password auth) - Restricting SSH access to specific IP addresses via firewall - Using tools like <code>fail2ban</code> to prevent brute-force attacks - Enabling audit logging for SSH sessions</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#setting-up-alerts","title":"Setting Up Alerts","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#alert-channels","title":"Alert Channels","text":"<p>Configure in Grafana: 1. Navigate to Alerting \u2192 Notification channels 2. Add Slack, Email, or PagerDuty 3. Test notification</p> <p>Note: Based on development conversations, alerting is not yet fully configured. This is a priority for operational readiness.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#conversation-derived-alert-thresholds","title":"Conversation-Derived Alert Thresholds","text":"<p>From the Claude conversations, these thresholds were identified as critical: - Critical: Service down, error rate &gt;10%, certificate expiring &lt;7 days - Warning: Response time &gt;2x normal, resource usage &gt;85% - Info: Certificate expiring &lt;30 days, resource usage &gt;70%</p> <p>These thresholds were determined through iterative testing during the rebuild process.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#example-alert-rules","title":"Example Alert Rules","text":"<p>High Error Rate:</p> <pre><code>Alert Name: High Error Rate\nCondition: Error rate &gt; 5% for 5 minutes\nSeverity: Critical\nNotification: #infrastructure-alerts\n</code></pre> <p>Service Down:</p> <pre><code>Alert Name: Service Unavailable\nCondition: Service replicas &lt; desired for 2 minutes\nSeverity: Critical\nNotification: PagerDuty + #infrastructure-alerts\n</code></pre> <p>High CPU Usage:</p> <pre><code>Alert Name: High CPU Usage\nCondition: CPU &gt; 85% for 10 minutes\nSeverity: Warning\nNotification: #infrastructure-alerts\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#backup-recovery","title":"Backup &amp; Recovery","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#database-backups","title":"Database Backups","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#automated-backups-digital-ocean","title":"Automated Backups (Digital Ocean)","text":"<pre><code># Backups run automatically via Digital Ocean\n# Frequency: Daily\n# Retention: 7 days\n# Time: ~2 AM UTC\n\n# Verify backups:\n# 1. Log into Digital Ocean console\n# 2. Navigate to Databases \u2192 sprocketbot-postgres\n# 3. Backups tab\n# 4. Verify latest backup &lt; 24 hours old\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#manual-database-backup","title":"Manual Database Backup","text":"<pre><code># Create snapshot\nPGPASSWORD='your-password' pg_dump \\\n  -h sprocketbot-postgres-...j.db.ondigitalocean.com \\\n  -p 25060 \\\n  -U doadmin \\\n  -d defaultdb \\\n  -F c \\\n  -f backup-$(date +%Y%m%d-%H%M%S).dump\n\n# Upload to S3\naws s3 cp backup-*.dump s3://sprocket-backups/database/ \\\n  --endpoint-url https://nyc3.digitaloceanspaces.com\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#database-restore","title":"Database Restore","text":"<pre><code># From automated backup (Digital Ocean):\n# 1. Go to Digital Ocean console\n# 2. Database \u2192 Backups\n# 3. Select backup\n# 4. Click \"Restore\"\n# 5. Choose to restore to new cluster or existing\n# 6. Wait for restore to complete (~10-30 minutes)\n# 7. Update Pulumi configs with new connection details if new cluster\n\n# From manual backup:\nPGPASSWORD='your-password' pg_restore \\\n  -h sprocketbot-postgres-...j.db.ondigitalocean.com \\\n  -p 25060 \\\n  -U doadmin \\\n  -d defaultdb \\\n  -c \\\n  backup-20251108-120000.dump\n\n# Verify restore\nPGPASSWORD='your-password' psql \\\n  -h sprocketbot-postgres-...j.db.ondigitalocean.com \\\n  -p 25060 \\\n  -U doadmin \\\n  -d defaultdb \\\n  -c \"SELECT COUNT(*) FROM users;\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#vault-backups","title":"Vault Backups","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#backup-vault-data","title":"Backup Vault Data","text":"<pre><code># Vault data is stored in S3 (Digital Ocean Spaces)\n# Bucket: vault-secrets\n# Path: vault_storage/\n\n# Backup entire bucket\naws s3 sync s3://vault-secrets/ ./vault-backup-$(date +%Y%m%d)/ \\\n  --endpoint-url https://nyc3.digitaloceanspaces.com\n\n# Backup unseal keys (CRITICAL!)\ncp -r global/services/vault/unseal-tokens/ ~/vault-unseal-backup-$(date +%Y%m%d)/\n\n# Store securely (encrypted, off-site)\ntar -czf vault-backup-$(date +%Y%m%d).tar.gz \\\n  vault-backup-*/\n\n# Encrypt\ngpg -c vault-backup-$(date +%Y%m%d).tar.gz\n\n# Upload to secure location\naws s3 cp vault-backup-$(date +%Y%m%d).tar.gz.gpg \\\n  s3://sprocket-secure-backups/vault/\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#restore-vault","title":"Restore Vault","text":"<pre><code># 1. Restore S3 data\naws s3 sync ./vault-backup-20251108/ s3://vault-secrets/ \\\n  --endpoint-url https://nyc3.digitaloceanspaces.com\n\n# 2. Restore unseal keys\ncp -r ~/vault-unseal-backup-20251108/ global/services/vault/unseal-tokens/\n\n# 3. Restart Vault service\ndocker service update --force vault-service\n\n# 4. Vault should auto-unseal using restored keys\n\n# 5. Verify Vault status\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg\nexport VAULT_TOKEN=$(cat global/services/vault/unseal-tokens/root_token.txt)\n\nvault status\n# Should show: Sealed: false\n\n# 6. Verify secrets\nvault kv list platform/sprocket/manual/oauth\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#pulumi-state-backups","title":"Pulumi State Backups","text":"<pre><code># Export current state (all stacks)\ncd layer_1\npulumi stack export &gt; ~/pulumi-backups/layer_1-$(date +%Y%m%d).json\n\ncd ../layer_2\npulumi stack export &gt; ~/pulumi-backups/layer_2-$(date +%Y%m%d).json\n\ncd ../platform\npulumi stack export &gt; ~/pulumi-backups/platform-$(date +%Y%m%d).json\n\n# Backup to S3\ncd ~/pulumi-backups\ntar -czf pulumi-state-$(date +%Y%m%d).tar.gz *.json\n\naws s3 cp pulumi-state-$(date +%Y%m%d).tar.gz \\\n  s3://sprocket-backups/pulumi-state/\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#disaster-recovery-test","title":"Disaster Recovery Test","text":"<p>Frequency: Quarterly Duration: 4-6 hours Responsibility: Team lead + ops engineer</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#procedure","title":"Procedure","text":"<ol> <li>Prepare Test Environment</li> <li>Provision separate test node</li> <li>Clone repository</li> <li> <p>Set up Pulumi backend access</p> </li> <li> <p>Test Database Restore</p> </li> <li>Restore latest automated backup to new cluster</li> <li>Verify data integrity</li> <li> <p>Test application connectivity</p> </li> <li> <p>Test Vault Restore</p> </li> <li>Restore Vault S3 data to test bucket</li> <li>Deploy Vault with test backend</li> <li> <p>Unseal and verify secrets</p> </li> <li> <p>Test Full Deployment</p> </li> <li>Deploy Layer 1 from scratch</li> <li>Deploy Layer 2</li> <li>Deploy Platform</li> <li> <p>Verify all services working</p> </li> <li> <p>Document Results</p> </li> <li>Record time to restore</li> <li>Document issues encountered</li> <li> <p>Update runbook if needed</p> </li> <li> <p>Cleanup</p> </li> <li>Destroy test resources</li> <li>Delete test data</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#scaling-operations","title":"Scaling Operations","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#vertical-scaling-single-node","title":"Vertical Scaling (Single Node)","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#scale-up-node-resources","title":"Scale Up Node Resources","text":"<pre><code># Digital Ocean:\n# 1. Power off node\n# 2. Resize droplet (more CPU/RAM)\n# 3. Power on node\n# 4. Verify services restart\n\n# Or with zero downtime (if multi-node):\n# 1. Add new larger node to swarm\n# 2. Drain old node\n# 3. Remove old node\n# 4. Destroy old node\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#when-to-scale-up","title":"When to Scale Up","text":"<ul> <li>CPU consistently &gt;70%</li> <li>Memory consistently &gt;70%</li> <li>Disk I/O bottlenecks</li> <li>Network bandwidth saturated</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#horizontal-scaling-multiple-nodes","title":"Horizontal Scaling (Multiple Nodes)","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#add-node-to-swarm","title":"Add Node to Swarm","text":"<pre><code># On manager node (current node)\ndocker swarm join-token worker\n\n# Output will be:\n# docker swarm join --token SWMTKN-1-xxx... &lt;manager-ip&gt;:2377\n\n# On new worker node\ndocker swarm join --token SWMTKN-1-xxx... &lt;manager-ip&gt;:2377\n\n# Verify node joined\ndocker node ls\n# Should show new node\n\n# Label node (optional)\ndocker node update --label-add role=worker node-2\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#scale-services-across-nodes","title":"Scale Services Across Nodes","text":"<pre><code># Increase replicas\ndocker service scale prod-sprocket-web-service=3\n\n# Service will distribute across available nodes\n\n# Constrain service to specific nodes (optional)\ndocker service update \\\n  --constraint-add node.labels.role==worker \\\n  prod-sprocket-web-service\n\n# Verify distribution\ndocker service ps prod-sprocket-web-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#scaling-considerations","title":"Scaling Considerations","text":"<p>Stateless Services (can scale horizontally): - Sprocket Web - Sprocket API - Discord Bot - All microservices</p> <p>Stateful Services (need careful planning): - Redis (requires Redis Cluster for multi-node) - RabbitMQ (requires clustering) - Vault (requires HA setup with Consul/etcd)</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#auto-scaling-future","title":"Auto-Scaling (Future)","text":"<p>Currently manual scaling only. Future improvements: - Docker Swarm auto-scaling (via custom scripts) - Metrics-based scaling triggers - Load-based node addition</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#security-operations","title":"Security Operations","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#secret-rotation","title":"Secret Rotation","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#rotate-database-password","title":"Rotate Database Password","text":"<pre><code># 1. Generate new password\nNEW_PASSWORD=$(openssl rand -base64 32)\n\n# 2. Update Digital Ocean database\n# Via console: Settings \u2192 Users \u2192 Reset Password\n\n# 3. Update Vault\nvault kv put infrastructure/data/postgres password=\"$NEW_PASSWORD\"\n\n# 4. Update Pulumi config\ncd layer_1\npulumi config set --secret postgres-password \"$NEW_PASSWORD\"\n\n# 5. Update services\ncd ../platform\npulumi up\n\n# 6. Verify connectivity\ndocker service logs prod-sprocket-core-service | grep -i \"database connection\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#rotate-vault-root-token","title":"Rotate Vault Root Token","text":"<pre><code># 1. Generate new root token\nvault token create -policy=root\n\n# Output:\n# Key                  Value\n# ---                  -----\n# token                s.xxxxxxxxxxxxxx\n\n# 2. Update environment variable\nexport VAULT_TOKEN=s.xxxxxxxxxxxxxx\n\n# 3. Revoke old root token (if not the initial root token)\nvault token revoke &lt;old-token&gt;\n\n# 4. Store new token securely\necho \"s.xxxxxxxxxxxxxx\" &gt; global/services/vault/unseal-tokens/root_token.txt\n\n# 5. Update documentation\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#rotate-oauth-credentials","title":"Rotate OAuth Credentials","text":"<pre><code># 1. Generate new credentials in provider console\n# (Google Cloud Console, Discord Developer Portal, etc.)\n\n# 2. Update Doppler\ndoppler secrets set GOOGLE_CLIENT_ID=\"new-id\"\ndoppler secrets set GOOGLE_CLIENT_SECRET=\"new-secret\"\n\n# 3. Re-run bootstrap script\ncd scripts\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg\nexport VAULT_TOKEN=$(cat ../global/services/vault/unseal-tokens/root_token.txt)\n./bootstrap-vault-secrets.sh\n\n# 4. Restart affected services\ndocker service update --force prod-sprocket-core-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#access-review","title":"Access Review","text":"<p>Frequency: Quarterly Responsibility: Security lead</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#checklist_2","title":"Checklist","text":"<ul> <li>[ ] Review GitHub organization members</li> <li>[ ] Review Vault policies and access</li> <li>[ ] Review Doppler project access</li> <li>[ ] Review SSH key access to nodes</li> <li>[ ] Review Digital Ocean account access</li> <li>[ ] Review Pulumi backend access</li> <li>[ ] Revoke unnecessary access</li> <li>[ ] Update access documentation</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#security-scanning","title":"Security Scanning","text":"<pre><code># Scan Docker images for vulnerabilities\n# Using Trivy (install first: https://github.com/aquasecurity/trivy)\ntrivy image asaxplayinghorse/sprocket-web:main\n\n# Scan infrastructure code\n# Using Checkov (install: pip install checkov)\ncd layer_1\ncheckov -d .\n\n# Review results and address HIGH/CRITICAL findings\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#certificate-management","title":"Certificate Management","text":"<pre><code># Check certificate expiration\necho | openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg 2&gt;/dev/null | openssl x509 -noout -dates\n\n# Output:\n# notBefore=Oct 10 00:00:00 2025 GMT\n# notAfter=Jan 8 00:00:00 2026 GMT  # Auto-renews at 30 days before\n\n# Let's Encrypt certificates auto-renew via Traefik\n# Monitor Traefik logs for renewal errors:\ndocker service logs traefik | grep -i \"renew\\|acme\"\n\n# If renewal fails:\n# 1. Check DNS is correct\n# 2. Check port 80 is accessible\n# 3. Check Let's Encrypt rate limits\n# 4. Restart Traefik: docker service update --force traefik\n\n#### Certificate Issues from Development\n\nBased on Claude conversations, these specific certificate issues were encountered:\n- Rate limiting (50 certificates/week per domain)\n- DNS propagation delays\n- Certificate provisioning timing (60+ seconds per attempt)\n\n**Recovery from Rate Limits**:\n- Wait 1 hour for soft limit reset\n- Use staging environment for testing\n- Verify DNS before requesting production certs\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#incident-response","title":"Incident Response","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#severity-levels","title":"Severity Levels","text":"<p>P0 - Critical: - Complete platform outage - Data loss/corruption - Security breach</p> <p>P1 - High: - Major feature unavailable - Significant performance degradation - High error rates (&gt;5%)</p> <p>P2 - Medium: - Minor feature unavailable - Moderate performance issues - Elevated error rates (1-5%)</p> <p>P3 - Low: - Cosmetic issues - Non-critical bugs - Low error rates (&lt;1%)</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#incident-response-process","title":"Incident Response Process","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#1-detection","title":"1. Detection","text":"<pre><code># Incident detected via:\n# - Automated alerts (Gatus, Grafana)\n# - User reports\n# - Monitoring dashboards\n# - Health checks\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#2-triage","title":"2. Triage","text":"<pre><code># Determine severity\n# P0/P1: Page on-call immediately\n# P2: Notify team, begin investigation\n# P3: Create ticket for next business day\n\n# Create incident channel\n# Slack: #incident-YYYYMMDD-description\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#3-investigation","title":"3. Investigation","text":"<pre><code># Gather information\n# 1. Check service status\ndocker service ls\n\n# 2. Check recent logs\ndocker service logs --since 1h &lt;affected-service&gt; | tail -100\n\n# 3. Check system resources\ndocker stats --no-stream\n\n# 4. Check recent changes\ncd platform\ngit log --since=\"2 hours ago\" --oneline\n\n# 5. Check external dependencies\n# - Database (Digital Ocean console)\n# - DNS (check propagation)\n# - Network (ping, traceroute)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#4-mitigation","title":"4. Mitigation","text":"<p>Service Down:</p> <pre><code># Restart service\ndocker service update --force &lt;service-name&gt;\n\n# Or rollback recent deployment\ndocker service update --rollback &lt;service-name&gt;\n</code></pre> <p>High Load:</p> <pre><code># Scale up\ndocker service scale &lt;service-name&gt;=3\n\n# Or throttle requests (Traefik rate limiting)\n# Update Traefik config to add rate limiter\n</code></pre> <p>Database Issues:</p> <pre><code># Check connections\n# In Digital Ocean console: Metrics \u2192 Connections\n\n# Kill long-running queries\nPGPASSWORD='pass' psql -h host -p 25060 -U doadmin -d defaultdb \\\n  -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'active' AND query_start &lt; now() - interval '5 minutes';\"\n</code></pre> <p>Certificate Expired:</p> <pre><code># Force cert renewal\ndocker service update --force traefik\n\n# Monitor logs\ndocker service logs -f traefik | grep -i acme\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#5-communication","title":"5. Communication","text":"<pre><code># Update status page (if exists)\n# Post in #incidents channel\n# Message template:\n</code></pre> <pre><code>\ud83d\udea8 Incident Update \ud83d\udea8\n\nIncident: [P0/P1/P2] Service Unavailable\nTime: YYYY-MM-DD HH:MM UTC\nStatus: Investigating / Mitigating / Resolved\nImpact: [describe user impact]\n\nDetails:\n[brief description]\n\nNext Update: [time]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#6-resolution","title":"6. Resolution","text":"<pre><code># Verify fix\n./quick-test.sh\n\n# Monitor for 30 minutes\nwatch -n 60 \"docker service ls &amp;&amp; docker stats --no-stream\"\n\n# Declare resolved when:\n# - Service restored\n# - No errors in logs\n# - Metrics normal\n# - User reports cleared\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#7-post-incident-review","title":"7. Post-Incident Review","text":"<pre><code># Post-Incident Report Template\n\n## Incident Summary\n- **Date**: YYYY-MM-DD\n- **Duration**: X hours\n- **Severity**: PX\n- **Services Affected**: [list]\n\n## Timeline\n- HH:MM - Incident detected\n- HH:MM - Investigation started\n- HH:MM - Root cause identified\n- HH:MM - Fix implemented\n- HH:MM - Incident resolved\n\n## Root Cause\n[Detailed explanation]\n\n## Resolution\n[How it was fixed]\n\n## Impact\n- Users affected: ~X\n- Revenue impact: $X\n- Downtime: X minutes\n\n## Action Items\n- [ ] [Preventive measure 1]\n- [ ] [Preventive measure 2]\n- [ ] [Documentation update]\n\n## Lessons Learned\n[What we learned]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#maintenance-procedures","title":"Maintenance Procedures","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#updating-docker-images","title":"Updating Docker Images","text":"<pre><code># Update image tag\ncd platform\npulumi config set image-tag v1.2.3\n\n# Preview changes\npulumi preview\n\n# Apply update (rolling)\npulumi up\n\n# Monitor rollout\ndocker service ps prod-sprocket-web-service --no-trunc\ndocker service logs -f prod-sprocket-web-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#updating-infrastructure-code","title":"Updating Infrastructure Code","text":"<pre><code># Pull latest code\ncd sprocket-infra\ngit pull origin main\n\n# Install dependencies (if changed)\ncd layer_1\nnpm install\n\ncd ../layer_2\nnpm install\n\ncd ../platform\nnpm install\n\n# Preview changes\npulumi preview\n\n# If changes look good:\npulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#system-updates","title":"System Updates","text":"<pre><code># Update node OS (requires downtime for single-node)\n# 1. Schedule maintenance window\n# 2. Notify users\n# 3. Export Pulumi state (backup)\npulumi stack export &gt; backup.json\n\n# 4. Update OS\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# 5. Reboot if kernel updated\nsudo reboot\n\n# 6. Wait for node to come back\n\n# 7. Verify Docker Swarm is active\ndocker info | grep Swarm\n\n# 8. Verify services restarted\ndocker service ls\n\n# 9. Run health check\n./quick-test.sh\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#database-maintenance","title":"Database Maintenance","text":"<pre><code># Database maintenance is handled by Digital Ocean\n# They perform:\n# - OS updates\n# - PostgreSQL updates\n# - Security patches\n# - Performance tuning\n\n# You can schedule maintenance windows in DO console:\n# Database \u2192 Settings \u2192 Maintenance Window\n# Recommended: Off-peak hours (2-4 AM)\n\n# Manual maintenance tasks:\n\n# 1. Analyze tables (improves query performance)\nPGPASSWORD='pass' psql -h host -p 25060 -U doadmin -d defaultdb \\\n  -c \"ANALYZE;\"\n\n# 2. Vacuum (reclaim storage)\nPGPASSWORD='pass' psql -h host -p 25060 -U doadmin -d defaultdb \\\n  -c \"VACUUM;\"\n\n# 3. Check table bloat\nPGPASSWORD='pass' psql -h host -p 25060 -U doadmin -d defaultdb \\\n  -c \"SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size FROM pg_tables WHERE schemaname NOT IN ('pg_catalog', 'information_schema') ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC LIMIT 10;\"\n\n#### Database Issues from Development\n\nFrom Claude conversations, these specific database issues were encountered:\n- Connection timeouts: Check network connectivity, firewall rules\n- Too many connections: Scale connection pool, optimize queries\n- Authentication failures: Verify credentials in Vault, check user permissions\n\n**Connection Management**:\n- Managed PostgreSQL has 100 connection limit\n- Stay well below limit to avoid contention\n- Monitor connection count in Digital Ocean console\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#common-tasks","title":"Common Tasks","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#view-service-logs","title":"View Service Logs","text":"<pre><code># Tail logs\ndocker service logs -f &lt;service-name&gt;\n\n# Tail last 100 lines\ndocker service logs --tail 100 &lt;service-name&gt;\n\n# Since specific time\ndocker service logs --since 2h &lt;service-name&gt;\n\n# Filter for errors\ndocker service logs &lt;service-name&gt; 2&gt;&amp;1 | grep -i error\n\n# Follow multiple services\ndocker service logs -f \\\n  prod-sprocket-web-service \\\n  prod-sprocket-core-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#check-service-health","title":"Check Service Health","text":"<pre><code># Service replicas\ndocker service ls | grep &lt;service-name&gt;\n\n# Service tasks\ndocker service ps &lt;service-name&gt;\n\n# Service details\ndocker service inspect &lt;service-name&gt;\n\n# Service logs\ndocker service logs &lt;service-name&gt; --tail 50\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#exec-into-container","title":"Exec into Container","text":"<pre><code># Get container ID\nCONTAINER_ID=$(docker ps -q -f name=&lt;service-name&gt;)\n\n# Exec into container\ndocker exec -it $CONTAINER_ID sh\n\n# Or bash (if available)\ndocker exec -it $CONTAINER_ID bash\n\n# Common tasks in container:\n# - Check environment variables: env\n# - Check processes: ps aux\n# - Check network: netstat -tuln\n# - Check disk: df -h\n# - Check files: ls -la /app\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#check-network-connectivity","title":"Check Network Connectivity","text":"<pre><code># From node to external service\ncurl https://api.external.com\n\n# From container to external service\ndocker exec $CONTAINER_ID curl https://api.external.com\n\n# Between containers\ndocker exec $CONTAINER_ID ping &lt;other-service-name&gt;\ndocker exec $CONTAINER_ID curl http://&lt;other-service-name&gt;:&lt;port&gt;\n\n# DNS resolution\ndocker exec $CONTAINER_ID nslookup &lt;service-name&gt;\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#clear-docker-resources","title":"Clear Docker Resources","text":"<p>Background: Docker storage can grow significantly over time due to: - Container layer duplication in overlay2 - Unlimited log growth from services - Unused images and volumes - Build cache accumulation</p> <p>Monitoring Storage:</p> <pre><code># Check overall Docker disk usage\ndocker system df\n\n# Check detailed breakdown\ndocker system df -v\n\n# Check overlay2 directory size\ndu -sh /var/lib/docker/overlay2\n\n# Check system disk usage\ndf -h /var/lib/docker\n</code></pre> <p>Storage Management Improvements (implemented Nov 2025): - Enhanced log rotation: 5MB max size, 5 files, compression enabled - Volume size limits: Redis (2GB), MinIO (10GB), InfluxDB (5GB), Airbyte (5GB) - Automated cleanup scripts available in repository root</p> <p>Cleanup Commands:</p> <pre><code># Remove stopped containers\ndocker container prune -f\n\n# Remove unused images\ndocker image prune -a -f\n\n# Remove unused volumes (\u26a0\ufe0f CAREFUL! May delete data)\ndocker volume prune -f\n\n# Remove unused networks\ndocker network prune -f\n\n# All at once (recommended for routine cleanup)\ndocker system prune -a -f\n\n# Check space saved\ndocker system df\n</code></pre> <p>Automated Cleanup Script:</p> <pre><code># Use the automated cleanup script (if available)\n./docker-storage-cleanup.sh\n\n# This script:\n# - Prunes all unused Docker resources\n# - Analyzes overlay2 directory\n# - Reports storage savings\n# - Provides recommendations\n</code></pre> <p>When to Run Cleanup: - Monthly maintenance (scheduled) - When disk usage &gt;80% - After major deployments - When Docker overlay2 exceeds 15GB</p> <p>Expected Results: - Typical cleanup: Reclaim 2-5GB - Major cleanup (after months): Reclaim 10-18GB - Post-cleanup overlay2: ~6-10GB (with containers running)</p> <p>RabbitMQ Connection Management (added Nov 2025): RabbitMQ services now include improved connection handling: - Proper health checks to prevent connection leaks - Connection pooling configuration - Auto-reconnect on connection failures</p> <p>If experiencing RabbitMQ connection issues:</p> <pre><code># Check RabbitMQ service health\ndocker service ps rabbitmq-service\n\n# View RabbitMQ logs\ndocker service logs rabbitmq-service --tail 100\n\n# Restart RabbitMQ if needed\ndocker service update --force rabbitmq-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#emergency-contacts","title":"Emergency Contacts","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#on-call-rotation","title":"On-Call Rotation","text":"Week Primary Secondary TBD Engineer 1 Engineer 2 TBD Engineer 2 Engineer 3 TBD Engineer 3 Engineer 1"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#contact-methods","title":"Contact Methods","text":"<p>During Business Hours (9 AM - 5 PM): - Slack: #infrastructure channel - Email: infra-team@example.com</p> <p>After Hours / Emergencies: - PagerDuty: (if configured) - Phone: On-call engineer - Escalation: Team lead</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#external-support","title":"External Support","text":"<p>Digital Ocean Support: - Priority Support: https://cloud.digitalocean.com/support - Phone: +1 (XXX) XXX-XXXX - Email: support@digitalocean.com</p> <p>Pulumi Support: - Slack: Pulumi Community - Email: support@pulumi.com - Docs: https://www.pulumi.com/docs/</p> <p>GitHub Support: - Email: support@github.com - Status: https://www.githubstatus.com/</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#service-status-pages","title":"Service Status Pages","text":"<ul> <li>Digital Ocean: https://status.digitalocean.com/</li> <li>GitHub: https://www.githubstatus.com/</li> <li>Docker Hub: https://status.docker.com/</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#appendix","title":"Appendix","text":""},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#useful-commands-reference","title":"Useful Commands Reference","text":"<pre><code># Docker Swarm\ndocker swarm init                    # Initialize swarm\ndocker node ls                       # List nodes\ndocker service ls                    # List services\ndocker service ps &lt;service&gt;          # List service tasks\ndocker service logs &lt;service&gt;        # View service logs\n\n# Docker System\ndocker system df                     # Show disk usage\ndocker system prune                  # Clean up unused resources\ndocker stats                         # Show resource usage\n\n# Pulumi\npulumi preview                       # Preview changes\npulumi up                            # Apply changes\npulumi destroy                       # Destroy resources\npulumi stack export                  # Export state\npulumi config                        # View configuration\n\n# Vault\nvault status                         # Check Vault status\nvault operator unseal                # Unseal Vault\nvault kv get &lt;path&gt;                  # Get secret\nvault kv put &lt;path&gt; key=value        # Put secret\n\n# PostgreSQL\npsql -h &lt;host&gt; -p &lt;port&gt; -U &lt;user&gt; -d &lt;db&gt;    # Connect\npg_dump -h &lt;host&gt; -p &lt;port&gt; -U &lt;user&gt; -d &lt;db&gt; # Backup\npg_restore -h &lt;host&gt; -p &lt;port&gt; -U &lt;user&gt; -d &lt;db&gt; &lt;file&gt; # Restore\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#quick-reference-urls","title":"Quick Reference URLs","text":"<ul> <li>Traefik Dashboard: https://traefik.sprocket.mlesports.gg</li> <li>Vault UI: https://vault.sprocket.mlesports.gg</li> <li>Grafana: https://grafana.sprocket.mlesports.gg</li> <li>Gatus: https://gatus.sprocket.mlesports.gg</li> <li>Sprocket Web: https://sprocket.mlesports.gg</li> <li>Sprocket API: https://api.sprocket.mlesports.gg</li> </ul> <p>Operations Runbook Version: 1.1 Last Updated: December 4, 2025 Review Cycle: Quarterly Next Review: March 2026</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#troubleshooting-guide-from-development-experience","title":"Troubleshooting Guide (From Development Experience)","text":"<p>This section contains specific solutions derived from the Claude conversation history during the infrastructure rebuild project. These are proven solutions to problems that were encountered and resolved during development.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#vault-unsealing-issues","title":"Vault Unsealing Issues","text":"<p>Problem: Vault shows as sealed after restart Root Cause: Auto-unseal script not working or unseal keys missing Solution:</p> <pre><code># Check if unseal tokens exist\nls -la global/services/vault/unseal-tokens/\n\n# If missing, manually unseal with stored keys\nvault operator unseal\n# Enter first unseal key from your secure storage\n\n# Repeat 2 more times with different keys\nvault status  # Should show \"Sealed: false\"\n</code></pre> <p>Prevention: Ensure unseal keys are backed up securely and the auto-initialization script is working.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#multi-environment-routing-issues","title":"Multi-Environment Routing Issues","text":"<p>Problem: 404 errors when accessing via different methods (localhost, IP, domain) Root Cause: Traefik routing rules not configured for all access patterns Solution:</p> <pre><code># Test each access method\ncurl -H \"Host: sprocket.mlesports.gg\" http://localhost\ncurl -H \"Host: 192.168.4.39\" http://localhost\n\n# Check Traefik routers\ndocker exec traefik wget -q -O- http://localhost:8080/api/http/routers | jq\n</code></pre> <p>Configuration: Ensure Pulumi config includes appropriate routing rules:</p> <pre><code># For local development\nhostname: localhost\nserver-ip: 192.168.4.39\ntailscale-ip: 100.110.185.84\n\n# For production (remove IP-based routing)\nhostname: sprocket.mlesports.gg\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#service-network-connectivity","title":"Service Network Connectivity","text":"<p>Problem: Services can't connect to RabbitMQ, Redis, or other Layer 2 services Root Cause: Network isolation or DNS resolution issues between layers Solution:</p> <pre><code># Check service discovery from container\ndocker exec &lt;service-container&gt; nslookup layer2_rabbitmq\ndocker exec &lt;service-container&gt; ping layer2_redis\n\n# Verify network membership\ndocker network inspect &lt;network-name&gt; | grep -A5 \"Containers\"\n\n# Test connections\ndocker exec &lt;service-container&gt; curl http://layer2_rabbitmq:5672\ndocker exec &lt;service-container&gt; redis-cli -h layer2_redis ping\n</code></pre> <p>Prevention: Ensure all services are on the correct Docker networks and DNS resolution is working.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#environment-variable-issues","title":"Environment Variable Issues","text":"<p>Problem: JWT secrets malformed, duplicate variables, line breaks in tokens Root Cause: Manual environment variable management Solution: Use the bootstrap script approach:</p> <pre><code># Bootstrap all secrets from Doppler to Vault\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg\nexport VAULT_TOKEN=&lt;root-token&gt;\n./scripts/bootstrap-vault-secrets.sh\n\n# Verify secrets in Vault\nvault kv get platform/sprocket/manual/oauth/google\n</code></pre> <p>Historical Context: The <code>generate-env.sh</code> script caused ongoing issues during development, leading to the current Vault-based approach.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#storage-migration-issues","title":"Storage Migration Issues","text":"<p>Problem: S3 connectivity issues after migration from MinIO Root Cause: Configuration changes or credential issues Solution:</p> <pre><code># Test S3 connectivity\naws s3 ls --endpoint-url https://nyc3.digitaloceanspaces.com\n\n# Check Vault S3 backend\nvault status  # Should show initialized: true\n\n# Verify configuration\npulumi config get s3-endpoint\npulumi config get s3-access-key\n</code></pre> <p>Migration Process: The conversations revealed a phased approach: 1. Dual operation (keep MinIO, add cloud S3) 2. Migrate Vault backend to cloud S3 first 3. Migrate application services 4. Remove MinIO completely</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#certificate-and-https-issues","title":"Certificate and HTTPS Issues","text":"<p>Problem: Let's Encrypt certificate renewal failures Root Cause: Rate limiting, DNS issues, or configuration problems Solution:</p> <pre><code># Check certificate expiration\necho | openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg | openssl x509 -noout -dates\n\n# Monitor Traefik logs\ndocker service logs traefik | grep -i \"acme\\|renew\\|certificate\"\n\n# Force renewal if needed\ndocker service update --force traefik\n</code></pre> <p>Known Issues from Development: - Rate limiting: 50 certificates/week per domain - DNS propagation delays - Certificate provisioning takes 60+ seconds</p> <p>Recovery: Use staging environment for testing, wait 1 hour for rate limit reset.</p>"},{"location":"departments/development/systems/production-infrastructure/OPERATIONS_RUNBOOK/#development-vs-production-differences","title":"Development vs Production Differences","text":"<p>Based on conversations during development, key differences between dev and prod environments:</p> <p>Development Environment: - Uses IP-based routing (localhost, LAN IP, Tailscale IP) - Self-signed certificates acceptable - <code>/etc/hosts</code> modifications for local testing - More permissive network access</p> <p>Production Environment: - Real domain names only - Let's Encrypt certificates required - Proper DNS configuration mandatory - Stricter security controls</p> <p>Configuration Strategy:</p> <pre><code># Development - multiple access patterns\nhostname: localhost\nserver-ip: 192.168.4.39\ntailscale-ip: 100.110.185.84\n\n# Production - domain-based only\nhostname: sprocket.mlesports.gg\n# Remove server-ip and tailscale-ip configs\n</code></pre> <p>This runbook is a living document. Please update it as procedures change or new issues are discovered.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/","title":"Sprocket Infrastructure Project Postmortem","text":"<p>Project: Sprocket Platform Infrastructure v1.0 Timeline: September 14, 2025 - November 8, 2025 (8 weeks) Status: \u2705 Successfully Deployed to Production Team: Infrastructure Engineering Author: Generated from project documentation and git history</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#executive-summary","title":"Executive Summary","text":"<p>This postmortem documents the complete journey of rebuilding the Sprocket gaming platform infrastructure from a partially-functional, outdated deployment to a fully production-ready system serving real users. The project took 8 weeks and involved fundamental architectural changes, migration to managed services, and resolution of complex technical challenges.</p> <p>Key Outcomes: - \u2705 Production deployment complete and serving users - \u2705 Three-layer architecture (Infrastructure \u2192 Data \u2192 Applications) - \u2705 Migrated to managed PostgreSQL (Digital Ocean) - \u2705 Migrated to cloud S3 storage (AWS S3 + Digital Ocean Spaces) - \u2705 Automated Vault unsealing and secret provisioning - \u2705 Multi-environment routing support (local, LAN, cloud) - \u2705 HTTPS with Let's Encrypt automation - \u2705 Comprehensive monitoring and observability</p> <p>Final Infrastructure: - Layer 1: Traefik, Vault, Socket Proxy - Layer 2: Redis, RabbitMQ, InfluxDB, Grafana, N8n, Neo4j, Gatus, Loki, Telegraf - Platform: Web UI, API, Discord Bot, Image Generation, 7 microservices</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#project-timeline","title":"Project Timeline","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#visual-timeline-overview","title":"Visual Timeline Overview","text":"<pre><code>September 2025                        October 2025                     November 2025\n|                                     |                                |\nWeek 1     Week 2     Week 3    Week 4-5      Week 6-7         Week 8\n|          |          |          |             |                |\nSept 14    Sept 18    Sept 30   Oct 26        Oct 26-27        Nov 4-8\n|          |          |          |             |                |\nv          v          v          v             v                v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 1 \u2502\u2502 Phase 2 \u2502\u2502 Phase 3 \u2502\u2502 Phase 4 \u2502 \u2502 Phase 5 \u2502     \u2502 Phase 6 \u2502\n\u2502Foundation\u2502 Vault   \u2502\u2502 Storage \u2502\u2502Platform \u2502 \u2502 Routing \u2502     \u2502  Final  \u2502\n\u2502 Rebuild \u2502\u2502Struggles\u2502\u2502Migration\u2502\u2502Resurrect\u2502 \u2502  Hell   \u2502     \u2502  Push   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2713          \u2713          \u2713          \u2713           \u2713               \u2713\n Layer 1    Vault      MinIO\u2192S3   Platform    Multi-env      Production\n Running    Unseals    Complete   Services    Routing        Complete!\n                                  Running     Working\n\nKey Milestones:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nSept 14  \u2502 Initial deployment - Layer 1 running\nSept 19  \u2502 \ud83c\udf89 Vault unsealing breakthrough!\nSept 21  \u2502 Vault fully automated\nSept 30  \u2502 S3 migration begins\nOct 8    \u2502 MinIO completely removed\nOct 26   \u2502 \ud83c\udf89 \"Sprocket is alive!\" - Platform deployed\nOct 27   \u2502 Multi-environment routing solved\nNov 1-3  \u2502 Database migrated to managed PostgreSQL\nNov 7    \u2502 Platform running on production domain\nNov 8    \u2502 \ud83c\udf89\ud83c\udf89\ud83c\udf89 PRODUCTION COMPLETE - v1.0 live!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nTotal Duration: 8 weeks (56 days)\nMajor Breakthroughs: 5\nCritical Blockers Resolved: 3\nServices Deployed: 22\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#effort-breakdown-by-phase","title":"Effort Breakdown by Phase","text":"Phase Duration Complexity Key Challenge Outcome Phase 1: Foundation 5 days \u2b50\u2b50 Getting rusty infrastructure working \u2705 Layer 1 operational Phase 2: Vault 3 days \u2b50\u2b50\u2b50\u2b50\u2b50 Automating Vault unsealing \u2705 Fully automated Phase 3: Storage 8 days \u2b50\u2b50\u2b50 Migrating off MinIO \u2705 Cloud S3 operational Phase 4: Platform 1 day \u2b50\u2b50 Service dependencies \u2705 All services running Phase 5: Routing 2 days \u2b50\u2b50\u2b50\u2b50 Multi-environment access \u2705 All access patterns work Phase 6: Final Push 5 days \u2b50\u2b50\u2b50 Production readiness \u2705 Production live! <p>Most Challenging Phase: Phase 2 (Vault Struggles) - 5 stars Quickest Win: Phase 4 (Platform Resurrection) - 1 day after months of prep Longest Phase: Phase 3 (Storage Migration) - 8 days for safe, phased migration</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-1-foundation-rebuild-sept-14-19-2025","title":"Phase 1: Foundation Rebuild (Sept 14-19, 2025)","text":"<p>Goal: Get Layer 1 infrastructure working again</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#key-milestones","title":"Key Milestones","text":"<ul> <li>Sept 14: Initial deployment - Layer 1 running</li> <li>Sept 16: Layer 2 configuration started</li> <li>Sept 17-18: Vault bootstrap challenges begin</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenges","title":"Challenges","text":"<ul> <li>Infrastructure hadn't been deployed in months (since 2024)</li> <li>Vault initialization process broken</li> <li>MinIO + Vault integration issues</li> <li>Secret bootstrapping not automated</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#outcomes","title":"Outcomes","text":"<ul> <li>Layer 1 deployable (Traefik + Vault + Socket Proxy)</li> <li>Basic networking established</li> <li>Identified critical blocker: Vault unsealing</li> </ul> <p>Commits: <code>3126c6d</code>, <code>8e2c54d</code>, <code>5de5d04</code></p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-2-the-vault-struggles-sept-18-21-2025","title":"Phase 2: The Vault Struggles (Sept 18-21, 2025)","text":"<p>Goal: Automate Vault initialization and unsealing</p> <p>This phase was the most challenging part of the project. Vault's security model requires manual unsealing after each start, creating a chicken-and-egg problem for automation.</p> <p>Conversation Context: The Claude conversations reveal the emotional journey through this challenge: - Sept 18: \"I feel like I'm losing my mind here\" - expressing frustration with Vault issues - Sept 19: \"vault actually unseals!\" - breakthrough moment (commit <code>5057afc</code>) - Sept 21: \"all tokens working now. Vault should be good!\" - final success</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#the-problem","title":"The Problem","text":"<pre><code>Services need secrets from Vault\n    \u2193\nVault must be unsealed to serve secrets\n    \u2193\nUnsealing requires unseal keys\n    \u2193\nUnseal keys must be stored securely\n    \u2193\nWhere do we store them safely?\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#attempts-and-failures","title":"Attempts and Failures","text":"<p>The conversations document the iterative problem-solving process:</p> <ol> <li>Attempt 1: Store unseal keys in Pulumi config</li> <li>Result: Security concern - even encrypted, this felt wrong</li> <li> <p>Conversation: \"Where to store unseal keys securely?\"</p> </li> <li> <p>Attempt 2: Manual unsealing after each deployment</p> </li> <li>Result: Not repeatable, requires human intervention</li> <li> <p>Conversation: \"Manual unsealing is not scalable\"</p> </li> <li> <p>Attempt 3: Store keys in MinIO</p> </li> <li>Result: MinIO needs Vault for credentials (circular dependency)</li> <li>Conversation: \"MinIO needs Vault for credentials - circular dependency\"</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#breakthrough-sept-19","title":"Breakthrough (Sept 19)","text":"<ul> <li>Commit <code>5057afc</code>: \"vault actually unseals!\"</li> <li>Conversation Evidence: User's excitement: \"vault actually unseals!\" shows the breakthrough moment</li> <li>Solution: Store unseal keys in local bind mount</li> <li>Tokens stored at <code>global/services/vault/unseal-tokens/</code></li> <li>Auto-initialization script checks for existing tokens</li> <li>Script unseals Vault automatically on container start</li> <li>S3 backend ensures Vault state persists</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#implementation","title":"Implementation","text":"<pre><code># auto-initialize.sh\n1. Check if Vault is already initialized (via S3 backend)\n2. If not initialized:\n   - Run `vault operator init`\n   - Save tokens to bind-mounted directory\n3. If initialized but sealed:\n   - Read unseal keys from bind-mounted directory\n   - Unseal with 3 of 5 keys\n4. Vault ready to serve secrets\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#final-success-sept-21","title":"Final Success (Sept 21)","text":"<ul> <li>Commit <code>2103358</code>: \"all tokens working now. Vault should be good!\"</li> <li>Vault reliably initializes and unseals</li> <li>Services can retrieve secrets</li> <li>Repeatable deployment achieved</li> </ul> <p>Lessons Learned: - Security vs. automation trade-offs are real - Local bind mounts are acceptable for node-specific secrets - S3 backend is crucial for Vault state persistence - Test the full initialization\u2192unsealing\u2192secret retrieval flow</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-3-storage-migration-sept-30-oct-8-2025","title":"Phase 3: Storage Migration (Sept 30 - Oct 8, 2025)","text":"<p>Goal: Move from self-hosted MinIO to cloud S3 storage</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#why-migrate","title":"Why Migrate?","text":"<p>Problems with MinIO: - Required significant resources (CPU, memory, storage) - Another service to maintain and monitor - Vault S3 backend had instability with self-hosted MinIO - Backup complexity (backing up the backup system) - Single point of failure</p> <p>Benefits of Cloud S3: - Managed service (no maintenance) - Built-in replication and durability (99.999999999%) - Professional-grade reliability - Lower operational overhead - Better performance</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#migration-process","title":"Migration Process","text":"<p>Phase 3a: Dual Operation (Sept 30) - Commit <code>3f32a74</code>: \"migrate to cloud S3 and fix service configurations\" - Kept MinIO running - Migrated Vault backend to Digital Ocean Spaces - Updated <code>SprocketMinioProvider</code> to support direct credentials - Tested stability for several days</p> <p>Phase 3b: Service Migration (Oct 1-7) - Updated application services to use cloud S3 - Migrated existing data (images, replays, backups) - Verified all services working with new storage</p> <p>Phase 3c: Cleanup (Oct 8) - Commit <code>3b45f27</code>: \"Move from MinIO to AWS. Remove postgres network.\" - Removed MinIO service completely - Cleaned up MinIO-related configuration - Updated documentation</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#configuration-changes","title":"Configuration Changes","text":"<pre><code># Before (MinIO)\ns3-endpoint: minio.sprocket.mlesports.gg\ns3-access-key: local-minio-key\ns3-secret-key: local-minio-secret\ns3-port: 9000\nssl: false\n\n# After (Cloud S3)\ns3-endpoint: nyc3.digitaloceanspaces.com\ns3-access-key: DO004ZHPV38R8C9Q46XG\ns3-secret-key: [from Vault]\ns3-port: 443\nssl: true\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#results","title":"Results","text":"<ul> <li>\u2705 Vault backend: Digital Ocean Spaces (vault-secrets bucket)</li> <li>\u2705 Application storage: AWS S3 / Digital Ocean Spaces</li> <li>\u2705 Reduced infrastructure complexity</li> <li>\u2705 Better reliability and durability</li> <li>\u2705 One less service to manage</li> </ul> <p>Lessons Learned: - Managed services reduce operational burden significantly - S3-compatible APIs make migration relatively easy - Test storage backends thoroughly before full migration - Migrate in phases to reduce risk</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-4-platform-resurrection-oct-26-2025","title":"Phase 4: Platform Resurrection (Oct 26, 2025)","text":"<p>Goal: Get application services running</p> <p>After fixing infrastructure and storage, it was time to deploy the actual platform.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenges_1","title":"Challenges","text":"<ul> <li>Services hadn't been deployed in months</li> <li>Configuration drift between local and production</li> <li>Dependency management (services depend on Layer 2)</li> <li>Database schema migrations needed</li> <li>Image versions and tags mismatched</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#breakthrough","title":"Breakthrough","text":"<ul> <li>Commit <code>6395d01</code>: \"Sprocket is alive!\"</li> <li>All platform services successfully deployed</li> <li>Web UI accessible</li> <li>API responding</li> <li>Discord bot connecting</li> <li>Microservices communicating</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#what-made-it-work","title":"What Made It Work","text":"<ol> <li>Proper layer ordering: Layer 1 \u2192 Layer 2 \u2192 Platform</li> <li>Secret provisioning: Ran <code>bootstrap-vault-secrets.sh</code> before platform deployment</li> <li>Database ready: Managed PostgreSQL already provisioned</li> <li>Storage ready: S3 buckets created and accessible</li> <li>Network configuration: All services on correct networks</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-5-routing-hell-oct-26-27-2025","title":"Phase 5: Routing Hell (Oct 26-27, 2025)","text":"<p>Goal: Support multiple access patterns</p> <p>Conversation Context: The conversations reveal the systematic approach to solving routing: - User: \"The platform needed to be accessible via: Local development (.localhost domains), LAN access (Direct IP), Tailscale access, Production (Real domain)\" - Recognition that \"Traefik routing is based on Host header\" was the root cause</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#the-problem_1","title":"The Problem","text":"<p>The platform needed to be accessible via: 1. Local development: <code>.localhost</code> domains (127.0.0.1) 2. LAN access: Direct IP (192.168.4.39) 3. Tailscale access: Tailscale IP (100.110.185.84) 4. Production: Real domain (sprocket.mlesports.gg)</p> <p>Each pattern requires different Traefik routing rules and certificate handling.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#symptoms","title":"Symptoms","text":"<ul> <li>\u2717 Accessing via <code>http://localhost</code> \u2192 404 error</li> <li>\u2717 Accessing via <code>http://192.168.4.39</code> \u2192 404 error</li> <li>\u2717 Accessing via domain \u2192 works but only from internet</li> <li>\u2717 Certificate errors everywhere</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#root-cause","title":"Root Cause","text":"<p>Traefik routes based on <code>Host</code> header:</p> <pre><code>Request to http://localhost\n  \u2192 Host: localhost\n  \u2192 Traefik rule: Host(`sprocket.mlesports.gg`)\n  \u2192 No match \u2192 404\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#solution-oct-27","title":"Solution (Oct 27)","text":"<ul> <li>Commit <code>7486e02</code>: \"add localhost-based routing and cloud deployment documentation\"</li> <li>Conversation Evidence: Systematic testing of different access patterns led to the IP-based routing workaround</li> </ul> <p>Multi-pattern routing:</p> <pre><code>// Platform.ts\nconst hostRules = [];\n\n// Hostname-based routing (production)\nif (hostname) {\n  hostRules.push(`Host(\\`${subdomain}.${hostname}\\`)`);\n}\n\n// IP-based routing (LAN access)\nif (serverIp) {\n  hostRules.push(`Host(\\`${serverIp}\\`)`);\n}\n\n// Tailscale routing\nif (tailscaleIp) {\n  hostRules.push(`Host(\\`${tailscaleIp}\\`)`);\n}\n\nconst routingRule = hostRules.join(' || ');\n</code></pre> <p>Result:</p> <pre><code>Host(`sprocket.mlesports.gg`) || Host(`192.168.4.39`) || Host(`100.110.185.84`)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#configuration-strategy","title":"Configuration Strategy","text":"<p>For Local Development:</p> <pre><code>hostname: localhost\nserver-ip: 192.168.4.39\ntailscale-ip: 100.110.185.84\n</code></pre> <p>For Production:</p> <pre><code>hostname: sprocket.mlesports.gg\n# Remove server-ip and tailscale-ip\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#additional-changes","title":"Additional Changes","text":"<ul> <li>Created <code>CLOUD_DEPLOYMENT.md</code> - production deployment guide</li> <li>Created <code>LOCAL_ACCESS.md</code> - local development guide</li> <li>Fixed duplicate router issues causing 404s</li> <li>Documented <code>/etc/hosts</code> workaround for local development</li> </ul> <p>Lessons Learned: - Design for production first, add development overrides later - IP-based routing is a workaround that may need revisiting for more elegant solutions - Document different access patterns clearly - Test each access method independently - Traefik routing rules can get complex fast</p> <p>Dev vs Prod Commentary: The conversations show this was described as a \"workaround\" - acceptable for development but production should use proper DNS-based routing</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-6-database-migration-nov-1-3-2025","title":"Phase 6: Database Migration (Nov 1-3, 2025)","text":"<p>Goal: Move to managed PostgreSQL</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#the-decision","title":"The Decision","text":"<p>Although PostgreSQL was working in Layer 2, the team decided to migrate to Digital Ocean Managed PostgreSQL for production.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#why-managed-database","title":"Why Managed Database?","text":"<p>Problems with Self-Hosted: - Data persistence concerns with Docker volumes - Backup/restore complexity - Resource management on single node - No built-in high availability - Manual maintenance (updates, patches, tuning) - Disaster recovery difficult</p> <p>Benefits of Managed: - Automated daily backups (7-day retention) - Point-in-time recovery - Professional-grade reliability - Automatic updates and patches - Easy scaling (vertical and horizontal) - Dedicated resources - Built-in monitoring</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#migration-process_1","title":"Migration Process","text":"<ol> <li>Created managed cluster on Digital Ocean</li> <li>Updated Pulumi configs:    <code>yaml    postgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com    postgres-port: 25060</code></li> <li>Removed PostgreSQL service from Layer 2</li> <li>Updated all service configurations to use external database</li> <li>Migrated data from old PostgreSQL to managed instance</li> <li>Updated <code>SprocketPostgresProvider</code> to support external connections</li> <li>Set <code>superuser=false</code> for managed DB</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#results_1","title":"Results","text":"<ul> <li>\u2705 PostgreSQL now an external dependency</li> <li>\u2705 Better reliability and performance</li> <li>\u2705 Automated backups</li> <li>\u2705 Reduced infrastructure complexity</li> <li>\u26a0\ufe0f Added monthly cost (~$15-60/month)</li> <li>\u26a0\ufe0f External dependency (requires internet connectivity)</li> </ul> <p>Lessons Learned: - Don't manage databases in production unless you have to - Managed services are worth the cost for critical data - driven by operational pain identified in conversations - Plan data migration carefully - Update all connection configs atomically - Document external dependencies</p> <p>Operational Insight: Conversations revealed this migration was driven by recognition that MinIO was \"causing more problems than it solved\"</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#phase-7-the-final-push-nov-4-8-2025","title":"Phase 7: The Final Push (Nov 4-8, 2025)","text":"<p>Goal: Deploy to production with real domain</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#last-mile-challenges","title":"Last Mile Challenges","text":"<ul> <li>DNS propagation delays</li> <li>Let's Encrypt rate limiting</li> <li>Certificate provisioning timing</li> <li>Service health verification</li> <li>Final configuration tweaks</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#the-sprint","title":"The Sprint","text":"<ul> <li>Nov 4 (<code>acf4e4e</code>): \"so close!\" - final debugging</li> <li>Nov 7 (<code>f0f2ef5</code>): \"Final stretch.\" - last mile work</li> <li>Nov 7 (<code>e0d8785</code>): \"Platform is up and running\" - services deployed</li> <li>Nov 8 (<code>a2862ea</code>): \"Sprocket v1 is finally up and running completely.\" - PRODUCTION COMPLETE! \ud83c\udf89</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#final-configuration","title":"Final Configuration","text":"<pre><code># Layer 1\nhostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\npostgres-port: \"25060\"\nvault-s3-endpoint: https://nyc3.digitaloceanspaces.com\nvault-s3-bucket: vault-secrets\n\n# Layer 2\nhostname: sprocket.mlesports.gg\ns3-endpoint: nyc3.digitaloceanspaces.com\n\n# Platform\nhostname: sprocket.mlesports.gg\nsubdomain: \"main\"\nimage-tag: main\npostgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#verification","title":"Verification","text":"<pre><code># All services running\ndocker service ls\n# 15/15 services at 1/1 replicas\n\n# HTTPS working\ncurl -I https://sprocket.mlesports.gg\n# HTTP/2 200\n\n# Certificates valid\nopenssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg\n# Issuer: Let's Encrypt\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#technical-challenges-deep-dive","title":"Technical Challenges Deep Dive","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenge-1-vault-bootstrapping-automation","title":"Challenge 1: Vault Bootstrapping Automation","text":"<p>Complexity: \u2b50\u2b50\u2b50\u2b50\u2b50 (Highest)</p> <p>The Chicken-and-Egg Problem:</p> <pre><code>Vault stores secrets \u2192 Services need secrets \u2192 Vault must be unsealed \u2192 Unsealing needs keys \u2192 Where to store keys?\n</code></pre> <p>Attempts: 1. Manual unsealing (not scalable) 2. Keys in Pulumi config (security concern) 3. Keys in MinIO (circular dependency) 4. Cloud KMS auto-unseal (adds complexity and cost)</p> <p>Final Solution: Local bind mount with auto-initialization script</p> <p>Code: <code>global/services/vault/scripts/auto-initialize.sh</code></p> <pre><code>#!/bin/sh\nset -e\n\nvault server -config=/vault.hcl &amp;\nVAULT_PID=$!\nsleep 5\n\nexport VAULT_ADDR=\"http://127.0.0.1:8200\"\n\n# Check if initialized\nif ! vault status | grep -q \"Initialized.*true\"; then\n  # Initialize and save tokens\n  vault operator init -key-shares=5 -key-threshold=3 &gt; /vault/unseal-tokens/unseal_tokens.txt\n  echo \"Vault initialized\"\nfi\n\n# Unseal\nif [ -f \"/vault/unseal-tokens/unseal_tokens.txt\" ]; then\n  grep 'Unseal Key' /vault/unseal-tokens/unseal_tokens.txt | \\\n    awk '{print $4}' | head -3 | \\\n    while read key; do\n      vault operator unseal $key\n    done\n  echo \"Vault unsealed\"\nfi\n\n# Keep container running\nwait $VAULT_PID\n</code></pre> <p>Why It Works: - S3 backend persists Vault state - Unseal keys stored in bind-mounted directory on node - Auto-initialization script handles both first-run and restarts - Repeatable across deployments</p> <p>Trade-offs: - \u2705 Fully automated - \u2705 Repeatable - \u2705 No manual intervention - \u26a0\ufe0f Unseal keys on node filesystem (acceptable for single-node deployment) - \u26a0\ufe0f Not true auto-unseal (would need cloud KMS)</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenge-2-multi-environment-routing","title":"Challenge 2: Multi-Environment Routing","text":"<p>Complexity: \u2b50\u2b50\u2b50\u2b50</p> <p>The Problem: Support 4 different access patterns with one codebase</p> Access Pattern Host Header Use Case localhost <code>sprocket.localhost</code> Local development LAN IP <code>192.168.4.39</code> Remote browser on same network Tailscale IP <code>100.110.185.84</code> Remote access via VPN Production domain <code>sprocket.mlesports.gg</code> Public internet <p>Why It's Hard: - Traefik routing is based on <code>Host</code> header - Let's Encrypt only works with public domains - Certificate handling different for each pattern - Can't use multiple routers with same name</p> <p>Solution: Conditional routing rules based on configuration</p> <pre><code>// Build routing rule based on config\nconst hostRules = [];\n\nif (hostname) {\n  hostRules.push(`Host(\\`${subdomain}.${hostname}\\`)`);\n}\n\nif (serverIp) {\n  hostRules.push(`Host(\\`${serverIp}\\`)`);\n}\n\nif (tailscaleIp) {\n  hostRules.push(`Host(\\`${tailscaleIp}\\`)`);\n}\n\nconst routingRule = hostRules.join(' || ');\n\n// Apply to Traefik labels\nnew TraefikLabels(`sprocket-web-${env}`)\n  .rule(routingRule)\n  .targetPort(3000)\n  .tls(\"lets-encrypt-tls\")\n</code></pre> <p>Configuration Strategy:</p> <pre><code># Local development\nhostname: localhost\nserver-ip: 192.168.4.39\ntailscale-ip: 100.110.185.84\n\n# Production\nhostname: sprocket.mlesports.gg\n# server-ip and tailscale-ip removed\n</code></pre> <p>Results: - \u2705 Single codebase for all environments - \u2705 Configuration-driven routing - \u2705 Works for local dev and production - \u26a0\ufe0f IP-based routing is a workaround - \u26a0\ufe0f Complex routing rules</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenge-3-secret-management-architecture","title":"Challenge 3: Secret Management Architecture","text":"<p>Complexity: \u2b50\u2b50\u2b50\u2b50</p> <p>The Problem: Secrets scattered across multiple systems</p> <p>Secret Sources: 1. Doppler: OAuth credentials, API tokens, application secrets 2. Vault: Runtime secret distribution 3. Pulumi Config: Infrastructure secrets 4. Docker Secrets: Mounted into containers</p> <p>Solution: Hierarchical secret management</p> <pre><code>Doppler (Source of Truth)\n    \u2193\nBootstrap Script\n    \u2193\nVault (Runtime Storage)\n    \u2193\nDocker Secrets\n    \u2193\nApplication Services\n</code></pre> <p>Implementation: <code>scripts/bootstrap-vault-secrets.sh</code></p> <pre><code># Load from Doppler\nexport GOOGLE_CLIENT_ID=$(doppler secrets get GOOGLE_CLIENT_ID)\nexport GOOGLE_CLIENT_SECRET=$(doppler secrets get GOOGLE_CLIENT_SECRET)\n# ... etc\n\n# Provision to Vault\nvault kv put platform/sprocket/manual/oauth/google \\\n  clientId=\"$GOOGLE_CLIENT_ID\" \\\n  clientSecret=\"$GOOGLE_CLIENT_SECRET\"\n</code></pre> <p>Why This Works: - Doppler: Single source of truth, team access control - Vault: Runtime distribution, dynamic secrets, auditing - Docker Secrets: Secure container mounting - Pulumi: Infrastructure-only secrets</p> <p>Secret Flow Example:</p> <pre><code>Google OAuth credentials\n  \u2192 Stored in Doppler\n  \u2192 Bootstrap script reads from Doppler\n  \u2192 Writes to Vault at platform/sprocket/manual/oauth/google\n  \u2192 Pulumi reads from Vault\n  \u2192 Creates Docker secret\n  \u2192 Mounts into core service at /app/secret/googleClientId.txt\n  \u2192 Application reads from file\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenge-4-storage-backend-migration","title":"Challenge 4: Storage Backend Migration","text":"<p>Complexity: \u2b50\u2b50\u2b50</p> <p>Why Migrate from MinIO? - Resource intensive (CPU, memory, storage) - Another service to maintain - Backup complexity - Vault S3 backend instability with self-hosted MinIO</p> <p>Migration Strategy: Phased approach</p> <p>Phase 1: Dual operation</p> <pre><code>// Support both local and cloud S3\nexport class SprocketMinioProvider {\n  constructor(args: {\n    vaultProvider?: vault.Provider,\n    s3Endpoint?: string,\n    accessKey?: string,\n    secretKey?: string\n  }) {\n    if (args.vaultProvider) {\n      // Use Vault for credentials\n    } else {\n      // Use direct credentials (for cloud S3)\n    }\n  }\n}\n</code></pre> <p>Phase 2: Migrate Vault backend first</p> <pre><code># Vault config\nstorage \"s3\" {\n  access_key = \"${accessKey}\"\n  secret_key = \"${secretKey}\"\n  bucket     = \"vault-secrets\"\n  endpoint   = \"https://nyc3.digitaloceanspaces.com\"\n  path       = \"vault_storage\"\n}\n</code></pre> <p>Phase 3: Migrate application storage - Updated service configurations - Migrated existing data - Verified all services working</p> <p>Phase 4: Remove MinIO - Removed service definition - Cleaned up configuration - Updated documentation</p> <p>Results: - \u2705 More reliable storage - \u2705 Less infrastructure to manage - \u2705 Better durability (99.999999999%) - \u2705 Professional-grade backups</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#challenge-5-lets-encrypt-certificate-management","title":"Challenge 5: Let's Encrypt Certificate Management","text":"<p>Complexity: \u2b50\u2b50\u2b50</p> <p>The Problem: Automatic TLS certificates for production</p> <p>Challenges: 1. Rate Limiting: 50 certificates per week per domain 2. DNS Requirements: Domain must resolve to public IP 3. Challenge Verification: HTTP-01 challenge must reach server 4. Local Development: Can't get real certs for localhost</p> <p>Failure Modes Encountered: - Hit rate limit during testing (had to wait 1 hour) - Tried to get certs before DNS propagated (failed) - Mixed local/production configs (chaos)</p> <p>Solutions:</p> <p>For Production:</p> <pre><code># Traefik static config\ncertificatesResolvers:\n  lets-encrypt-tls:\n    acme:\n      email: admin@sprocket.mlesports.gg\n      storage: /data/acme.json\n      httpChallenge:\n        entryPoint: web\n</code></pre> <p>For Local Development:</p> <pre><code># No certificate resolver\n# Use Traefik's default self-signed certs\n# Accept certificate warnings in browser\n</code></pre> <p>For Testing:</p> <pre><code># Use Let's Encrypt staging\ncertificatesResolvers:\n  lets-encrypt-tls:\n    acme:\n      caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n</code></pre> <p>Best Practices: 1. Always test with staging first 2. Verify DNS before requesting prod certs 3. Persistent storage for <code>acme.json</code> (don't lose certs!) 4. One cert provisioning attempt takes 60+ seconds 5. Rate limits are per domain</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#architecture-evolution","title":"Architecture Evolution","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#before-2024","title":"Before (2024)","text":"<pre><code>Single-layer deployment\n  \u251c\u2500 All services in one stack\n  \u251c\u2500 Self-hosted PostgreSQL\n  \u251c\u2500 Self-hosted MinIO\n  \u251c\u2500 Manual Vault unsealing\n  \u251c\u2500 No automated secret provisioning\n  \u2514\u2500 Inconsistent configuration\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#after-2025","title":"After (2025)","text":"<pre><code>Three-layer architecture\n  \u251c\u2500 Layer 1 (Infrastructure)\n  \u2502   \u251c\u2500 Traefik (reverse proxy)\n  \u2502   \u251c\u2500 Vault (secrets, S3 backend)\n  \u2502   \u2514\u2500 Socket Proxy\n  \u2502\n  \u251c\u2500 Layer 2 (Data Services)\n  \u2502   \u251c\u2500 Redis\n  \u2502   \u251c\u2500 RabbitMQ\n  \u2502   \u251c\u2500 InfluxDB\n  \u2502   \u251c\u2500 Grafana\n  \u2502   \u251c\u2500 N8n\n  \u2502   \u251c\u2500 Neo4j\n  \u2502   \u251c\u2500 Gatus\n  \u2502   \u251c\u2500 Loki\n  \u2502   \u2514\u2500 Telegraf\n  \u2502\n  \u2514\u2500 Platform (Applications)\n      \u251c\u2500 Sprocket Web\n      \u251c\u2500 Sprocket API\n      \u251c\u2500 Discord Bot\n      \u251c\u2500 Image Generation (frontend &amp; service)\n      \u2514\u2500 6 microservices\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#external-services","title":"External Services","text":"<pre><code>Critical Dependencies\n  \u251c\u2500 Digital Ocean Managed PostgreSQL\n  \u251c\u2500 Digital Ocean Spaces (Vault backend)\n  \u251c\u2500 AWS S3 (application storage)\n  \u251c\u2500 Doppler (secret management)\n  \u251c\u2500 GitHub Organization (Vault auth)\n  \u2514\u2500 DNS Provider (Let's Encrypt)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#key-decisions-and-rationale","title":"Key Decisions and Rationale","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#decision-1-three-layer-architecture","title":"Decision 1: Three-Layer Architecture","text":"<p>Rationale: - Clear separation of concerns - Deploy layers independently - Each layer depends on previous layers - Easier troubleshooting - Better resource isolation</p> <p>Trade-offs: - \u2705 More organized - \u2705 Easier to understand - \u2705 Better deployment control - \u26a0\ufe0f More complex deployment process - \u26a0\ufe0f Layer dependencies must be managed</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#decision-2-managed-postgresql","title":"Decision 2: Managed PostgreSQL","text":"<p>Rationale: - Critical data requires professional-grade reliability - Automated backups essential - Point-in-time recovery needed - Team doesn't want to manage DB operations - Single node deployment limits HA options</p> <p>Trade-offs: - \u2705 Better reliability - \u2705 Automated backups - \u2705 Less operational burden - \u2705 Professional support - \u26a0\ufe0f Monthly cost ($15-60) - \u26a0\ufe0f External dependency - \u26a0\ufe0f Less control over tuning</p> <p>ROI: Worth it for production. The time saved on maintenance and peace of mind from automated backups easily justifies the cost.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#decision-3-cloud-s3-storage","title":"Decision 3: Cloud S3 Storage","text":"<p>Rationale: - S3-compatible storage needed for Vault backend - MinIO adds operational complexity - Cloud storage more reliable - Better durability guarantees</p> <p>Trade-offs: - \u2705 More reliable - \u2705 Less to manage - \u2705 Better durability - \u2705 Professional-grade - \u26a0\ufe0f Monthly cost (minimal) - \u26a0\ufe0f External dependency</p> <p>ROI: Absolutely worth it. MinIO was causing more problems than it solved.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#decision-4-doppler-for-secret-management","title":"Decision 4: Doppler for Secret Management","text":"<p>Rationale: - Need centralized secret management - Team collaboration on secrets - Audit trail for secret access - Better than storing secrets in Pulumi config</p> <p>Trade-offs: - \u2705 Centralized management - \u2705 Team access control - \u2705 Audit trail - \u2705 Easy updates - \u26a0\ufe0f External dependency - \u26a0\ufe0f Potential vendor lock-in</p> <p>Alternative Considered: AWS Secrets Manager (didn't want to lock into AWS)</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#decision-5-local-bind-mount-for-vault-unseal-keys","title":"Decision 5: Local Bind Mount for Vault Unseal Keys","text":"<p>Rationale: - Need automated unsealing - Cloud KMS too complex and costly for single-node - Pulumi config doesn't feel right for unseal keys - Acceptable risk for single-node deployment</p> <p>Trade-offs: - \u2705 Fully automated - \u2705 Simple implementation - \u2705 Repeatable - \u26a0\ufe0f Keys on node filesystem - \u26a0\ufe0f Not true auto-unseal</p> <p>Future Consideration: Migrate to cloud KMS auto-unseal if multi-node deployment needed.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#metrics-and-statistics","title":"Metrics and Statistics","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#project-metrics","title":"Project Metrics","text":"<ul> <li>Duration: 8 weeks (Sept 14 - Nov 8, 2025)</li> <li>Total Commits: 20+ commits in rebuild phase</li> <li>Major Breakthroughs: 5 key milestones</li> <li>Services Deployed: 22 total services</li> <li>3 in Layer 1</li> <li>9 in Layer 2</li> <li>10 in Platform</li> <li>External Dependencies: 6 critical systems</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#deployment-complexity","title":"Deployment Complexity","text":"<ul> <li>Pulumi Stacks: 3 (layer_1, layer_2, prod)</li> <li>Docker Networks: 5 overlay networks</li> <li>Docker Volumes: ~15 persistent volumes</li> <li>Configuration Files: 50+ JSON/YAML configs</li> <li>Secret Paths: 20+ Vault paths</li> <li>Infrastructure as Code: ~5,000 lines of TypeScript</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#git-activity","title":"Git Activity","text":"<ul> <li>Code Additions: Significant refactoring</li> <li>Code Deletions: Removed deprecated services</li> <li>Documentation: Added 4 major docs (README, CLOUD_DEPLOYMENT, LOCAL_ACCESS, this postmortem)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#what-went-well","title":"What Went Well","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-iterative-approach","title":"1. Iterative Approach","text":"<ul> <li>Small changes, frequent commits</li> <li>Test each layer independently</li> <li>Rollback easy if something breaks</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-comprehensive-documentation","title":"2. Comprehensive Documentation","text":"<ul> <li>Documented decisions as they were made</li> <li>Detailed commit messages</li> <li>Created guides for different scenarios (local, cloud)</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-migration-to-managed-services","title":"3. Migration to Managed Services","text":"<ul> <li>PostgreSQL migration smooth</li> <li>Storage migration phased well</li> <li>Reduced operational complexity significantly</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-automation","title":"4. Automation","text":"<ul> <li>Vault unsealing automated</li> <li>Secret provisioning scripted</li> <li>Deployment repeatable</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#5-problem-solving-persistence","title":"5. Problem-Solving Persistence","text":"<ul> <li>Vault unsealing took 5+ attempts</li> <li>Routing issues resolved through iteration</li> <li>Didn't give up when things got hard</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#what-could-have-been-better","title":"What Could Have Been Better","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-database-from-day-one","title":"1. Database from Day One","text":"<p>Issue: Started with self-hosted PostgreSQL, then migrated to managed Better Approach: Start with managed PostgreSQL from the beginning Impact: Would have saved time and complexity</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-storage-strategy-earlier","title":"2. Storage Strategy Earlier","text":"<p>Issue: Spent time setting up MinIO, then migrated away Better Approach: Use cloud S3 from the start Impact: Would have avoided migration effort</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-earlier-documentation","title":"3. Earlier Documentation","text":"<p>Issue: Some decisions documented after the fact Better Approach: Document architectural decisions as they're made (ADRs) Impact: Better context for future maintainers</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-testing-environments","title":"4. Testing Environments","text":"<p>Issue: Tested some changes directly in production config Better Approach: Use Let's Encrypt staging, have dev/staging/prod stacks Impact: Would have avoided rate limiting</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#5-initial-planning","title":"5. Initial Planning","text":"<p>Issue: Underestimated complexity, especially Vault automation Better Approach: Research Vault best practices before starting Impact: Might have gone with cloud KMS auto-unseal from start</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#lessons-learned","title":"Lessons Learned","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#technical-lessons","title":"Technical Lessons","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-use-managed-services-for-critical-infrastructure","title":"1. Use Managed Services for Critical Infrastructure","text":"<p>Lesson: Don't self-host databases or storage in production unless you have a really good reason.</p> <p>Rationale: - Managed services are worth the cost - Time saved on maintenance is valuable - Professional-grade reliability and backups - Team can focus on application, not infrastructure</p> <p>Application: Migrated both PostgreSQL and S3 to managed services. Both decisions were correct.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-automate-or-document-never-just-know","title":"2. Automate or Document, Never \"Just Know\"","text":"<p>Lesson: If it requires human memory or manual steps, automate it or document it thoroughly.</p> <p>Examples: - \u2705 Vault unsealing: Automated with script - \u2705 Secret provisioning: Scripted bootstrap process - \u2705 Deployment order: Documented in README - \u274c Some manual DNS changes: Should be in Terraform/Pulumi</p> <p>Application: Created scripts for repeatable processes, documentation for manual steps.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-design-for-production-first","title":"3. Design for Production First","text":"<p>Lesson: Design infrastructure for production, then add development overrides.</p> <p>Anti-pattern:</p> <pre><code># Starting with localhost, then trying to make it work in production\nhostname: localhost\n# Now how do I deploy to production?\n</code></pre> <p>Better Pattern:</p> <pre><code># Production-first design\nhostname: sprocket.mlesports.gg\n\n# Local development overrides\n# hostname: localhost\n# server-ip: 192.168.4.39\n</code></pre> <p>Application: Routing system designed for production domains, with optional IP-based access for development.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-secrets-management-requires-strategy","title":"4. Secrets Management Requires Strategy","text":"<p>Lesson: Secret management is more than just \"keep them safe\" - you need a coherent strategy.</p> <p>Our Strategy: 1. Source of Truth: Doppler (team access, audit trail) 2. Runtime Distribution: Vault (dynamic secrets, policies) 3. Container Secrets: Docker secrets (secure mounting) 4. Infrastructure Secrets: Pulumi config (encrypted)</p> <p>Why It Works: - Clear ownership of each secret type - Team can manage application secrets via Doppler - Infrastructure secrets separated - Auditable access</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#5-test-the-full-flow-not-just-individual-components","title":"5. Test the Full Flow, Not Just Individual Components","text":"<p>Lesson: Integration testing is critical for infrastructure.</p> <p>Example: Vault unsealing - \u2705 Vault initializes correctly - \u2705 Unseal script works - \u2705 Secrets can be written - \u274c But does the entire flow work on container restart?</p> <p>Application: Created verification scripts that test end-to-end flows: - <code>quick-test.sh</code>: Basic health checks - <code>verify-deployment.sh</code>: Comprehensive verification</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#6-git-history-is-invaluable","title":"6. Git History Is Invaluable","text":"<p>Lesson: Detailed commit messages are worth the effort.</p> <p>Good Commit:</p> <pre><code>feat(layer_2): migrate to cloud S3 and fix service configurations\n\nMajor changes:\n- Replaced local MinIO service with cloud S3-compatible storage\n- Updated SprocketMinioProvider to support direct credentials\n- Fixed N8n PostgreSQL port configuration\n- Removed postgres duplicate parameter\n\nBreaking changes:\n- Services now require s3-endpoint, s3-access-key, s3-secret-key config\n</code></pre> <p>Bad Commit:</p> <pre><code>updates\n</code></pre> <p>Application: This postmortem was possible because of detailed commit messages. Future debugging will be easier.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#7-external-dependencies-must-be-documented","title":"7. External Dependencies Must Be Documented","text":"<p>Lesson: Any external system your infrastructure depends on must be clearly documented.</p> <p>Our External Dependencies: 1. Doppler (secrets) 2. GitHub Organization (Vault auth) 3. Digital Ocean (database, storage) 4. DNS Provider (Let's Encrypt) 5. Pulumi Backend (state) 6. Docker Hub (private images)</p> <p>Application: Created <code>EXTERNALITIES_AND_DEPENDENCIES.md</code> with: - What each dependency provides - Why it's required - What happens if it's unavailable - How to set it up</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#process-lessons","title":"Process Lessons","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-break-big-problems-into-smaller-pieces","title":"1. Break Big Problems Into Smaller Pieces","text":"<p>Lesson: Don't try to solve everything at once.</p> <p>Example: Vault automation 1. First: Get Vault running 2. Then: Make initialization work 3. Then: Make unsealing work 4. Then: Make it repeatable 5. Finally: Make it automated</p> <p>Application: Layered architecture itself is an example - Layer 1 \u2192 Layer 2 \u2192 Platform.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-document-as-you-learn","title":"2. Document As You Learn","text":"<p>Lesson: Write documentation while the context is fresh.</p> <p>Examples: - Sept 19: \"docs: update readme with learnings\" (while debugging Vault) - Oct 27: Created CLOUD_DEPLOYMENT.md (right after solving routing) - Nov 8: Created comprehensive documentation package</p> <p>Application: If we'd waited until the end, we'd have forgotten crucial details.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-persistence-pays-off","title":"3. Persistence Pays Off","text":"<p>Lesson: Complex infrastructure problems often require multiple attempts.</p> <p>Example: Vault unsealing - Attempt 1: Failed (keys in Pulumi) - Attempt 2: Failed (manual unsealing) - Attempt 3: Failed (keys in MinIO) - Attempt 4: Failed (initialization issues) - Attempt 5: Success! (bind mount + auto-initialize script)</p> <p>Application: Don't give up on the first failure. Learn from each attempt.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-plan-for-multiple-environments","title":"4. Plan for Multiple Environments","text":"<p>Lesson: Local development and production are different beasts.</p> <p>Differences: | Aspect | Local | Production | |--------|-------|------------| | DNS | <code>.localhost</code> | Real domain | | Certificates | Self-signed | Let's Encrypt | | Routing | IP-based | Host-based | | Access | Local network | Public internet |</p> <p>Application: Created separate documentation for each environment (LOCAL_ACCESS.md, CLOUD_DEPLOYMENT.md).</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#recommendations-for-future-projects","title":"Recommendations for Future Projects","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-start-with-managed-services","title":"1. Start with Managed Services","text":"<p>For any new infrastructure project: - \u2705 Use managed databases (RDS, Digital Ocean, etc.) - \u2705 Use managed storage (S3, Spaces, GCS) - \u2705 Use managed secrets (Doppler, AWS Secrets Manager) - \u274c Don't self-host unless you have a specific reason</p> <p>Rationale: Your time is valuable. Let professionals manage infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-invest-in-automation-early","title":"2. Invest in Automation Early","text":"<p>Don't wait until deployment is painful to automate: - Write deployment scripts from day one - Automate secret provisioning - Create verification scripts - Document the deployment process</p> <p>Rationale: Manual deployments don't scale and lead to errors.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-use-infrastructure-as-code","title":"3. Use Infrastructure as Code","text":"<p>Everything should be in code: - \u2705 Infrastructure (Pulumi, Terraform) - \u2705 Configuration (YAML, JSON in git) - \u2705 Secrets (references, not values) - \u2705 Deployment scripts (bash, make)</p> <p>Rationale: If it's not in code, it's not repeatable.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-document-architectural-decisions","title":"4. Document Architectural Decisions","text":"<p>Create ADRs (Architectural Decision Records) for major decisions: - Why did we choose X over Y? - What were the trade-offs? - What assumptions did we make? - When should we revisit this decision?</p> <p>Rationale: Future you (and your team) will thank you.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#5-plan-for-disaster-recovery","title":"5. Plan for Disaster Recovery","text":"<p>Before going to production, answer these questions: - What if the database goes down? - What if Vault is lost? - What if Pulumi state is corrupted? - What if we lose access to Doppler? - How do we restore from backups?</p> <p>Rationale: Hope for the best, plan for the worst.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#open-questions-and-future-work","title":"Open Questions and Future Work","text":""},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#1-multi-node-deployment","title":"1. Multi-Node Deployment","text":"<p>Question: How would this architecture scale to multiple nodes?</p> <p>Considerations: - Vault unsealing (cloud KMS auto-unseal?) - Traefik load balancing - Persistent volume management - Database connection pooling</p> <p>Recommendation: Test with 3-node setup before scaling to production.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#2-disaster-recovery","title":"2. Disaster Recovery","text":"<p>Question: Can we fully restore from scratch?</p> <p>Gaps: - Vault unseal keys (currently on node filesystem) - Pulumi state (if backend is lost) - Doppler secrets (if account compromised)</p> <p>Recommendation: Document and test full disaster recovery procedure.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#3-cost-optimization","title":"3. Cost Optimization","text":"<p>Question: Can we reduce monthly costs?</p> <p>Current Costs: - Digital Ocean Managed PostgreSQL: $15-60/month - Digital Ocean Spaces: $5/month - AWS S3: Variable - Doppler: Free tier (for now) - Pulumi: Free tier (for now)</p> <p>Recommendation: Review costs quarterly, look for optimization opportunities.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#4-monitoring-and-alerting","title":"4. Monitoring and Alerting","text":"<p>Question: How do we know when things break?</p> <p>Current State: - \u2705 Gatus for service monitoring - \u2705 Grafana for metrics - \u2705 Loki for logs - \u274c No alerting configured - \u274c No on-call rotation</p> <p>Recommendation: Set up PagerDuty or similar, configure alerts.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#5-security-hardening","title":"5. Security Hardening","text":"<p>Question: Is the infrastructure secure enough for production?</p> <p>Current State: - \u2705 Secrets in Vault, not in code - \u2705 TLS everywhere - \u2705 Minimal exposed ports - \u274c No regular security audits - \u274c No automated vulnerability scanning - \u274c No WAF or DDoS protection</p> <p>Recommendation: Security audit, implement automated scanning, consider Cloudflare.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#conclusion","title":"Conclusion","text":"<p>The Sprocket infrastructure rebuild was a complex, 8-week journey from a partially-functional system to a production-ready platform. The project involved fundamental architectural changes, migration to managed services, and resolution of challenging technical problems.</p>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#key-achievements","title":"Key Achievements","text":"<ul> <li>\u2705 Production-ready infrastructure serving real users</li> <li>\u2705 Three-layer architecture with clear separation of concerns</li> <li>\u2705 Managed services for critical components (database, storage)</li> <li>\u2705 Automated processes for deployment and secret management</li> <li>\u2705 Comprehensive documentation for future maintenance</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#critical-success-factors","title":"Critical Success Factors","text":"<ol> <li>Persistence: Many problems required multiple attempts</li> <li>Iteration: Small changes, frequent commits, test often</li> <li>Managed Services: Database and storage migrations simplified operations</li> <li>Documentation: Writing things down as we learned saved the project</li> <li>Team Collaboration: Doppler and GitHub org enabled team access</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#most-important-lessons","title":"Most Important Lessons","text":"<ol> <li>Use managed services for critical infrastructure</li> <li>Design for production first, add dev overrides later</li> <li>Automate secret provisioning to reduce human error</li> <li>Document decisions when making them, not after</li> <li>Test each layer independently before stacking</li> <li>Git history is invaluable for understanding evolution</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/POSTMORTEM/#final-thoughts","title":"Final Thoughts","text":"<p>This project demonstrates that even complex infrastructure challenges can be solved through: - Systematic problem-solving - Willingness to iterate and learn - Good documentation practices - Appropriate use of managed services - Team collaboration</p> <p>The result is a robust, maintainable infrastructure that serves as a foundation for the Sprocket gaming platform. While there's always room for improvement (security hardening, better monitoring, disaster recovery testing), the infrastructure is production-ready and serving users successfully.</p> <p>The most valuable output of this project isn't just working infrastructure - it's the knowledge captured in documentation, code, and this postmortem that will enable future teams to maintain, extend, and learn from this work.</p> <p>Project Status: \u2705 COMPLETE Documentation Status: \u2705 COMPREHENSIVE Production Status: \u2705 LIVE AND SERVING USERS Team Satisfaction: \ud83c\udf89 HIGH</p> <p>This postmortem was generated from project documentation, git history, and architectural analysis. For detailed technical information, see the companion documents: TECHNICAL_CHALLENGES.md, CONTEXT_SUMMARY.md, EXTERNALITIES_AND_DEPENDENCIES.md, and GIT_HISTORY_ANALYSIS.md.</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/","title":"Technical Challenges and Solutions","text":"<p>A detailed account of the major technical challenges encountered during the Sprocket infrastructure deployment and how they were resolved.</p> <p>Note: For operational troubleshooting procedures derived from Claude conversation history, see <code>TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS.md</code> and the troubleshooting section in <code>OPERATIONS_RUNBOOK.md</code>.</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-1-vault-bootstrapping-and-unsealing","title":"Challenge 1: Vault Bootstrapping and Unsealing","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem","title":"The Problem","text":"<p>Vault requires initialization and unsealing on first start, but the unseal keys need to be stored securely and used automatically on subsequent deployments. This created a chicken-and-egg problem: - Can't provision secrets without Vault being unsealed - Can't unseal Vault without having the unseal keys - Can't make deployment repeatable without automating unseal process</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#symptoms","title":"Symptoms","text":"<ul> <li>Vault container would start but be sealed</li> <li>Services couldn't retrieve secrets from Vault</li> <li>Manual unsealing required after every deployment</li> <li>Unseal tokens stored inconsistently</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#evolution-through-commits","title":"Evolution Through Commits","text":"<ol> <li>Sept 17-18: Initial Vault deployment issues</li> <li>Commit <code>5de5d04</code>: \"bootstrapping problems with minio and other vault secrets\"</li> <li> <p>Vault wouldn't stay accessible</p> </li> <li> <p>Sept 18: Localhost access achieved</p> </li> <li>Commit <code>cde2961</code>: \"Accessing vault now works on localhost\"</li> <li> <p>But still manual process</p> </li> <li> <p>Sept 19: Breakthrough on automation</p> </li> <li>Commit <code>5057afc</code>: \"vault actually unseals!\"</li> <li>Auto-unseal script working</li> <li> <p>Commit <code>346c2a9</code>: Documentation of learnings</p> </li> <li> <p>Sept 19: Still polishing</p> </li> <li>Commit <code>fff0c3c</code>: \"still polishing layer_1 to be repeatable\"</li> <li> <p>Making the process truly repeatable</p> </li> <li> <p>Sept 21: Final solution</p> </li> <li>Commit <code>2103358</code>: \"all tokens working now. Vault should be good!\"</li> <li>Automated unseal process complete</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution","title":"Solution","text":"<ol> <li> <p>Initialization Script: Created script to detect if Vault is initialized    <code>bash    # Check if unseal_tokens.txt exists    if [ ! -f \"unseal_tokens.txt\" ]; then      vault operator init &gt; unseal_tokens.txt    fi</code></p> </li> <li> <p>Auto-unseal Process: Script reads unseal keys and unseals Vault    <code>bash    # Extract unseal keys and unseal    grep 'Unseal Key' unseal_tokens.txt | awk '{print $4}' | head -3 | while read key; do      vault operator unseal $key    done</code></p> </li> <li> <p>Persistence: Store unseal tokens securely (not in git)</p> </li> <li>Added to <code>.gitignore</code></li> <li> <p>Documented where to find them</p> </li> <li> <p>S3 Backend: Vault state persists to Digital Ocean Spaces</p> </li> <li>Survives container restarts</li> <li>Shared across deployments</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Vault unsealing cannot be fully automated without cloud auto-unseal (which requires cloud KMS)</li> <li>Store unseal keys securely but accessibly for automation</li> <li>Test initialization process from scratch multiple times</li> <li>Document the manual recovery process for when automation fails</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-2-database-management-vs-managed-services","title":"Challenge 2: Database Management vs. Managed Services","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_1","title":"The Problem","text":"<p>Initially PostgreSQL was deployed as a Docker service managed by Pulumi. This created several issues: - Data persistence concerns with Docker volumes - Backup/restore complexity - Resource management on single node - No built-in high availability - Manual maintenance required</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-decision-point","title":"The Decision Point","text":"<p>Key Question: Should we manage PostgreSQL ourselves or use a managed service?</p> <p>Trade-offs: | Self-Managed | Managed Service (Digital Ocean) | |--------------|--------------------------------| | Full control | Less control | | No extra cost | Monthly cost | | Complex backups | Automated backups | | Manual scaling | Easy scaling | | Single point of failure | High availability | | DIY maintenance | Managed updates |</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-solution","title":"The Solution","text":"<p>November 1-3, 2025: Migrated to Digital Ocean Managed PostgreSQL</p> <p>Implementation Steps: 1. Created managed PostgreSQL cluster on Digital Ocean 2. Updated all Pulumi configs with new connection details:    - <code>postgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com</code>    - <code>postgres-port: 25060</code> 3. Removed PostgreSQL service from <code>layer_2</code> 4. Updated all service providers to connect to external database 5. Migrated data from old PostgreSQL to managed instance 6. Updated <code>SprocketPostgresProvider</code> to support external connections</p> <p>Code Changes: - Removed local PostgreSQL service definition - Updated connection strings across all services - Modified provider to set <code>superuser=false</code> for managed DB - Removed PostgreSQL-specific Docker networks</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#impact","title":"Impact","text":"<p>\u2705 Positive: - Automated daily backups with 7-day retention - Point-in-time recovery capability - Better performance (dedicated resources) - Reduced maintenance burden - Professional-grade reliability</p> <p>\u26a0\ufe0f Considerations: - Monthly cost added to infrastructure - External dependency (requires internet connectivity) - Less flexibility for database-specific tuning - Vendor lock-in to Digital Ocean</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_1","title":"Lessons Learned","text":"<ul> <li>Don't manage databases in production unless you have to</li> <li>Managed services are worth the cost for critical data stores</li> <li>Plan data migration carefully (backup before, verify after)</li> <li>Update all connection configs in one atomic change</li> <li>Document external dependencies clearly</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-3-storage-backend-evolution-minio-s3","title":"Challenge 3: Storage Backend Evolution (MinIO \u2192 S3)","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_2","title":"The Problem","text":"<p>Initially used MinIO for S3-compatible object storage, deployed as a Docker service. Issues encountered: - MinIO required significant resources - Vault S3 backend had stability issues with self-hosted MinIO - Additional service to maintain - Backup complexity for MinIO data itself</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#timeline","title":"Timeline","text":"<ol> <li>Early deployment: MinIO for all S3 needs</li> <li>Sept 30 (commit <code>3f32a74</code>): Began migration to cloud S3</li> <li>\"migrate to cloud S3 and fix service configurations\"</li> <li>Updated <code>SprocketMinioProvider</code> to support direct credentials</li> <li>Oct 8 (commit <code>3b45f27</code>): Completed migration</li> <li>\"Move from MinIO to AWS. Remove postgres network.\"</li> <li>Removed MinIO service entirely</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution_1","title":"Solution","text":"<ol> <li>Phase 1: Dual operation</li> <li>Kept MinIO running</li> <li>Migrated Vault to Digital Ocean Spaces</li> <li> <p>Tested stability</p> </li> <li> <p>Phase 2: Service migration</p> </li> <li>Updated application services to use cloud S3</li> <li>Migrated existing data</li> <li> <p>Verified all services working</p> </li> <li> <p>Phase 3: Cleanup</p> </li> <li>Removed MinIO service definition</li> <li>Removed MinIO-related configuration</li> <li>Updated documentation</li> </ol> <p>Configuration Changes:</p> <pre><code># Old (MinIO)\ns3-endpoint: minio.localhost\ns3-access-key: local-minio-key\ns3-secret-key: local-minio-secret\n\n# New (Digital Ocean Spaces)\ns3-endpoint: nyc3.digitaloceanspaces.com\ns3-access-key: DO004ZHPV38R8C9Q46XG\ns3-secret-key: [secure]\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_2","title":"Lessons Learned","text":"<ul> <li>Cloud storage is more reliable than self-hosted for critical data</li> <li>S3-compatible APIs make migration easier</li> <li>Test storage backend changes thoroughly (Vault especially sensitive)</li> <li>Migrate in phases to reduce risk</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-4-routing-complexity-localhost-lan-cloud","title":"Challenge 4: Routing Complexity (Localhost, LAN, Cloud)","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_3","title":"The Problem","text":"<p>Need to support three different access patterns: 1. Local development: <code>.localhost</code> domains 2. LAN access: Direct IP (192.168.4.39) 3. Tailscale access: Tailscale IP (100.110.185.84) 4. Production cloud: Real domain (sprocket.mlesports.gg)</p> <p>Each pattern requires different Traefik routing rules and certificate handling.</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#symptoms_1","title":"Symptoms","text":"<ul> <li>404 errors when accessing via different methods</li> <li>Certificate warnings for localhost</li> <li>Can't access from LAN using domain names</li> <li>Duplicate router errors in Traefik</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#evolution","title":"Evolution","text":"<p>Oct 27 (commit <code>7486e02</code>): Major routing overhaul - Added IP-based routing alongside hostname routing - Created routing rules: <code>Host(\\</code>sprocket.localhost`) || Host(`192.168.4.39`) || Host(`100.110.185.84`)` - Documented differences between local and cloud deployments - Created CLOUD_DEPLOYMENT.md and LOCAL_ACCESS.md</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution_2","title":"Solution","text":"<p>For Local Development:</p> <pre><code>// Platform.ts - Multi-mode routing\nconst hostRules = [];\n\n// Hostname-based routing\nif (hostname) {\n  hostRules.push(`Host(\\`${subdomain}.${hostname}\\`)`);\n}\n\n// IP-based routing for LAN access\nif (serverIp) {\n  hostRules.push(`Host(\\`${serverIp}\\`)`);\n}\n\n// Tailscale routing\nif (tailscaleIp) {\n  hostRules.push(`Host(\\`${tailscaleIp}\\`)`);\n}\n\nconst routingRule = hostRules.join(' || ');\n</code></pre> <p>For Production:</p> <pre><code># Pulumi config\nhostname: sprocket.mlesports.gg\n# Remove server-ip and tailscale-ip configs\n</code></pre> <p>Traefik Configuration: - Let's Encrypt for production domains - Self-signed certs for localhost - No cert verification for IP-based access</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_3","title":"Lessons Learned","text":"<ul> <li>Design for multiple access patterns from the start</li> <li>Document which config to use for which scenario</li> <li>Test each access method independently</li> <li>IP-based routing is a hack, use proper DNS when possible</li> <li><code>/etc/hosts</code> modifications work for local testing</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-5-secret-management-across-vault-and-doppler","title":"Challenge 5: Secret Management Across Vault and Doppler","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_4","title":"The Problem","text":"<p>Secrets scattered across multiple sources: - Some in Pulumi config (encrypted) - Some in Vault (retrieved by services) - Some in Doppler (primary source of truth) - Manual secret provisioning steps</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenges","title":"Challenges","text":"<ol> <li>Bootstrapping: Need secrets to initialize Vault, but Vault stores secrets</li> <li>Service Configuration: Services need Vault address before Vault exists</li> <li>OAuth Credentials: Multiple providers with different secret formats</li> <li>Rotation: How to update secrets across all systems</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution-architecture","title":"Solution Architecture","text":"<p>1. Doppler as Source of Truth: - All secrets stored in Doppler project - Categories:   - OAuth (Google, Discord, Epic, Steam)   - API tokens (Ballchasing)   - Database credentials   - SMTP settings   - Application secrets</p> <p>2. Pulumi for Infrastructure Secrets:</p> <pre><code># Only infrastructure-level secrets in Pulumi\ndocker-access-token: [secure]\nvault-s3-secret-key: [secure]\npostgres-password: [secure]\n</code></pre> <p>3. Vault for Application Secrets: - Services retrieve secrets at runtime - Vault policies control access - GitHub teams map to Vault policies</p> <p>4. Bootstrap Process:</p> <pre><code># scripts/bootstrap-vault-secrets.sh\n1. Load secrets from Doppler\n2. Wait for Vault to be unsealed\n3. Provision secrets to Vault paths:\n   - platform/{env}/manual/oauth/*\n   - platform/ballchasing\n   - platform/{env}/chatwoot\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#secret-flow","title":"Secret Flow","text":"<pre><code>Doppler (Source of Truth)\n    \u2193\nBootstrap Script\n    \u2193\nVault (Runtime Storage)\n    \u2193\nApplication Services\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_4","title":"Lessons Learned","text":"<ul> <li>Centralize secret source of truth (Doppler)</li> <li>Use Vault for runtime secret distribution</li> <li>Keep infrastructure secrets in Pulumi config</li> <li>Document secret bootstrap process clearly</li> <li>Never commit secrets to git (even encrypted ones are risky)</li> <li>Automate secret provisioning to reduce human error</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-6-service-discovery-and-traefik-integration","title":"Challenge 6: Service Discovery and Traefik Integration","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_5","title":"The Problem","text":"<p>Docker Swarm services need to be automatically discovered by Traefik and routed correctly, but: - Services may not be ready when Traefik starts - Labels need to be precisely configured - Multiple services on same port need different routing - TLS termination complexity</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#symptoms_2","title":"Symptoms","text":"<ul> <li>404 errors even though services are running</li> <li>Traefik dashboard shows no routers</li> <li>Services visible in <code>docker service ls</code> but not accessible</li> <li>Duplicate router errors</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#root-causes","title":"Root Causes","text":"<ol> <li>Orphaned Service Tasks: Old service versions not cleaned up</li> <li>Label Syntax Errors: Traefik label typos or incorrect values</li> <li>Network Misconfiguration: Services not on ingress network</li> <li>Timing Issues: Traefik caches configuration</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution_3","title":"Solution","text":"<p>1. Proper Service Labels:</p> <pre><code>// In service definition\nlabels: {\n  'traefik.enable': 'true',\n  'traefik.http.routers.sprocket-web.rule': `Host(\\`sprocket.${hostname}\\`)`,\n  'traefik.http.routers.sprocket-web.entrypoints': 'websecure',\n  'traefik.http.routers.sprocket-web.tls': 'true',\n  'traefik.http.routers.sprocket-web.tls.certresolver': 'letsencrypt',\n  'traefik.http.services.sprocket-web.loadbalancer.server.port': '3000',\n}\n</code></pre> <p>2. Network Configuration:</p> <pre><code>// Ensure service is on ingress network\nnetworks: ['ingress']\n</code></pre> <p>3. Cleanup Procedure:</p> <pre><code># Remove orphaned tasks\ndocker service update --force &lt;service-name&gt;\n\n# Or full cleanup\ndocker service rm &lt;service-name&gt;\npulumi up  # Recreate\n</code></pre> <p>4. Verification:</p> <pre><code># Check Traefik sees the service\ndocker exec $(docker ps -q -f name=traefik) \\\n  wget -qO- http://localhost:8080/api/http/routers | jq\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_5","title":"Lessons Learned","text":"<ul> <li>Always verify network membership</li> <li>Use consistent label naming scheme</li> <li>Test with Traefik dashboard/API</li> <li>Force update services when routing changes</li> <li>Give Traefik 30-60 seconds for discovery</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#challenge-7-certificate-management-lets-encrypt","title":"Challenge 7: Certificate Management (Let's Encrypt)","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#the-problem_6","title":"The Problem","text":"<p>Need automatic TLS certificates for production, but: - Let's Encrypt requires public DNS - Rate limits prevent excessive testing - Localhost can't get real certificates - Certificate renewal must be automatic</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#failure-modes-encountered","title":"Failure Modes Encountered","text":"<ol> <li>Rate Limiting: Too many cert requests during testing</li> <li>DNS Not Ready: Requesting certs before DNS propagates</li> <li>Challenge Failures: HTTP-01 challenge can't reach server</li> <li>Mixed Local/Prod: Using production config locally</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#solution_4","title":"Solution","text":"<p>For Production:</p> <pre><code># Traefik configuration\n--certificatesresolvers.letsencrypt.acme.email=admin@example.com\n--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json\n--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web\n</code></pre> <p>For Local Development: - Use Traefik's default self-signed certificates - Accept certificate warnings in browser - Don't configure Let's Encrypt resolver</p> <p>For Testing:</p> <pre><code># Use Let's Encrypt staging\n--certificatesresolvers.letsencrypt.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directory\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#rate-limit-recovery","title":"Rate Limit Recovery","text":"<p>When hit with rate limits: 1. Wait 1 hour (soft limit resets) 2. Use staging environment for testing 3. Verify DNS before requesting prod certs 4. Consolidate to wildcard cert if possible</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#lessons-learned_6","title":"Lessons Learned","text":"<ul> <li>Test with Let's Encrypt staging first</li> <li>Verify DNS propagation before cert requests</li> <li>One cert provisioning attempt can take 60+ seconds</li> <li>Persistent storage crucial (don't lose acme.json)</li> <li>Rate limits are per domain, plan accordingly</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#patterns-and-best-practices-discovered","title":"Patterns and Best Practices Discovered","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#1-layered-deployment","title":"1. Layered Deployment","text":"<p>Pattern: Deploy infrastructure in dependency order</p> <pre><code>Layer 1 (Foundation) \u2192 Layer 2 (Data) \u2192 Platform (Apps)\n</code></pre> <p>Why: Each layer depends on previous layers being healthy</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#2-configuration-hierarchy","title":"2. Configuration Hierarchy","text":"<p>Pattern: Environment-specific config overrides</p> <pre><code>Global defaults \u2192 Stack config \u2192 Environment variables \u2192 Doppler\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#3-health-checks","title":"3. Health Checks","text":"<p>Pattern: Verify each layer before proceeding</p> <pre><code># After each layer\n1. Check service status: docker service ls\n2. Check service logs: docker service logs &lt;name&gt;\n3. Verify connectivity: curl https://endpoint\n4. Check Traefik routes: query Traefik API\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#4-rollback-strategy","title":"4. Rollback Strategy","text":"<p>Pattern: Keep previous working state</p> <pre><code># Before major changes\n1. Document current state\n2. Export Pulumi stack: pulumi stack export &gt; backup.json\n3. Tag git commit\n4. Test rollback procedure\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#5-secret-provisioning","title":"5. Secret Provisioning","text":"<p>Pattern: Separate secret sources by use case</p> <pre><code>Infrastructure secrets \u2192 Pulumi config\nApplication secrets \u2192 Vault (from Doppler)\nTemporary tokens \u2192 Vault (generated)\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#what-would-we-do-differently","title":"What Would We Do Differently?","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#1-database-from-day-one","title":"1. Database from Day One","text":"<ul> <li>Start with managed PostgreSQL instead of self-hosted</li> <li>Saves time and complexity</li> <li>Worth the cost</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#2-simpler-storage","title":"2. Simpler Storage","text":"<ul> <li>Use cloud S3 from the beginning</li> <li>Skip MinIO entirely</li> <li>One less service to manage</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#3-better-secret-strategy","title":"3. Better Secret Strategy","text":"<ul> <li>Establish Doppler \u2192 Vault flow earlier</li> <li>Document secret paths from start</li> <li>Create secret migration scripts</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#4-routing-design","title":"4. Routing Design","text":"<ul> <li>Design for production first</li> <li>Add development overrides later</li> <li>Avoid IP-based routing hacks</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#5-documentation-as-we-go","title":"5. Documentation as We Go","text":"<ul> <li>Document decisions when making them</li> <li>Keep architectural decision records (ADRs)</li> <li>Don't rely on commit messages alone</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#6-testing-environments","title":"6. Testing Environments","text":"<ul> <li>Separate dev/staging/prod stacks earlier</li> <li>Use Let's Encrypt staging for testing</li> <li>Create reproducible test environments</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES/#critical-success-factors","title":"Critical Success Factors","text":"<p>What made this deployment eventually successful:</p> <ol> <li>Persistence: Many iterations to get it right</li> <li>Git History: Detailed commit messages helped track changes</li> <li>Incremental Changes: Small commits, test often</li> <li>External Services: Managed DB and storage reduced complexity</li> <li>Community Tools: Pulumi, Traefik, Vault well-documented</li> <li>Automation: Scripts for repeatable processes</li> <li>Documentation: Writing things down as we learned</li> </ol> <p>The journey from first commit to production took months, but the result is a robust, maintainable infrastructure.</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/","title":"Technical Challenges and Solutions from Claude Conversations","text":"<p>Document Type: Operations Supplement Source: Claude conversation history analysis Purpose: Detailed technical solutions for common operational issues</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#1-vault-unsealing-and-automation","title":"1. Vault Unsealing and Automation","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#problem","title":"Problem","text":"<p>Vault requires manual unsealing after each restart, creating a chicken-and-egg problem for automation: - Services need secrets from Vault - Vault must be unsealed to serve secrets - Unsealing requires unseal keys - Where to store unseal keys securely?</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#solution-implemented","title":"Solution Implemented","text":"<p>Local Bind Mount with Auto-Initialization Script</p> <pre><code># File: global/services/vault/scripts/auto-initialize.sh\n# Key components:\n1. Check if Vault is initialized (via S3 backend state)\n2. If not initialized: Run vault operator init, save tokens\n3. If initialized but sealed: Read unseal keys from bind mount\n4. Unseal with 3 of 5 keys\n5. Vault ready to serve secrets\n</code></pre> <p>File Structure:</p> <pre><code>global/services/vault/unseal-tokens/\n\u251c\u2500\u2500 unseal_tokens.txt    # Store unseal keys\n\u2514\u2500\u2500 root_token.txt       # Store root token\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#operational-commands","title":"Operational Commands","text":"<pre><code># Check Vault status\nvault status\n\n# Manual unseal (if automation fails)\nvault operator unseal &lt;unseal-key&gt;\n\n# Verify tokens work\nvault token lookup\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Issue: Vault shows as sealed after restart</li> <li>Solution: Check <code>/vault/unseal-tokens/unseal_tokens.txt</code> exists and contains valid keys</li> <li>Issue: Auto-unseal fails</li> <li>Solution: Manually unseal, then investigate script logs</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#2-multi-environment-routing-issues","title":"2. Multi-Environment Routing Issues","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#problem_1","title":"Problem","text":"<p>Platform needs to be accessible via multiple patterns: - Local development: <code>.localhost</code> domains - LAN access: Direct IP (192.168.4.39) - Tailscale access: Tailscale IP (100.110.185.84) - Production: Real domain (sprocket.mlesports.gg)</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#root-cause","title":"Root Cause","text":"<p>Traefik routes based on <code>Host</code> header, causing 404s when accessing via IP/localhost.</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#solution","title":"Solution","text":"<p>Conditional Routing Rules in TypeScript</p> <pre><code>// Build routing rule based on configuration\nconst hostRules = [];\n\nif (hostname) {\n  hostRules.push(`Host(\\`${subdomain}.${hostname}\\`)`);\n}\n\nif (serverIp) {\n  hostRules.push(`Host(\\`${serverIp}\\`)`);\n}\n\nif (tailscaleIp) {\n  hostRules.push(`Host(\\`${tailscaleIp}\\`)`);\n}\n\nconst routingRule = hostRules.join(' || ');\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#configuration-examples","title":"Configuration Examples","text":"<p>Local Development:</p> <pre><code>hostname: localhost\nserver-ip: 192.168.4.39\ntailscale-ip: 100.110.185.84\n</code></pre> <p>Production:</p> <pre><code>hostname: sprocket.mlesports.gg\n# Remove server-ip and tailscale-ip\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#operational-commands_1","title":"Operational Commands","text":"<pre><code># Test routing\ncurl -H \"Host: sprocket.mlesports.gg\" http://localhost\ncurl -H \"Host: 192.168.4.39\" http://localhost\n\n# Check Traefik routers\ndocker exec traefik wget -q -O- http://localhost:8080/api/http/routers\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#3-service-network-connectivity","title":"3. Service Network Connectivity","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#common-issues","title":"Common Issues","text":"<ol> <li>Services can't find RabbitMQ host</li> <li>Redis connection failures</li> <li>Cross-layer network isolation</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check service discovery\ndocker exec &lt;service-container&gt; nslookup layer2_rabbitmq\ndocker exec &lt;service-container&gt; ping layer2_redis\n\n# Check network connectivity\ndocker network ls\ndocker network inspect &lt;network-name&gt;\n\n# Test connections from container\ndocker exec &lt;service-container&gt; curl http://layer2_rabbitmq:5672\ndocker exec &lt;service-container&gt; redis-cli -h layer2_redis ping\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#solutions","title":"Solutions","text":"<ol> <li>Ensure services are on correct networks</li> <li>Verify DNS resolution works between layers</li> <li>Check firewall/security group rules</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#4-environment-variable-management","title":"4. Environment Variable Management","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#historical-issues","title":"Historical Issues","text":"<p>From conversations: - JWT_SECRET generation produced malformed strings - Duplicate REDIS_PASSWORD variables - Line breaks in tokens (INFLUX_ADMIN_TOKEN, FORWARD_AUTH_SECRET)</p>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#current-solution","title":"Current Solution","text":"<p>Bootstrap Script Approach: <code>scripts/bootstrap-vault-secrets.sh</code></p> <pre><code># Environment variables \u2192 Vault \u2192 Docker secrets\n# Source: Doppler or environment\n# Destination: Vault paths\n# Usage: Services read from Vault via Pulumi\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#operational-commands_2","title":"Operational Commands","text":"<pre><code># Bootstrap all secrets\nexport VAULT_ADDR=https://vault.sprocket.mlesports.gg\nexport VAULT_TOKEN=&lt;root-token&gt;\n./scripts/bootstrap-vault-secrets.sh\n\n# Verify secrets in Vault\nvault kv get platform/sprocket/manual/oauth/google\nvault kv get infrastructure/data/postgres\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#troubleshooting_1","title":"Troubleshooting","text":"<ul> <li>Issue: Secret not found</li> <li>Solution: Run bootstrap script, check Vault paths</li> <li>Issue: Service can't read secret</li> <li>Solution: Check Vault policies and service permissions</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#5-storage-migration-minio-cloud-s3","title":"5. Storage Migration (MinIO \u2192 Cloud S3)","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#migration-process","title":"Migration Process","text":"<ol> <li>Phase 1: Dual operation (keep MinIO, add cloud S3)</li> <li>Phase 2: Migrate Vault backend to cloud S3</li> <li>Phase 3: Migrate application services</li> <li>Phase 4: Remove MinIO completely</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#configuration-changes","title":"Configuration Changes","text":"<pre><code># Before (MinIO)\ns3-endpoint: minio.sprocket.mlesports.gg\ns3-access-key: local-minio-key\ns3-secret-key: local-minio-secret\ns3-port: 9000\nssl: false\n\n# After (Cloud S3)\ns3-endpoint: nyc3.digitaloceanspaces.com\ns3-access-key: DO004ZHPV38R8C9Q46XG\ns3-secret-key: [from Vault]\ns3-port: 443\nssl: true\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#verification-commands","title":"Verification Commands","text":"<pre><code># Test S3 connectivity\naws s3 ls --endpoint-url https://nyc3.digitaloceanspaces.com\n\n# Check Vault S3 backend\nvault status  # Should show initialized: true\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#6-certificate-and-https-issues","title":"6. Certificate and HTTPS Issues","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#lets-encrypt-challenges","title":"Let's Encrypt Challenges","text":"<ul> <li>Rate limiting (50 certificates/week per domain)</li> <li>DNS propagation delays</li> <li>Certificate provisioning timing</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#operational-commands_3","title":"Operational Commands","text":"<pre><code># Check certificate expiration\necho | openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg | openssl x509 -noout -dates\n\n# Force certificate renewal\ndocker service update --force traefik\n\n# Monitor Traefik logs for ACME issues\ndocker service logs traefik | grep -i \"acme\\|renew\\|certificate\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#troubleshooting_2","title":"Troubleshooting","text":"<ul> <li>Issue: Certificate renewal fails</li> <li>Solutions:</li> <li>Check DNS resolution</li> <li>Verify port 80 accessibility</li> <li>Check Let's Encrypt rate limits</li> <li>Use staging environment for testing</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#7-service-health-and-debugging","title":"7. Service Health and Debugging","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#quick-health-check","title":"Quick Health Check","text":"<pre><code># Run comprehensive test\n./quick-test.sh\n\n# Check all services\ndocker service ls\n\n# Check recent logs\ndocker service logs --since 1h &lt;service-name&gt;\n\n# Check resource usage\ndocker stats --no-stream\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#common-service-issues","title":"Common Service Issues","text":"<p>Service Won't Start:</p> <pre><code># Check logs for errors\ndocker service logs &lt;service-name&gt; --tail 100\n\n# Check configuration\ndocker service inspect &lt;service-name&gt;\n\n# Restart service\ndocker service update --force &lt;service-name&gt;\n</code></pre> <p>High Resource Usage:</p> <pre><code># Identify resource hogs\ndocker stats --no-stream\n\n# Check for memory leaks\ndocker service logs &lt;service-name&gt; | grep -i \"memory\\|leak\"\n\n# Scale if needed\ndocker service scale &lt;service-name&gt;=3\n</code></pre> <p>Network Issues:</p> <pre><code># Test connectivity\ndocker exec &lt;container&gt; ping &lt;target-service&gt;\ndocker exec &lt;container&gt; curl &lt;target-url&gt;\n\n# Check DNS\ndocker exec &lt;container&gt; nslookup &lt;service-name&gt;\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#8-database-connection-issues","title":"8. Database Connection Issues","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#managed-postgresql-digital-ocean","title":"Managed PostgreSQL (Digital Ocean)","text":"<pre><code># Connect to database\nPGPASSWORD='password' psql \\\n  -h sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com \\\n  -p 25060 \\\n  -U doadmin \\\n  -d defaultdb\n\n# Check connection count\nSELECT count(*) FROM pg_stat_activity;\n\n# Kill long-running queries\nSELECT pg_terminate_backend(pid) \nFROM pg_stat_activity \nWHERE state = 'active' \n  AND query_start &lt; now() - interval '5 minutes';\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#common-issues_1","title":"Common Issues","text":"<ol> <li>Connection timeouts: Check network connectivity, firewall rules</li> <li>Too many connections: Scale connection pool, optimize queries</li> <li>Authentication failures: Verify credentials in Vault, check user permissions</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#9-secret-rotation-procedures","title":"9. Secret Rotation Procedures","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#database-password-rotation","title":"Database Password Rotation","text":"<pre><code># 1. Generate new password\nNEW_PASSWORD=$(openssl rand -base64 32)\n\n# 2. Update Digital Ocean console\n# 3. Update Vault\nvault kv put infrastructure/data/postgres password=\"$NEW_PASSWORD\"\n\n# 4. Update Pulumi config\npulumi config set --secret postgres-password \"$NEW_PASSWORD\"\n\n# 5. Redeploy affected services\npulumi up\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#oauth-credential-rotation","title":"OAuth Credential Rotation","text":"<pre><code># 1. Generate new credentials in provider console\n# 2. Update Doppler\ndoppler secrets set GOOGLE_CLIENT_ID=\"new-id\"\ndoppler secrets set GOOGLE_CLIENT_SECRET=\"new-secret\"\n\n# 3. Re-run bootstrap\n./scripts/bootstrap-vault-secrets.sh\n\n# 4. Restart services\ndocker service update --force prod-sprocket-core-service\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#10-emergency-procedures","title":"10. Emergency Procedures","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#complete-service-outage","title":"Complete Service Outage","text":"<pre><code># 1. Check system status\ndocker node ls\ndocker service ls\n\n# 2. Check logs for critical errors\ndocker service logs traefik --since 1h | grep -i error\n\n# 3. Restart core services\ndocker service update --force traefik\ndocker service update --force vault-service\n\n# 4. Verify restoration\n./quick-test.sh\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#database-recovery","title":"Database Recovery","text":"<pre><code># From Digital Ocean automated backup:\n# 1. Console \u2192 Databases \u2192 Backups\n# 2. Select backup \u2192 Restore\n# 3. Update connection strings if new cluster\n# 4. Verify connectivity\nPGPASSWORD='pass' psql -h new-host -p 25060 -U doadmin -d defaultdb -c \"SELECT 1;\"\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#vault-recovery","title":"Vault Recovery","text":"<pre><code># 1. Restore S3 data\naws s3 sync ./vault-backup/ s3://vault-secrets/\n\n# 2. Restore unseal keys\ncp -r ./vault-unseal-backup/ global/services/vault/unseal-tokens/\n\n# 3. Restart Vault\ndocker service update --force vault-service\n\n# 4. Verify unsealed\nvault status\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#monitoring-and-alerting-setup","title":"Monitoring and Alerting Setup","text":""},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>Service availability (Gatus)</li> <li>Response times (Grafana)</li> <li>Error rates (application logs)</li> <li>Resource utilization (CPU, memory, disk)</li> <li>Certificate expiration</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TECHNICAL_CHALLENGES_FROM_CLAUDE_CONVERSATIONS/#alert-thresholds","title":"Alert Thresholds","text":"<ul> <li>Critical: Service down, error rate &gt;10%, certificate expiring &lt;7 days</li> <li>Warning: Response time &gt;2x normal, resource usage &gt;85%</li> <li>Info: Certificate expiring &lt;30 days, resource usage &gt;70%</li> </ul> <p>This document supplements the main Operations Runbook with specific solutions derived from the Claude conversation history during development.</p>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/","title":"Transfer Instructions","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#quick-transfer-to-your-workstation","title":"Quick Transfer to Your Workstation","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#option-1-scp-secure-copy","title":"Option 1: SCP (Secure Copy)","text":"<pre><code># From your workstation\nscp -r root@&lt;production-node-ip&gt;:/root/sprocket-infra/docs-output ~/sprocket-docs\n\n# Or if using a specific SSH key\nscp -i ~/.ssh/your-key -r root@&lt;production-node-ip&gt;:/root/sprocket-infra/docs-output ~/sprocket-docs\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#option-2-rsync-better-for-large-files","title":"Option 2: rsync (Better for large files)","text":"<pre><code># From your workstation\nrsync -avz root@&lt;production-node-ip&gt;:/root/sprocket-infra/docs-output/ ~/sprocket-docs/\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#option-3-git-commit-and-pull","title":"Option 3: Git Commit and Pull","text":"<pre><code># On production node\ncd /root/sprocket-infra\ngit add docs-output/\ngit commit -m \"docs: add comprehensive infrastructure documentation package\"\ngit push origin main\n\n# On your workstation\ngit pull origin main\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#what-youre-getting","title":"What You're Getting","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#files-in-docs-output","title":"Files in docs-output/","text":"<ol> <li>README.md (7.8 KB) - This overview and guide</li> <li>CONTEXT_SUMMARY.md (9.9 KB) - Current state and architecture</li> <li>TECHNICAL_CHALLENGES.md (17 KB) - Detailed problem/solution analysis</li> <li>GIT_HISTORY_ANALYSIS.md (12 KB) - Commit-by-commit journey</li> <li>EXTERNALITIES_AND_DEPENDENCIES.md (16 KB) - External requirements</li> <li>TRANSFER_INSTRUCTIONS.md - This file</li> </ol> <p>Total: ~63 KB of comprehensive documentation</p>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#on-your-workstation","title":"On Your Workstation","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#1-set-up-mkdocs","title":"1. Set up MkDocs","text":"<pre><code># Install MkDocs with Material theme\npip install mkdocs mkdocs-material\n\n# Create new MkDocs project\nmkdocs new sprocket-infrastructure-docs\ncd sprocket-infrastructure-docs\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#2-configure-mkdocs","title":"2. Configure MkDocs","text":"<p>Edit <code>mkdocs.yml</code>:</p> <pre><code>site_name: Sprocket Infrastructure Documentation\ntheme:\n  name: material\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.top\n    - search.suggest\n    - search.highlight\n    - content.code.copy\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - admonition\n  - pymdownx.details\n  - tables\n  - footnotes\n\nnav:\n  - Home: index.md\n  - Postmortem:\n      - Journey: postmortem/journey.md\n      - Challenges: postmortem/challenges.md\n      - Lessons Learned: postmortem/lessons.md\n  - Architecture:\n      - Overview: architecture/overview.md\n      - Externalities: architecture/externalities.md\n      - Layers: architecture/layers.md\n  - Deployment:\n      - Prerequisites: deployment/prerequisites.md\n      - Cloud Deployment: deployment/cloud.md\n      - Local Development: deployment/local.md\n  - Reference:\n      - Git History: reference/git-history.md\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#3-organize-the-documentation","title":"3. Organize the Documentation","text":"<pre><code>cd sprocket-infrastructure-docs\n\n# Copy transferred docs\ncp ~/sprocket-docs/README.md docs/index.md\n\n# Create directory structure\nmkdir -p docs/postmortem\nmkdir -p docs/architecture\nmkdir -p docs/deployment\nmkdir -p docs/reference\n\n# Place the docs\ncp ~/sprocket-docs/GIT_HISTORY_ANALYSIS.md docs/postmortem/journey.md\ncp ~/sprocket-docs/TECHNICAL_CHALLENGES.md docs/postmortem/challenges.md\ncp ~/sprocket-docs/CONTEXT_SUMMARY.md docs/architecture/overview.md\ncp ~/sprocket-docs/EXTERNALITIES_AND_DEPENDENCIES.md docs/architecture/externalities.md\n\n# You can split and reorganize as needed\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#4-preview-the-site","title":"4. Preview the Site","text":"<pre><code>mkdocs serve\n\n# Open browser to http://127.0.0.1:8000\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#5-build-for-deployment","title":"5. Build for Deployment","text":"<pre><code>mkdocs build\n\n# This creates a 'site/' directory with static HTML\n# Deploy to GitHub Pages, Netlify, or your org's doc platform\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#what-to-do-next","title":"What to Do Next","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#immediate-on-workstation","title":"Immediate (On Workstation)","text":"<ol> <li>\u2705 Transfer documentation</li> <li>\u2705 Set up MkDocs project</li> <li>\u2705 Copy files into docs structure</li> <li>\u2705 Preview site locally</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#short-term-this-week","title":"Short-term (This Week)","text":"<ol> <li>\ud83d\udcdd Split long documents into logical pages</li> <li>\ud83d\udcdd Add diagrams (architecture, network, data flow)</li> <li>\ud83d\udcdd Enhance existing docs (CLOUD_DEPLOYMENT.md, LOCAL_ACCESS.md)</li> <li>\ud83d\udcdd Create component-specific pages</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#medium-term-next-2-weeks","title":"Medium-term (Next 2 Weeks)","text":"<ol> <li>\ud83d\udcdd Write detailed deployment guide with screenshots</li> <li>\ud83d\udcdd Create operations runbook</li> <li>\ud83d\udcdd Build troubleshooting guide</li> <li>\ud83d\udcdd Add code examples and configurations</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#long-term-next-month","title":"Long-term (Next Month)","text":"<ol> <li>\ud83d\udcdd Complete all planned documentation</li> <li>\ud83d\udcdd Review with team</li> <li>\ud83d\udcdd Test with someone new to the project</li> <li>\ud83d\udcdd Deploy to organization's doc platform</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#tips-for-writing-the-full-documentation","title":"Tips for Writing the Full Documentation","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#use-the-source-material","title":"Use the Source Material","text":"<ul> <li>CONTEXT_SUMMARY.md: Architecture overview, current state</li> <li>TECHNICAL_CHALLENGES.md: Why decisions were made, lessons learned</li> <li>GIT_HISTORY_ANALYSIS.md: Timeline, evolution, commit context</li> <li>EXTERNALITIES_AND_DEPENDENCIES.md: Prerequisites, setup, requirements</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#add-visual-elements","title":"Add Visual Elements","text":"<ul> <li>Architecture diagrams (use Mermaid, Draw.io, or Lucidchart)</li> <li>Network topology</li> <li>Data flow diagrams</li> <li>Screenshots of successful outputs</li> <li>Command output examples</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#write-for-different-audiences","title":"Write for Different Audiences","text":"<ul> <li>New team members: Step-by-step, assume little context</li> <li>Operators: Focus on day-2 operations, troubleshooting</li> <li>Developers: Integration points, service configs</li> <li>Leadership: High-level architecture, costs, decisions</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#include-real-examples","title":"Include Real Examples","text":"<pre><code># Good: Show actual commands with real outputs\n$ pulumi stack output\nhostname: sprocket.mlesports.gg\npostgres-host: sprocketbot-postgres-d5033d2...\n</code></pre> <pre><code># Good: Show expected success states\n$ docker service ls\nID             NAME              MODE         REPLICAS   IMAGE\nabc123         traefik           replicated   1/1        traefik:v2.10\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#use-admonitions","title":"Use Admonitions","text":"<pre><code>!!! warning \"Important\"\n    Always unseal Vault before deploying Layer 2\n\n!!! tip \"Pro Tip\"\n    Use `pulumi preview` to see changes before applying\n\n!!! danger \"Critical\"\n    Never commit secrets to git, even encrypted ones\n</code></pre>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#documentation-principles","title":"Documentation Principles","text":"<ol> <li>Assume Zero Knowledge: Write for someone new</li> <li>Show Expected Outputs: Every command shows success state</li> <li>Explain the Why: Don't just say what, explain why</li> <li>Include Failure Modes: What goes wrong and how to fix</li> <li>Use Real Examples: Actual commands, actual configs</li> <li>Keep It DRY: Reference common procedures</li> <li>Make It Searchable: Good headings, consistent terms</li> </ol>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#validation-checklist","title":"Validation Checklist","text":"<p>Before publishing documentation, ensure:</p> <ul> <li>[ ] Can someone new follow the deployment guide?</li> <li>[ ] Are all prerequisites clearly listed?</li> <li>[ ] Are all external dependencies documented?</li> <li>[ ] Are troubleshooting steps included?</li> <li>[ ] Are diagrams clear and accurate?</li> <li>[ ] Are code examples tested and working?</li> <li>[ ] Is the documentation searchable?</li> <li>[ ] Are there cross-references between related topics?</li> <li>[ ] Is the language clear and concise?</li> <li>[ ] Are security considerations addressed?</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#questions-to-answer-in-full-documentation","title":"Questions to Answer in Full Documentation","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#architecture","title":"Architecture","text":"<ul> <li>[x] What are the three layers and why?</li> <li>[x] Why managed PostgreSQL instead of self-hosted?</li> <li>[x] How does Vault integrate with everything?</li> <li>[x] What's the network topology?</li> <li>[ ] How does service discovery work?</li> <li>[ ] What's the monitoring strategy?</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#deployment","title":"Deployment","text":"<ul> <li>[x] What are all the prerequisites?</li> <li>[ ] What's the exact step-by-step process?</li> <li>[ ] What does success look like at each step?</li> <li>[ ] How do you verify deployment health?</li> <li>[ ] What are common deployment failures?</li> <li>[ ] How do you rollback if needed?</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#operations","title":"Operations","text":"<ul> <li>[ ] How do you update services?</li> <li>[ ] How do you scale services?</li> <li>[ ] How do you backup and restore?</li> <li>[ ] How do you rotate secrets?</li> <li>[ ] How do you monitor health?</li> <li>[ ] How do you respond to incidents?</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>[x] What are common issues and solutions?</li> <li>[ ] How do you debug service failures?</li> <li>[ ] How do you debug networking issues?</li> <li>[ ] How do you debug certificate issues?</li> <li>[ ] Where are logs located?</li> <li>[ ] What do specific error messages mean?</li> </ul>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#resource-estimates","title":"Resource Estimates","text":""},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#documentation-effort","title":"Documentation Effort","text":"<ul> <li>Postmortem docs: \u2705 Complete (~4 hours of work already done)</li> <li>Architecture docs: \ud83d\udcdd 8-12 hours (diagrams, deep dives)</li> <li>Deployment guide: \ud83d\udcdd 10-15 hours (detailed steps, screenshots)</li> <li>Component docs: \ud83d\udcdd 12-16 hours (all services documented)</li> <li>Operations runbook: \ud83d\udcdd 8-10 hours (procedures, playbooks)</li> <li>Troubleshooting: \ud83d\udcdd 6-8 hours (common issues, debugging)</li> <li>Polish and review: \ud83d\udcdd 4-6 hours (consistency, testing)</li> </ul> <p>Total estimated: 48-71 additional hours for complete documentation</p>"},{"location":"departments/development/systems/production-infrastructure/TRANSFER_INSTRUCTIONS/#benefits","title":"Benefits","text":"<ul> <li>New team members can deploy independently</li> <li>Faster incident response with documented procedures</li> <li>Reduced \"tribal knowledge\" dependency</li> <li>Better onboarding experience</li> <li>Reference for future infrastructure projects</li> <li>Organizational knowledge preservation</li> </ul> <p>Good luck with the documentation! The hardest part (understanding the system) is done. Now it's just organizing and expanding the knowledge. \ud83d\ude80</p>"}]}